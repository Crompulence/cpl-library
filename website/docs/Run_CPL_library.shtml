<!DOCTYPE html>
<html lang="en-UK">
<title>About CPL Library</title>

<head>
    <link rel="stylesheet" href="../styles.css">
    <link rel="shortcut icon" href="../cpl_logo.ico">
</head>


<body>

    <div class="center">

        <!-- Navigation toolbar -->
<div id="nav">

    <table>
    <tr>
        <td style="text-align: left;"> 
            <homebutton><a style="text-decoration: none;" href='/index.shtml'>cpl library</a></homebutton>
        </td>
        <td style="text-align: right;">
        <ul>
            <li><a style="text-decoration: none;" class="links" href='/about.shtml'>about </a></li>
            <li><a style="text-decoration: none;" class="links" href='/download.shtml'>download </a></li>
            <li><a style="text-decoration: none;" class="links" href='/documentation.shtml'>documentation </a></li>
            <li><a style="text-decoration: none;" class="links" href='/faq.shtml'>faq </a></li>
            <li><a style="text-decoration: none;" class="links" href='/contact.shtml'>contact </a></li>
        </ul>
        </td>
    </tr>
    </table>

    <hr>

</div> 

<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Ways_to_Run_CPL_Library"><span class="tocnumber">1</span> <span class="toctext">Ways to Run CPL Library</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Multiple_Program_Multiple_Data_.28MPMD.29"><span class="tocnumber">1.1</span> <span class="toctext">Multiple Program Multiple Data (MPMD)</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#MPI_Spawn_--_Creating_runs_as_we_need_them"><span class="tocnumber">1.2</span> <span class="toctext">MPI_Spawn -- Creating runs as we need them</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#A_Better_Option_--_Create_both_Runs_and_link_them_using_MPI_Port"><span class="tocnumber">1.3</span> <span class="toctext">A Better Option -- Create both Runs and link them using MPI_Port</span></a></li>
</ul>
</li>
</ul>
</div>

<h2><span class="mw-headline" id="Ways_to_Run_CPL_Library">Ways to Run CPL Library</span></h2>
<h3><span class="mw-headline" id="Multiple_Program_Multiple_Data_.28MPMD.29">Multiple Program Multiple Data (MPMD)</span></h3>
<p>Currently two coupled codes are compiled separately.
They will link through CPL library by running both using MPI's MPMD mode 
</p><p><code> mpiexec -n 64 ./md&nbsp;: -n 8 ./cfd </code>
</p><p>with all exchanges between them through CPL_send, CPL_recv using the single <code>MPI_COMM_WORLD</code> created by MPMD jobs.
The problem with this approach is that they share a single <code>MPI_COMM_WORLD</code>.
When we come to new codes, these require <code>MPI_COMM_WORLD</code> to 
be changed throughout the code if any communication in the code assume 
they are the only code in the world (pretty much every software does!).
Although not too much work, the changes must then be maintained as a 
patch or re-applied with each new version of the code from the 
downstream repository. As an example, the LAMMPS socket (or APP) 
currently has four different patches just from changes in the last two 
years (and this is an established code).  
</p>
<h3><span class="mw-headline" id="MPI_Spawn_--_Creating_runs_as_we_need_them">MPI_Spawn -- Creating runs as we need them</span></h3>
<p>Much better is to provide a function,
</p><p><code> cplexec -n 64 ./md&nbsp;: -n 8 ./cfd </code>
</p><p>which creates two codes with their own MPI_COMM_WORLD. All 
intercomms will then be setup by CPL library and no changes are required
 to each code. One promising candidate to do this would be to write a 
C++ program which uses <code>MMPI_Comm_spawn</code>  based on the command line input supplied. This would look something like,
</p>
<div class="mw-highlight mw-content-ltr" dir="ltr"><pre>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt; 
#include &lt;string.h&gt;
#include "mpi.h"

#define NREALMS 2


int main(int argc, char **argv)
{

    //Parse commandline and convert to exec names, universe size and processors
    int n = 0;
    int UNIVERSE_SIZE = 0;
    int nprocs[NREALMS];
    char *ptr;
    char *execnames[NREALMS];


    MPI_Comm parentcomm, intercomm[NREALMS];
    MPI_Info infos[2] = { MPI_INFO_NULL, MPI_INFO_NULL };


    //Parse input arguments
    for (int i = 0; i &lt; argc; ++i)
    {
        if ( strcmp("-n",argv[i]) == 0){
            int nproc = strtol(argv[i+1], &amp;ptr, 10);
            UNIVERSE_SIZE += nproc;
            nprocs[n] = nproc;
            execnames[n] = argv[i+2];
            printf("%d, %d, %s, %s \n", nprocs[n], UNIVERSE_SIZE, argv[i+2], execnames[n]);
            n += 1;
        }
    }

    // Now spawn the programs you have parsed
    int ierr = MPI_Init( NULL, NULL);
    MPI_Comm_get_parent( &amp;parentcomm );
    if (parentcomm == MPI_COMM_NULL)
    {

        //Loop and spawn
        for (int n = 0; n &lt; 2; ++n){
            int errcodes[nprocs[n]];
            ierr = MPI_Comm_spawn( execnames[n], MPI_ARGV_NULL, nprocs[n], 
                                   MPI_INFO_NULL, 0, MPI_COMM_WORLD, &amp;intercomm[n], errcodes );
        }

    } else {
        printf("ERROR -- parent comm has it's own parent!?\n");
        return 1;
    }
    fflush(stdout);
    ierr = MPI_Finalize(); 
    return 0;

}</pre></div>
<p>compiled using <code> mpic++ ./code.cpp -o ./cplexec </code> and run as above,
</p><p><code> cplexec -n 64 ./md -n 8 ./cfd </code>
</p><p>with sample md.exe and cfd.exe of the form,
</p>
<div class="mw-highlight mw-content-ltr" dir="ltr"><pre>//StackWorkers
#include &lt;iostream&gt;
#include &lt;mpi.h&gt;
#include &lt;string.h&gt;
#include &lt;stdlib.h&gt;     /* malloc, free, rand */

int main(int argc, char *argv[])
{
    int ierr = MPI_Init(NULL, NULL);
    MPI_Comm parent, comm;
    ierr = MPI_Comm_get_parent(&amp;parent);
    int np; int rank; int nu; int ncp; int crank;
    MPI_Comm_size (MPI_COMM_WORLD, &amp;np); // Find out number of processes
    MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank); // Find out process rank
    MPI_Comm_remote_size(parent, &amp;nu);
    comm = MPI_COMM_WORLD;

    printf("I'm the spawned %s %d %d %d %d %d %d \n", argv[0], parent, np, rank, nu, ncp, crank );

    //Allocate and array and gather some data
    float sub_avg = float(rank);
    float *sub_avgs = NULL;
    if (rank == 0) {
      sub_avgs = (float *)malloc(sizeof(float) * np);
    }
    MPI_Gather(&amp;sub_avg, 1, MPI_FLOAT, sub_avgs, 1, MPI_FLOAT, 0,
               MPI_COMM_WORLD);

    if (rank == 0) {
        for (int i = 0; i &lt; np; ++i)
            printf("proc %d send %f \n ", i, sub_avgs[i]);
    }
    fflush(stdout);
    ierr = MPI_Finalize();
    return 0;
}</pre></div>
<p>There is a major problem with all this: The process which spawns the CFD and MD child processes is then sitting doing nothing.
I don't think we can simple finalise it and be sure it won't take the children with it.
One option would be to merge the parent into one of the child runs. This can be achieved by adding,
</p>
<div class="mw-highlight mw-content-ltr" dir="ltr"><pre>    //Merge parent with cfd processors
    MPI_Comm comm;
    MPI_Intercomm_merge(intercomm[1], 0, &amp;comm );
    MPI_Barrier(comm);</pre></div>
<p>on the parent and,
</p>
<div class="mw-highlight mw-content-ltr" dir="ltr"><pre>    if ( strcmp("./cfd.exe",argv[0]) == 0){
        MPI_Intercomm_merge(parent, 1, &amp;comm );
        MPI_Barrier(comm);
    }else{
        comm = MPI_COMM_WORLD;
    }</pre></div>
<p>to the child. However, the merged cfd and parent still don't share an
 MPI_COMM_WORLD so this won't work in general! 
In addition, the topology may be extremely badly setup with the parent 
on an entirely different node to the remaining cfd processors which it 
will commonly communicate (spawn is discouraged for reasons of 
efficiency in general).
</p>
<h3><span class="mw-headline" id="A_Better_Option_--_Create_both_Runs_and_link_them_using_MPI_Port">A Better Option -- Create both Runs and link them using MPI_Port</span></h3>
<p>The use of MPI ports and connect presents a better options. This 
allow two previous unrelated MPI jobs to link up and begin sharing an 
intercomm.
In principle, this requires names to be posted on an ompi-server (or 
mpich equivalent).
However, for scientific computing, writing to a shared file space is a 
common feature and one which we can safely assume is possible. In fact, 
many coupling schemes actually go no futher than writing and reading 
based coupling between code.
</p><p>So, if we were to open a port in one MPI program and write this 
to a file. All we would need to do in program two is to read the port 
from this file, connect and begin sending information. The code to do 
this,
</p>
<div class="mw-highlight mw-content-ltr" dir="ltr"><pre>#include "mpi.h"
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include &lt;unistd.h&gt;
#include &lt;iostream&gt;
#include &lt;fstream&gt;

using namespace std;

int main( int argc, char *argv[] )
{
    int num_errors = 0;
    int rank, size;
    char port1[MPI_MAX_PORT_NAME];
    char port2[MPI_MAX_PORT_NAME];
    MPI_Status status;
    MPI_Comm comm1, comm2;
    int data = 0;

    char *ptr;
    int runno = strtol(argv[1], &amp;ptr, 10);
    for (int i = 0; i &lt; argc; ++i)
        printf("inputs %d %d %s \n", i,runno, argv[i]);

    MPI_Init(&amp;argc, &amp;argv);
    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);

    if (runno == 0)
    {
        printf("0: opening ports.\n");fflush(stdout);
        MPI_Open_port(MPI_INFO_NULL, port1);
        printf("opened port1: &lt;%s&gt;\n", port1);

        //Write port file
        ofstream myfile;
        myfile.open("port");
        if( !myfile )
                cout &lt;&lt; "Opening file failed" &lt;&lt; endl;
        myfile &lt;&lt; port1 &lt;&lt; endl;
        if( !myfile )
            cout &lt;&lt; "Write failed" &lt;&lt; endl;
        myfile.close();

        printf("Port %s written to file \n", port1); fflush(stdout);

        printf("accepting port1.\n");fflush(stdout);

        //Establish connection and send data
        MPI_Comm_accept(port1, MPI_INFO_NULL, 0, MPI_COMM_SELF, &amp;comm1);

        printf("sending 5 \n");fflush(stdout);
        data = 5;
        MPI_Send(&amp;data, 1, MPI_INT, 0, 0, comm1);
        MPI_Close_port(port1);
    }
    else if (runno == 1)
    {

        //Read port file
        size_t   chars_read = 0;  
        ifstream myfile;
        //Wait until file exists and is avaialble
        myfile.open("port");
        while(!myfile){
            myfile.open("port");
            cout &lt;&lt; "Opening file failed" &lt;&lt; myfile &lt;&lt; endl;
            usleep(30000);
        }
        while( myfile &amp;&amp; chars_read &lt; 255 ) {
            myfile &gt;&gt; port1[ chars_read ];    
            if( myfile ) 
                 ++chars_read; 
            
            if( port1[ chars_read - 1 ] == '\n' ) 
                 break;
        }
        printf("Reading port %s from file \n", port1); fflush(stdout);
        remove( "port" );

        //Establish connection and recieve data
        MPI_Comm_connect(port1, MPI_INFO_NULL, 0, MPI_COMM_SELF, &amp;comm1);
        MPI_Recv(&amp;data, 1, MPI_INT, 0, 0, comm1, &amp;status);
        printf("Received %d 1\n", data); fflush(stdout);

    }

    //Barrier on intercomm before disconnecting
    MPI_Barrier(comm1);
    MPI_Comm_disconnect(&amp;comm1);
    MPI_Finalize();
    return 0;
}</pre></div>
<p>This code should be compiled and then run as two separate executables with run number <code>runno&lt;\code&gt; specified by a command line option,
</code></p><p><code>&lt;code&gt; mpiexec  ./a.out 0 &amp; mpiexec ./a.out 1 </code>
</p><p>This means that the CPL_init can be written to check if <code>MPI_COMM_WORLD</code> contains everything and if not, attempt to open a socket between the two codes.
Once open, this can be used as CPL_INTER_COMM is in the main setup and extended to an intracomm by merging the two wit <code>MPI_intercomm_create(CPL_REALM_COMM, comm_size - 1, CPL_WORLD_COMM ... </code> etc
</p><p>This does all mean that the cplexec will not be needed, instead both codes can simply be run using something like,
</p><p><code> mpiexec -n 64 ./md.exe &gt; MD_run_hist &amp; mpiexec -n 8 ./cfd.exe &gt; CFD_run_hist </code>
</p><p>It is this approach that CPL library uses. A function, cplexec, 
is still provided to create both jobs as Python subprocesses, check all 
libraries are consistent between both runs, kill both jobs neatly if 
either fails and attempt to ensure any errors are raised correctly.
</p>


</body></html>
