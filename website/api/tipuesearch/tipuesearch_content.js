var tipuesearch = {"pages":[{"text":"Fortran Program The coupler (CPL) consists of a series of function calls which are loosely based on the MPI framework in terms of both functionality and scope. Their aim is to facilitate the exchange of data between two parallel solvers -- Molecular Dynamics (MD) and continuum Computational Fluid Dynamics (CFD). The routines have been developed in Fortran 2008 with sufficient generality that they could be used as a language independent API through External functional interfaces. These routines are compiled into a library module which can be linked to both the Molecular Dynamics (MD) and Computational Fluid Dynamics (CFD) codes. Note Both codes should be run using the MPI multiple program multiple data paradigm (MPMD). Note mpiexec -n 32 ./md.code : -n 4 ./cfd.code This project was part funded under the HECToR Distributed Computational Science and Engineering (CSE) Service operated by NAG Ltd. HECToR - A Research Councils UK High End Computing Service - is the UK's national supercomputing service, managed by EPSRC on behalf of the participating Research Councils. Its mission is to support capability science and engineering in UK academia. The HECToR supercomputers are managed by UoE HPCx Ltd and the CSE Support Service is provided by NAG Ltd. http://www.hector.ac.uk Developer Info Edward Smith\nDavid Trevelyan","tags":"home","loc":"index.html","title":" Fortran Program "},{"text":"Procedures Procedure Location Procedure Type Description coupler_cfd_init coupler_module Subroutine Initialisation routine for coupler module - Every variable is sent and stored\n to ensure both md and cfd region have an identical list of parameters coupler_md_get_average_period coupler Function coupler_md_get_dt_cfd coupler Function coupler_md_get_md_steps_per_cfd_dt coupler Function coupler_md_get_nsteps coupler Function coupler_md_get_save_period coupler Function coupler_md_init coupler_module Subroutine Initialisation routine for coupler module - Every variable is sent and stored\n to ensure both md and cfd region have an identical list of parameters CPL_Cart_coords coupler Subroutine Determines process coords in appropriate realm's cartesian topology \n given a rank in any communicator CPL_cfd_adjust_domain coupler_module Subroutine CPL_comm_style coupler Function CPL_create_comm coupler_module Subroutine CPL_create_map coupler_module Subroutine CPL_gather coupler Subroutine Perform gather operation on CPL_OLAP_COMM communicator. The CFD processor\n is the root process. The gathered data is effectively \"slotted\" into the\n correct part of the recvarray, and is intented for use in providing the\n CFD simulation boundary conditions with data obtained from the MD\n simulation. CPL_get coupler Subroutine Wrapper to retrieve (read only) parameters from the coupler_module \n Note - this ensures all variable in the coupler are protected\n from corruption by either CFD or MD codes CPL_get_rank coupler Subroutine Return rank of current processor in specified COMM CPL_new_fileunit coupler_module Function CPL_olap_extents coupler Subroutine Get maximum and minimum cells for current communicator within\n the overlapping region only CPL_overlap coupler Function Check if current processor is in the overlap region CPL_pack coupler Subroutine CPL_proc_extents coupler Subroutine Gets maximum and minimum cells for processor coordinates CPL_proc_portion coupler Subroutine Get maximum and minimum cell indices, i.e. the 'portion', of the\n input cell extents 'limits' that is contributed by the current\n overlapping processor. CPL_rank_map coupler_module Subroutine CPL_realm coupler Function CPL_recv coupler Interface CPL_scatter coupler Subroutine Scatter cell-wise data from CFD processor to corresponding MD processors\n on the overlap communicator CPL_OLAP_COMM. CPL_send coupler Interface CPL_unpack coupler Subroutine CPL_write_header coupler_module Subroutine Writes header information to specified filename in the format\n Variable description ; variable name ; variable error_abort coupler_module Interface globalise coupler Function Get molecule's global position from position local to processor. localise coupler Function Get local position on processor from molecule's global position. locate coupler_module Subroutine map_cfd2md_global coupler Function Map global CFD position in global MD coordinate frame map_md2cfd_global coupler Function Map global MD position to global CFD coordinate frame messenger_lasterrorcheck coupler_module Subroutine printf coupler_module Subroutine read_coupler_input coupler_module Subroutine set_coupled_timing coupler_module Subroutine write_matrix coupler_module Subroutine write_matrix_int coupler_module Subroutine","tags":"list procedures","loc":"lists/procedures.html","title":"\nAll Procedures – Fortran Program\n"},{"text":"Source Files File Description coupler.f90 Routines accessible from application ( molecular or continuum ) after \n the name, in parenthesis, is the realm in which each routine must be called coupler_module.f90 COUPLER MODULE: \n A single coupler module for both codes - this contains the same information \n on both md and cfd side","tags":"list files","loc":"lists/files.html","title":"\nAll Files – Fortran Program\n"},{"text":"Modules Module Source File Description coupler coupler.f90 coupler_module coupler_module.f90 (cfd+md) Splits MPI_COMM_WORLD in both the CFD and MD code respectively\n         and create intercommunicator between CFD and MD","tags":"list modules","loc":"lists/modules.html","title":"\nAll Modules – Fortran Program\n"},{"text":"coupler.f90 Source File Source File coupler.f90 Modules coupler Source Code coupler.f90 All Source Files coupler.f90 coupler_module.f90 Routines accessible from application ( molecular or continuum ) after \n the name, in parenthesis, is the realm in which each routine must be called CPL_send_data              (cfd+md)   sends grid data exchanged between \n                                      realms ( generic interface) CPL_recv_data              (cfd+md)   receives data exchanged between realms \n                                      ( generic interface) CPL_cfd_get               (cfd)    returns coupler internal parameters \n                                      for CFD realm CPL_md_get                 (md)    returns coupler internal parameters \n                                      for MD realm CPL_md_get_save_period     (md)    auxiliary used for testing CPL_md_get_average_period  (md)    returns average period of BC CPL_md_get_md_per_cfd_dt   (md)  returns the number of step MD does for \n                                      each CFD step CPL_md_get_nsteps          (md)    returm CFD nsteps CPL_md_get_dt_cfd          (md)    returns MD dt CPL_md_set                 (md)    sets zL if CFD is 2D CPL_md_get_density         (md)    gets CFD density CPL_md_get_cfd_id          (md)    id for CFD code, possible values set \n                                      in coupler_parameters @author  Lucian Anton, November 2011 @author Edward Smith, Dave Trevelyan September 2012\n @see coupler_module Source Code !============================================================================= ! !    ________/\\\\\\\\\\\\\\\\\\__/\\\\\\\\\\\\\\\\\\\\\\\\\\____/\\\\\\_____________ !     _____/\\\\\\////////__\\/\\\\\\/////////\\\\\\_\\/\\\\\\_____________ !      ___/\\\\\\/___________\\/\\\\\\_______\\/\\\\\\_\\/\\\\\\_____________ !       __/\\\\\\_____________\\/\\\\\\\\\\\\\\\\\\\\\\\\\\/__\\/\\\\\\_____________ !        _\\/\\\\\\_____________\\/\\\\\\/////////____\\/\\\\\\_____________ !         _\\//\\\\\\____________\\/\\\\\\_____________\\/\\\\\\_____________ !          __\\///\\\\\\__________\\/\\\\\\_____________\\/\\\\\\_____________ !           ____\\////\\\\\\\\\\\\\\\\\\_\\/\\\\\\_____________\\/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\_ !            _______\\/////////__\\///______________\\///////////////__ ! ! !                         C P L  -  L I B R A R Y ! !           Copyright (C) 2012-2015 Edward Smith & David Trevelyan ! !License ! !    This file is part of CPL-Library. ! !    CPL-Library is free software: you can redistribute it and/or modify !    it under the terms of the GNU General Public License as published by !    the Free Software Foundation, either version 3 of the License, or !    (at your option) any later version. ! !    CPL-Library is distributed in the hope that it will be useful, !    but WITHOUT ANY WARRANTY; without even the implied warranty of !    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the !    GNU General Public License for more details. ! !    You should have received a copy of the GNU General Public License !    along with CPL-Library.  If not, see <http://www.gnu.org/licenses/>. ! ! !Description ! ! !Author(s) ! !   Edward Smith !   David Trevelyan ! !! Routines accessible from application ( molecular or continuum ) after !! the name, in parenthesis, is the realm in which each routine must be called !! ! SIMULATION SIMULATION SIMULATION SIMULATION SIMULATION SIMULATION SIMULATION !! !! - CPL_send_data        \t  (cfd+md)   sends grid data exchanged between !!                                      realms ( generic interface) !! !! - CPL_recv_data        \t  (cfd+md)   receives data exchanged between realms !!                                      ( generic interface) !! !! - CPL_cfd_get               (cfd)    returns coupler internal parameters !!                                      for CFD realm !! !! - CPL_md_get                 (md)    returns coupler internal parameters !!                                      for MD realm !! !! - CPL_md_get_save_period     (md)    auxiliary used for testing !! !! - CPL_md_get_average_period  (md)    returns average period of BC !! !! - CPL_md_get_md_per_cfd_dt   (md) \treturns the number of step MD does for !!                                      each CFD step !! !! - CPL_md_get_nsteps          (md)    returm CFD nsteps !! !! - CPL_md_get_dt_cfd          (md)    returns MD dt !! !! - CPL_md_set                 (md)    sets zL if CFD is 2D !! !! - CPL_md_get_density         (md)    gets CFD density !! !! - CPL_md_get_cfd_id          (md)    id for CFD code, possible values set !!                                      in coupler_parameters !! !! @author  Lucian Anton, November 2011 !! @author Edward Smith, Dave Trevelyan September 2012 !! @see coupler_module !============================================================================= module coupler USE ISO_C_BINDING implicit none interface CPL_send module procedure CPL_send_3d , CPL_send_4d end interface interface CPL_recv module procedure CPL_recv_3d , CPL_recv_4d end interface private CPL_send_3d , CPL_send_4d , & CPL_send_xd , CPL_recv_3d , CPL_recv_4d ,& CPL_recv_xd contains !============================================================================= !\t\t\t\t _____ _                 _       _   _ !\t\t\t\t/  ___(_)               | |     | | (_) !\t\t\t\t\\ `--. _ _ __ ___  _   _| | __ _| |_ _  ___  _ __ !\t\t\t\t `--. \\ | '_ ` _ \\| | | | |/ _` | __| |/ _ \\| '_ \\ !\t\t\t\t/\\__/ / | | | | | | |_| | | (_| | |_| | (_) | | | | !\t\t\t\t\\____/|_|_| |_| |_|\\__,_|_|\\__,_|\\__|_|\\___/|_| |_| ! !------------------------------------------------------------------------------ !                              CPL_gather                                     - !------------------------------------------------------------------------------ !> !! Perform gather operation on CPL_OLAP_COMM communicator. The CFD processor !! is the root process. The gathered data is effectively \"slotted\" into the !! correct part of the recvarray, and is intented for use in providing the !! CFD simulation boundary conditions with data obtained from the MD !! simulation. !! !! - Synopsis !! !!  - CPL_gather(gatherarray,npercell,limits,recvarray) !! !! - Input !! !!  - gatherarray !!   - Assumed shape array of data to be gathered from each MD processor !!     in the overlap communicator. !! !!  - limits !!   - Integer array of length 6, specifying the global cell extents of the !!     region to be gathered, is the same on ALL processors. !! !!  - npercell !!   - number of data points per cell to be gathered (integer) !!     Note: should be the same as size(gatherarray(1)) for MD !!     processor. E.G. npercell = 3 for gathering 3D velocities. !! !! - Input/Output !!  - recvarray !!   - The array in which the gathered values are to be stored on the CFD !!     processor. The only values to be changed in recvarray are: !!     recvarray(limits(1):limits(2),limits(3):limits(4),limits(5):limits(6)) !! !! - Output Parameters !!  - NONE !! !! @author David Trevelyan subroutine CPL_gather ( gatherarray , npercell , limits , recvarray ) !todo better name than recvarray use mpi use coupler_module implicit none integer , intent ( in ) :: npercell integer , intent ( in ) :: limits ( 6 ) real ( kind ( 0. d0 )), dimension (:,:,:,:), intent ( in ) :: gatherarray real ( kind ( 0. d0 )), dimension (:,:,:,:), intent ( inout ) :: recvarray integer :: sendcount integer , dimension (:), allocatable :: recvcounts , displs real ( kind ( 0. d0 )), dimension (:), allocatable :: sendbuf real ( kind ( 0. d0 )), dimension (:), allocatable :: recvbuf if ( .not. CPL_overlap ()) return call check_limits_consistency call prepare_gatherv_parameters if ( realm .eq. md_realm ) call pack_sendbuf call MPI_gatherv ( sendbuf , sendcount , MPI_DOUBLE_PRECISION , recvbuf , & recvcounts , displs , MPI_DOUBLE_PRECISION , CFDid_olap , & CPL_OLAP_COMM , ierr ) if ( realm .eq. cfd_realm ) call unpack_recvbuf call deallocate_gather_u contains subroutine check_limits_consistency implicit none logical :: consistent = .true. integer :: i , cfd_limits ( 6 ), md_limits ( 6 ) integer :: md_limits_send ( 6 ), cfd_limits_send ( 6 ) select case ( realm ) case ( md_realm ) md_limits = limits cfd_limits = 0 case ( cfd_realm ) md_limits = 0 cfd_limits = limits end select md_limits_send = md_limits cfd_limits_send = cfd_limits ! Since we set md/cfd limits to zero in opposing realms, MPI_MAX ! will result in the correct limits on both sides call MPI_Allreduce ( md_limits_send , md_limits , 6 , MPI_INTEGER , MPI_MAX , & CPL_OLAP_COMM , ierr ) call MPI_Allreduce ( cfd_limits_send , cfd_limits , 6 , MPI_INTEGER , MPI_MAX , & CPL_OLAP_COMM , ierr ) do i = 1 , 6 if ( md_limits ( i ) .ne. cfd_limits ( i )) consistent = .false. end do if ( .not. consistent ) then call error_abort ( \"MD and CFD limits not consistent in CPL_gather\" ) else return end if end subroutine check_limits_consistency subroutine prepare_gatherv_parameters implicit none integer :: coord ( 3 ), portion ( 6 ) integer :: ncells , bufsize integer :: trank_olap , tid_olap ! Check send limits are inside overlap region if ( limits ( 1 ) .lt. icmin .or. & limits ( 2 ) .gt. icmax .or. & limits ( 3 ) .lt. jcmin .or. & limits ( 4 ) .gt. jcmax .or. & limits ( 5 ) .lt. kcmin .or. & limits ( 6 ) .lt. kcmax ) then call error_abort ( \"Gather limits are outside global domain. \" // & \"Aborting simulation.\" ) end if ! Check if CFD processor has tried to \"send\" anything if ( myid_olap .eq. CFDid_olap .and. product ( shape ( gatherarray )) .ne. 0 ) then call error_abort ( 'CFD proc input to CPL_gather: ' // & 'gatherarray has nonzero size. Aborting ' // & 'from prepare_gatherv_parameters' ) end if ! Allocate send buffer if ( realm .eq. md_realm ) then call CPL_Cart_coords ( CPL_CART_COMM , rank_cart , realm , 3 , coord , ierr ) call CPL_proc_portion ( coord , md_realm , limits , portion , ncells ) bufsize = npercell * ncells else bufsize = 0 end if allocate ( sendbuf ( bufsize )) ! Allocate array of sendbuffer sizes and populate it allocate ( recvcounts ( nproc_olap )) do trank_olap = 1 , nproc_olap tid_olap = trank_olap - 1 if ( tid_olap .eq. CFDid_olap ) then recvcounts ( trank_olap ) = 0 else call CPL_Cart_coords ( CPL_OLAP_COMM , trank_olap , md_realm , 3 , & coord , ierr ) call CPL_proc_portion ( coord , md_realm , limits , portion , ncells ) recvcounts ( trank_olap ) = npercell * ncells end if end do ! Own sendbuffer size sendcount = size ( sendbuf ) ! Sanity check if ( sendcount .ne. recvcounts ( rank_olap )) then call error_abort ( 'Send buffer sizes calculated incorrectly ' // & 'in prepare_gatherv_parameters. Aborting.' ) end if ! Allocate recvbuffer on CFD proc allocate ( recvbuf ( sum ( recvcounts ))) ! Calculate displacements for each proc in array recvbuffer allocate ( displs ( nproc_olap )) displs ( 1 ) = 0 do trank_olap = 2 , nproc_olap displs ( trank_olap ) = sum ( recvcounts ( 1 : trank_olap - 1 )) end do end subroutine prepare_gatherv_parameters subroutine pack_sendbuf implicit none integer :: pos , ixyz , icell , jcell , kcell integer :: i , j , k integer :: coord ( 3 ), portion ( 6 ), mdextents ( 6 ) ! Get MD processor extents and cells portion of send region call CPL_Cart_coords ( CPL_CART_COMM , rank_cart , realm , 3 , coord , ierr ) call CPL_proc_extents ( coord , realm , mdextents ) call CPL_proc_portion ( coord , realm , limits , portion ) ! If MD proc has nothing to send, exit if ( any ( portion .eq. VOID )) return pos = 1 do ixyz = 1 , npercell do icell = portion ( 1 ), portion ( 2 ) do jcell = portion ( 3 ), portion ( 4 ) do kcell = portion ( 5 ), portion ( 6 ) ! Map to local coords (assumed shape array has ! lower bound 1 by default when input to subroutine) i = icell - mdextents ( 1 ) + 1 j = jcell - mdextents ( 3 ) + 1 k = kcell - mdextents ( 5 ) + 1 sendbuf ( pos ) = gatherarray ( ixyz , i , j , k ) pos = pos + 1 end do end do end do end do end subroutine pack_sendbuf subroutine unpack_recvbuf implicit none integer :: coord ( 3 ), portion ( 6 ), cfdextents ( 6 ) integer :: trank_olap , tid_olap integer :: pos , ixyz , icell , jcell , kcell integer :: i , j , k integer :: tempextents ( 6 ) ! Get CFD proc coords and extents, allocate suitable array call CPL_cart_coords ( CPL_OLAP_COMM , rank_olap , cfd_realm , 3 , coord , ierr ) call CPL_proc_extents ( coord , cfd_realm , cfdextents ) call CPL_proc_portion ( coord , cfd_realm , limits , portion ) ! Loop over all processors in overlap comm do trank_olap = 1 , nproc_olap tid_olap = trank_olap - 1 if ( tid_olap .eq. CFDid_olap ) cycle call CPL_Cart_coords ( CPL_OLAP_COMM , trank_olap , md_realm , 3 , coord , ierr ) call CPL_proc_portion ( coord , md_realm , limits , portion ) call CPL_proc_extents ( coord , md_realm , tempextents ) if ( any ( portion .eq. VOID )) cycle ! Set position and unpack MD proc's part of recvbuf to ! correct region of recvarray pos = displs ( trank_olap ) + 1 do ixyz = 1 , npercell do icell = portion ( 1 ), portion ( 2 ) do jcell = portion ( 3 ), portion ( 4 ) do kcell = portion ( 5 ), portion ( 6 ) i = icell - cfdextents ( 1 ) + 1 j = jcell - cfdextents ( 3 ) + 1 k = kcell - cfdextents ( 5 ) + 1 recvarray ( ixyz , i , j , k ) = recvbuf ( pos ) pos = pos + 1 !\t\t\t\twrite(8000+myid_world,'(a,i4,a,i4,a,i4,a,i4,a,f20.1)'),   & !\t\t\t\t      'recvarray(',ixyz,',',icell,',',jcell,',',kcell,') =', & !\t\t\t\t       recvarray(ixyz,i,j,k) end do end do end do end do end do end subroutine unpack_recvbuf subroutine deallocate_gather_u implicit none if ( allocated ( recvcounts )) deallocate ( recvcounts ) if ( allocated ( displs )) deallocate ( displs ) if ( allocated ( sendbuf )) deallocate ( sendbuf ) if ( allocated ( recvbuf )) deallocate ( recvbuf ) end subroutine deallocate_gather_u end subroutine CPL_gather !------------------------------------------------------------------------------ !                              CPL_scatter                                    - !------------------------------------------------------------------------------ !> !! Scatter cell-wise data from CFD processor to corresponding MD processors !! on the overlap communicator CPL_OLAP_COMM. !! !! - Synopsis !! !!  - CPL_scatter(scatterarray,npercell,limits,recvarray) !! !! - Input !! !!  - scatterarray !!   - assumed shape array of data to be scattered (real(kind(0.d0))) !! !!  - limits !!   - integer array of length 6, specifying the global cell extents of the !!     region to be scattered, is the same on all processors. !! !!  - npercell !!   - number of data points per cell to be scattered (integer). !!     Note: should be the same as size(scatterarray(1)) for CFD proc !! !! - Input/Output !!  - recvarray !!   - the array in which the scattered values are stored on the MD !!     processors. !! !! - Output !!  - NONE !! !! @author David Trevelyan subroutine CPL_scatter ( scatterarray , npercell , limits , recvarray ) use coupler_module use mpi implicit none integer , intent ( in ) :: npercell integer , intent ( in ) :: limits ( 6 ) real ( kind ( 0. d0 )), dimension (:,:,:,:), intent ( in ) :: scatterarray real ( kind ( 0. d0 )), dimension (:,:,:,:), intent ( inout ) :: recvarray integer :: recvcount integer , dimension (:), allocatable :: displs , sendcounts real ( kind ( 0. d0 )), dimension (:), allocatable :: recvbuf real ( kind ( 0. d0 )), dimension (:), allocatable :: scatterbuf if ( .not. CPL_overlap ()) return call prepare_scatterv_parameters if ( realm .eq. cfd_realm ) call pack_scatterbuf call MPI_scatterv ( scatterbuf , sendcounts , displs , MPI_DOUBLE_PRECISION , & recvbuf , recvcount , MPI_DOUBLE_PRECISION , CFDid_olap , & CPL_OLAP_COMM , ierr ) if ( realm .eq. md_realm ) call unpack_scatterbuf call deallocate_scatter_s contains subroutine prepare_scatterv_parameters implicit none integer :: ncells integer :: coord ( 3 ), portion ( 6 ) integer :: bufsize integer :: trank_olap , tid_olap ! Check send limits are inside overlap region if ( limits ( 1 ) .lt. icmin .or. & limits ( 2 ) .gt. icmax .or. & limits ( 3 ) .lt. jcmin .or. & limits ( 4 ) .gt. jcmax .or. & limits ( 5 ) .lt. kcmin .or. & limits ( 6 ) .lt. kcmax ) then call error_abort ( \"Scatter limits are outside global domain. \" // & \"Aborting simulation.\" ) end if if ( realm .eq. cfd_realm ) then call CPL_Cart_coords ( CPL_CART_COMM , rank_cart , cfd_realm , 3 , coord , ierr ) call CPL_proc_portion ( coord , cfd_realm , limits , portion , ncells ) bufsize = npercell * ncells else bufsize = 0 end if allocate ( scatterbuf ( bufsize )) allocate ( sendcounts ( nproc_olap )) allocate ( displs ( nproc_olap )) ! Loop over all procs in overlap comm do trank_olap = 1 , nproc_olap tid_olap = trank_olap - 1 ! Calculate number of data points to scatter to each proc if ( tid_olap .eq. CFDid_olap ) then sendcounts ( trank_olap ) = 0 else call CPL_Cart_coords ( CPL_OLAP_COMM , trank_olap , md_realm , 3 , & coord , ierr ) call CPL_proc_portion ( coord , md_realm , limits , portion , ncells ) sendcounts ( trank_olap ) = npercell * ncells end if end do ! Get number of data points this MD proc will receive recvcount = sendcounts ( rank_olap ) ! Calculate starting positions of each MD proc region in ! scatterbuf array displs ( 1 ) = 0 do trank_olap = 2 , nproc_olap displs ( trank_olap ) = sum ( sendcounts ( 1 : trank_olap - 1 )) end do ! Allocate space to receive data allocate ( recvbuf ( sum ( sendcounts ))) end subroutine prepare_scatterv_parameters subroutine pack_scatterbuf implicit none integer :: pos , n integer :: tid_olap integer :: coord ( 3 ), cfdextents ( 6 ), portion ( 6 ) integer :: ixyz , icell , jcell , kcell integer :: i , j , k ! Grab CFD proc extents to be used for local cell mapping (when an ! array is an input to subroutine it has lower bound 1 by default) call CPL_cart_coords ( CPL_CART_COMM , rank_cart , cfd_realm , 3 , coord , ierr ) call CPL_proc_extents ( coord , cfd_realm , cfdextents ) ! Loop over procs in olap comm and pack scatter buffer ! in separate regions for each MD proc pos = 1 do n = 1 , nproc_olap tid_olap = n - 1 if ( tid_olap .eq. CFDid_olap ) cycle call CPL_Cart_coords ( CPL_OLAP_COMM , n , md_realm , 3 , coord , ierr ) call CPL_proc_portion ( coord , md_realm , limits , portion ) if ( any ( portion .eq. VOID )) cycle do ixyz = 1 , npercell do icell = portion ( 1 ), portion ( 2 ) do jcell = portion ( 3 ), portion ( 4 ) do kcell = portion ( 5 ), portion ( 6 ) i = icell - cfdextents ( 1 ) + 1 j = jcell - cfdextents ( 3 ) + 1 k = kcell - cfdextents ( 5 ) + 1 scatterbuf ( pos ) = scatterarray ( ixyz , i , j , k ) pos = pos + 1 end do end do end do end do end do end subroutine pack_scatterbuf subroutine unpack_scatterbuf implicit none integer :: pos , ierr integer :: coord ( 3 ), portion ( 6 ), extents ( 6 ) integer :: ixyz , icell , jcell , kcell integer :: i , j , k call CPL_cart_coords ( CPL_OLAP_COMM , rank_olap , md_realm , 3 , coord , ierr ) call CPL_proc_extents ( coord , realm , extents ) call CPL_proc_portion ( coord , realm , limits , portion ) if ( any ( portion .eq. VOID )) return pos = 1 do ixyz = 1 , npercell do icell = portion ( 1 ), portion ( 2 ) do jcell = portion ( 3 ), portion ( 4 ) do kcell = portion ( 5 ), portion ( 6 ) i = icell - extents ( 1 ) + 1 j = jcell - extents ( 3 ) + 1 k = kcell - extents ( 5 ) + 1 recvarray ( ixyz , i , j , k ) = recvbuf ( pos ) !\t\t\twrite(7000+myid_realm,'(i4,a,i4,a,i4,a,i4,a,i4,a,f20.1)'),        & !\t\t\t\t  rank_cart,' recvarray(',ixyz,',',icell,',',jcell,',',kcell, & !\t\t\t\t  ') =',recvarray(ixyz,i,j,k) pos = pos + 1 end do end do end do end do end subroutine unpack_scatterbuf subroutine deallocate_scatter_s implicit none if ( allocated ( displs )) deallocate ( displs ) if ( allocated ( scatterbuf )) deallocate ( scatterbuf ) if ( allocated ( recvbuf )) deallocate ( recvbuf ) if ( allocated ( sendcounts )) deallocate ( sendcounts ) end subroutine deallocate_scatter_s end subroutine CPL_scatter !============================================================================= !> !! CPL_send_data wrapper for 3d arrays !! see CPL_send_xd for input description !! @see coupler#subroutine_CPL_send_xd !----------------------------------------------------------------------------- subroutine CPL_send_3d ( temp , icmin_send , icmax_send , jcmin_send , & jcmax_send , kcmin_send , kcmax_send , send_flag ) use coupler_module , only : icmin_olap , icmax_olap , & jcmin_olap , jcmax_olap , & kcmin_olap , kcmax_olap , error_abort implicit none logical , intent ( out ), optional :: send_flag integer , intent ( in ), optional :: icmax_send , icmin_send integer , intent ( in ), optional :: jcmax_send , jcmin_send integer , intent ( in ), optional :: kcmax_send , kcmin_send real ( kind = kind ( 0. d0 )), dimension (:,:,:), intent ( in ) :: temp integer :: n1 , n2 , n3 , n4 integer :: icmin , icmax , jcmin , jcmax , kcmin , kcmax real ( kind = kind ( 0. d0 )), dimension (:,:,:,:), allocatable :: asend !Revert to default i domain sending - top of overlap to bottom of overlap if (( present ( icmax_send )) .and. ( present ( icmin_send ))) then icmax = icmax_send ; icmin = icmin_send elseif (( .not. present ( icmax_send )) .and. ( .not. present ( icmin_send ))) then icmax = icmax_olap ; icmin = icmin_olap else call error_abort ( \"CPL_send error - both max and min i limits \" // & \"required and only one supplied\" ) endif !Revert to default j domain sending - top of overlap to bottom of overlap if (( present ( jcmax_send )) .and. ( present ( jcmin_send ))) then jcmax = jcmax_send ; jcmin = jcmin_send elseif (( .not. present ( jcmax_send )) .and. ( .not. present ( jcmin_send ))) then jcmax = jcmax_olap ; jcmin = jcmin_olap else call error_abort ( \"CPL_send error - both max and min j limits \" // & \"required and only one supplied\" ) endif !Revert to default k domain sending - top of overlap to bottom of overlap if (( present ( kcmax_send )) .and. ( present ( kcmin_send ))) then kcmax = kcmax_send ; kcmin = kcmin_send elseif (( .not. present ( kcmax_send )) .and. ( .not. present ( kcmin_send ))) then kcmax = kcmax_olap ; kcmin = kcmin_olap else call error_abort ( \"CPL_send error - both max and min k limits \" // & \"required and only one supplied\" ) endif n1 = 1 n2 = size ( temp , 1 ) n3 = size ( temp , 2 ) n4 = size ( temp , 3 ) !Add padding column to 3D array to make it 4D allocate ( asend ( n1 , n2 , n3 , n4 )) asend ( 1 ,:,:,:) = temp (:,:,:) call CPL_send_xd ( asend , icmax , icmin , jcmax , & jcmin , kcmax , kcmin , send_flag ) end subroutine CPL_send_3d !============================================================================= !> !! CPL_send_data wrapper for 4d arrays !! see CPL_send_xd for input description !! @see coupler#subroutine_CPL_send_xd !----------------------------------------------------------------------------- subroutine CPL_send_4d ( asend , icmin_send , icmax_send , jcmin_send , & jcmax_send , kcmin_send , kcmax_send , send_flag ) use coupler_module , only : icmin_olap , icmax_olap , & jcmin_olap , jcmax_olap , & kcmin_olap , kcmax_olap , error_abort implicit none logical , intent ( out ), optional :: send_flag integer , intent ( in ), optional :: icmax_send , icmin_send integer , intent ( in ), optional :: jcmax_send , jcmin_send integer , intent ( in ), optional :: kcmax_send , kcmin_send real ( kind = kind ( 0. d0 )), dimension (:,:,:,:), intent ( in ) :: asend integer :: npercell integer :: icmin , icmax , jcmin , jcmax , kcmin , kcmax npercell = size ( asend , 1 ) !if ((present(icmax_send))) print*, 'icmax_send', icmax_send !if ((present(icmin_send))) print*, 'icmin_send', icmin_send !if ((present(jcmax_send))) print*, 'jcmax_send', jcmax_send !if ((present(jcmin_send))) print*, 'jcmin_send', jcmin_send !if ((present(kcmax_send))) print*, 'kcmax_send', kcmax_send !if ((present(kcmin_send))) print*, 'kcmin_send', kcmin_send !Revert to default i domain sending - top of overlap to bottom of overlap if (( present ( icmax_send )) .and. ( present ( icmin_send ))) then icmax = icmax_send ; icmin = icmin_send elseif (( .not. present ( icmax_send )) .and. ( .not. present ( icmin_send ))) then icmax = icmax_olap ; icmin = icmin_olap else call error_abort ( \"CPL_send error - both max and min i limits \" // & \"required and only one supplied\" ) endif !Revert to default j domain sending - top of overlap to bottom of overlap if (( present ( jcmax_send )) .and. ( present ( jcmin_send ))) then jcmax = jcmax_send ; jcmin = jcmin_send elseif (( .not. present ( jcmax_send )) .and. ( .not. present ( jcmin_send ))) then jcmax = jcmax_olap ; jcmin = jcmin_olap else call error_abort ( \"CPL_send error - both max and min j limits \" // & \"required and only one supplied\" ) endif !Revert to default k domain sending - top of overlap to bottom of overlap if (( present ( kcmax_send )) .and. ( present ( kcmin_send ))) then kcmax = kcmax_send ; kcmin = kcmin_send elseif (( .not. present ( kcmax_send )) .and. ( .not. present ( kcmin_send ))) then kcmax = kcmax_olap ; kcmin = kcmin_olap else call error_abort ( \"CPL_send error - both max and min k limits \" // & \"required and only one supplied\" ) endif !send_extents = (/npercell,icmax-icmin+1,jcmax-jcmin+1,kcmax-kcmin+1 /) !if (any(shape(asend) .lt. send_extents)) then !\tprint'(2(a,4i5))', '  Shape of input array = ', shape(asend), & !\t\t\t\t\t  '  Passed range = ',send_extents !\tcall error_abort(\"CPL_send error - Specified send range is greater\" // & !\t\t\t\t\t \"than the number of cells on processor\") !endif call CPL_send_xd ( asend , icmin , icmax , jcmin , & jcmax , kcmin , kcmax , send_flag ) end subroutine CPL_send_4d !============================================================================= !\t\t\t\t\t\tCPL_send_xd !> !! Send data from the local grid to the associated ranks from the other !! realm !! !! - Synopsis !! !!  - CPL_send_xd(asend,icmin_send,icmax_send,jcmin_send, !!\t\t\t\t\t\t     jcmax_send,kcmin_send,kcmax_send,send_flag) !! !! - Input Parameters !! !!   - asend !! !!   - icmin_send !! !!   - icmax_send !! !!   - jcmin_send !! !!   - jcmax_send !! !!   - kcmin_send !! !!   - kcmax_send !! !! - Output Parameter !! !!   - send_flag !! !! @author Edward Smith ! ---------------------------------------------------------------------------- subroutine CPL_send_xd ( asend , icmin_send , icmax_send , jcmin_send , & jcmax_send , kcmin_send , kcmax_send , send_flag ) use mpi use coupler_module , only : md_realm , cfd_realm , & error_abort , CPL_GRAPH_COMM , myid_graph , olap_mask , & rank_world , realm , & iblock_realm , jblock_realm , kblock_realm , ierr , VOID implicit none !Flag set if processor has passed data logical , intent ( out ), optional :: send_flag ! Minimum and maximum values to send integer , intent ( in ) :: icmax_send , icmin_send , & jcmax_send , jcmin_send , & kcmax_send , kcmin_send ! Array containing data distributed on the grid real ( kind = kind ( 0. d0 )), dimension (:,:,:,:), intent ( in ) :: asend !Number of halos !integer :: nh = 1 !?? todo needed? !Neighbours integer :: nneighbors integer , dimension (:), allocatable :: id_neighbors ! local indices integer :: icell , jcell , kcell integer :: ix , iy , iz , icmin , icmax , jcmin , jcmax , kcmin , kcmax integer :: n , pos , iclmin , iclmax , jclmin , jclmax , kclmin , kclmax integer :: npercell ! auxiliaries integer :: nbr , ndata , itag , destid , ncells integer :: pcoords ( 3 ) integer , dimension ( 6 ) :: extents , limits , portion real ( kind = kind ( 0. d0 )), allocatable :: vbuf (:) ! This local CFD domain is outside MD overlap zone if ( olap_mask ( rank_world ) .eqv. .false. ) return ! Save limits array of Minimum and maximum values to send limits = ( / icmin_send , icmax_send , jcmin_send , jcmax_send , kcmin_send , kcmax_send / ) ! Get local grid box ranges seen by this rank for CFD if ( realm .eq. cfd_realm ) then !Load extents of CFD processor pcoords = ( / iblock_realm , jblock_realm , kblock_realm / ) call CPL_proc_extents ( pcoords , cfd_realm , extents ) endif ! Number of components at each grid point npercell = size ( asend , 1 ) !Get neighbours call MPI_Graph_neighbors_count ( CPL_GRAPH_COMM , myid_graph , nneighbors , ierr ) allocate ( id_neighbors ( nneighbors )) call MPI_Graph_neighbors ( CPL_GRAPH_COMM , myid_graph , nneighbors , id_neighbors , ierr ) !Set sendflag to false and only change if anything is sent if ( present ( send_flag )) send_flag = .false. ! loop through the maps and send the corresponding sections of asend do nbr = 1 , nneighbors !Get taget processor from mapping destid = id_neighbors ( nbr ) ! ----------------- pack data for destid----------------------------- if ( realm .eq. cfd_realm ) then !Get extents of nbr MD processor to send to call CPL_Cart_coords ( CPL_GRAPH_COMM , destid + 1 , md_realm , 3 , pcoords , ierr ) elseif ( realm .eq. md_realm ) then !Data to send is based on current processor as MD proc size < CFD proc size pcoords = ( / iblock_realm , jblock_realm , kblock_realm / ) !Get extents of current processor call CPL_proc_extents ( pcoords , md_realm , extents ) endif ! If limits passed to send routine, use these instead ! of overlap/processor limits call CPL_proc_portion ( pcoords , md_realm , limits , portion , ncells ) ! Amount of data to be sent if ( any ( portion .eq. VOID )) then !print*, 'VOID send qqqq',realm_name(realm),rank_world,rank_realm ndata = 0 else ! Get data range on processor's local extents iclmin = portion ( 1 ) - extents ( 1 ) + 1 ; iclmax = portion ( 2 ) - extents ( 1 ) + 1 jclmin = portion ( 3 ) - extents ( 3 ) + 1 ; jclmax = portion ( 4 ) - extents ( 3 ) + 1 kclmin = portion ( 5 ) - extents ( 5 ) + 1 ; kclmax = portion ( 6 ) - extents ( 5 ) + 1 ndata = npercell * ncells if ( allocated ( vbuf )) deallocate ( vbuf ); allocate ( vbuf ( ndata )) if ( present ( send_flag )) send_flag = .true. ! Pack array into buffer pos = 1 !print'(a,5i4,2i6,i4,24i4)', 'send qqqq',rank_world,rank_realm,rank_olap,ndata,nbr,destid, & !\t\t\t\t\t\tsize(asend),pos,& !\t\t\t\t\t\ticlmin,   iclmax,   jclmin,   jclmax,   kclmin,   kclmax,     & !\t\t\t\t\t\ticmin_send,icmax_send,jcmin_send,jcmax_send,kcmin_send,kcmax_send, & !\t\t\t\t\t\tportion, extents do kcell = kclmin , kclmax do jcell = jclmin , jclmax do icell = iclmin , iclmax do n = 1 , npercell vbuf ( pos ) = asend ( n , icell , jcell , kcell ) !write(98000+destid+1+10*rank_world,'(3i8,f20.5)') rank_world,destid+1,n, vbuf(pos) pos = pos + 1 end do end do end do end do ! ----------------- pack data for destid ----------------------------- ! Send data itag = 0 !mod( ncalls, MPI_TAG_UB) !Attention ncall could go over max tag value for long runs!! call MPI_send ( vbuf , ndata , MPI_DOUBLE_PRECISION , destid , itag , CPL_GRAPH_COMM , ierr ) endif enddo end subroutine CPL_send_xd !============================================================================= !> !! CPL_recv_xd wrapper for 3d arrays !! see CPL_recv_xd for input description !! @see coupler#subroutine_CPL_recv_xd !----------------------------------------------------------------------------- subroutine CPL_recv_3d ( temp , icmin_recv , icmax_recv , jcmin_recv , & jcmax_recv , kcmin_recv , kcmax_recv , recv_flag ) use coupler_module , only : icmin_olap , icmax_olap , & jcmin_olap , jcmax_olap , & kcmin_olap , kcmax_olap , error_abort implicit none logical , intent ( out ), optional :: recv_flag integer , intent ( in ), optional :: icmax_recv , icmin_recv integer , intent ( in ), optional :: jcmax_recv , jcmin_recv integer , intent ( in ), optional :: kcmax_recv , kcmin_recv real ( kind ( 0. d0 )), dimension (:,:,:), intent ( inout ) :: temp integer :: n1 , n2 , n3 , n4 integer :: icmin , icmax , jcmin , jcmax , kcmin , kcmax real ( kind ( 0. d0 )), dimension (:,:,:,:), allocatable :: arecv !if ((present(icmax_recv))) print*, 'icmax_recv', icmax_recv !if ((present(icmin_recv))) print*, 'icmin_recv', icmin_recv !if ((present(jcmax_recv))) print*, 'jcmax_recv', jcmax_recv !if ((present(jcmin_recv))) print*, 'jcmin_recv', jcmin_recv !if ((present(kcmax_recv))) print*, 'kcmax_recv', kcmax_recv !if ((present(kcmin_recv))) print*, 'kcmin_recv', kcmin_recv !Revert to default i domain sending - top of overlap to bottom of overlap if (( present ( icmax_recv )) .and. ( present ( icmin_recv ))) then icmax = icmax_recv ; icmin = icmin_recv elseif (( .not. present ( icmax_recv )) .and. ( .not. present ( icmin_recv ))) then icmax = icmax_olap ; icmin = icmin_olap else call error_abort ( \"CPL_recv error - both max and min i limits \" // & \"required and only one supplied\" ) endif !Revert to default j domain sending - top of overlap to bottom of overlap if (( present ( jcmax_recv )) .and. ( present ( jcmin_recv ))) then jcmax = jcmax_recv ; jcmin = jcmin_recv elseif (( .not. present ( jcmax_recv )) .and. ( .not. present ( jcmin_recv ))) then jcmax = jcmax_olap ; jcmin = jcmin_olap else call error_abort ( \"CPL_recv error - both max and min j limits \" // & \"required and only one supplied\" ) endif !Revert to default k domain sending - top of overlap to bottom of overlap if (( present ( kcmax_recv )) .and. ( present ( kcmin_recv ))) then kcmax = kcmax_recv ; kcmin = kcmin_recv elseif (( .not. present ( kcmax_recv )) .and. ( .not. present ( kcmin_recv ))) then kcmax = kcmax_olap ; kcmin = kcmin_olap else call error_abort ( \"CPL_recv error - both max and min k limits \" // & \"required and only one supplied\" ) endif n1 = 1 n2 = size ( temp , 1 ) n3 = size ( temp , 2 ) n4 = size ( temp , 3 ) !Add padding column to 3D array to make it 4D allocate ( arecv ( n1 , n2 , n3 , n4 )) call CPL_recv_xd ( arecv , icmin , icmax , jcmin , & jcmax , kcmin , kcmax , recv_flag ) temp (:,:,:) = arecv ( 1 ,:,:,:) end subroutine CPL_recv_3d !============================================================================= !> !! CPL_recv_xd  wrapper for 4d arrays !! See CPL_recv_xd for input description !! @see coupler#subroutine_CPL_recv_xd !----------------------------------------------------------------------------- subroutine CPL_recv_4d ( arecv , icmin_recv , icmax_recv , jcmin_recv , & jcmax_recv , kcmin_recv , kcmax_recv , recv_flag ) use coupler_module , only : icmin_olap , icmax_olap , & jcmin_olap , jcmax_olap , & kcmin_olap , kcmax_olap , error_abort implicit none logical , intent ( out ), optional :: recv_flag integer , intent ( in ), optional :: icmax_recv , icmin_recv integer , intent ( in ), optional :: jcmax_recv , jcmin_recv integer , intent ( in ), optional :: kcmax_recv , kcmin_recv real ( kind = kind ( 0. d0 )), dimension (:,:,:,:), intent ( out ) :: arecv integer :: n1 , n2 , n3 , n4 integer :: icmin , icmax , jcmin , jcmax , kcmin , kcmax !if ((present(icmax_recv))) print*, 'icmax_recv', icmax_recv !if ((present(icmin_recv))) print*, 'icmin_recv', icmin_recv !if ((present(jcmax_recv))) print*, 'jcmax_recv', jcmax_recv !if ((present(jcmin_recv))) print*, 'jcmin_recv', jcmin_recv !if ((present(kcmax_recv))) print*, 'kcmax_recv', kcmax_recv !if ((present(kcmin_recv))) print*, 'kcmin_recv', kcmin_recv !Revert to default i domain sending - top of overlap to bottom of overlap if (( present ( icmax_recv )) .and. ( present ( icmin_recv ))) then icmax = icmax_recv ; icmin = icmin_recv elseif (( .not. present ( icmax_recv )) .and. ( .not. present ( icmin_recv ))) then icmax = icmax_olap ; icmin = icmin_olap else call error_abort ( \"CPL_recv error - both max and min i limits \" // & \"required and only one supplied\" ) endif !Revert to default j domain sending - top of overlap to bottom of overlap if (( present ( jcmax_recv )) .and. ( present ( jcmin_recv ))) then jcmax = jcmax_recv ; jcmin = jcmin_recv elseif (( .not. present ( jcmax_recv )) .and. ( .not. present ( jcmin_recv ))) then jcmax = jcmax_olap ; jcmin = jcmin_olap else call error_abort ( \"CPL_recv error - both max and min j limits \" // & \"required and only one supplied\" ) endif !Revert to default k domain sending - top of overlap to bottom of overlap if (( present ( kcmax_recv )) .and. ( present ( kcmin_recv ))) then kcmax = kcmax_recv ; kcmin = kcmin_recv elseif (( .not. present ( kcmax_recv )) .and. ( .not. present ( kcmin_recv ))) then kcmax = kcmax_olap ; kcmin = kcmin_olap else call error_abort ( \"CPL_recv error - both max and min k limits \" // & \"required and only one supplied\" ) endif call CPL_recv_xd ( arecv , icmin , icmax , jcmin , & jcmax , kcmin , kcmax , recv_flag ) end subroutine CPL_recv_4d !============================================================================= !\t\t\t\t\t\tCPL_recv_xd !> !! Receive data from to local grid from the associated ranks from the other !! realm !! !! - Synopsis !! !!  - CPL_recv_xd(arecv,icmin_recv,icmax_recv,jcmin_recv, !!\t\t\t\t\t\t     jcmax_recv,kcmin_recv,kcmax_recv,recv_flag) !! !! - Input Parameters !! !!   - arecv !! !!   - icmin_recv !! !!   - icmax_recv !! !!   - jcmin_recv !! !!   - jcmax_recv !! !!   - kcmin_recv !! !!   - kcmax_recv !! !! - Output Parameter !! !!   - recv_flag !! !! @author Edward Smith ! ---------------------------------------------------------------------------- !----------------------------------------------------------------------------- subroutine CPL_recv_xd ( arecv , icmin_recv , icmax_recv , jcmin_recv , & jcmax_recv , kcmin_recv , kcmax_recv , recv_flag ) use mpi use coupler_module , only : md_realm , cfd_realm , & rank_graph , & error_abort , CPL_GRAPH_COMM , myid_graph , olap_mask , & rank_world , realm , & iblock_realm , jblock_realm , kblock_realm , VOID , ierr implicit none !Flag set if processor has received data logical , intent ( out ), optional :: recv_flag ! Minimum and maximum values of j to send integer , intent ( in ) :: icmax_recv , icmin_recv , & jcmax_recv , jcmin_recv , & kcmax_recv , kcmin_recv ! Array that recieves grid distributed data real ( kind ( 0. d0 )), dimension (:,:,:,:), intent ( inout ) :: arecv !Neighbours integer :: nneighbors integer , dimension (:), allocatable :: id_neighbors ! local indices integer :: n , nbr , icell , jcell , kcell integer :: pos , iclmin , iclmax , jclmin , jclmax , kclmin , kclmax integer :: pcoords ( 3 ), npercell , ndata , ncells integer , dimension ( 6 ) :: extents , portion , limits ! auxiliaries integer :: itag , sourceid , start_address integer , dimension (:), allocatable :: req integer , dimension (:,:), allocatable :: status real ( kind ( 0. d0 )), dimension (:), allocatable :: vbuf ! This local CFD domain is outside MD overlap zone if ( olap_mask ( rank_world ) .eqv. .false. ) return ! Save limits array of Minimum and maximum values to recv limits = ( / icmin_recv , icmax_recv , jcmin_recv , jcmax_recv , kcmin_recv , kcmax_recv / ) ! Number of components at each grid point npercell = size ( arecv , 1 ) ! Get local grid box ranges seen by this rank for CFD and allocate buffer if ( realm .eq. cfd_realm ) then !Load CFD cells per processor call CPL_Cart_coords ( CPL_GRAPH_COMM , rank_graph , cfd_realm , 3 , pcoords , ierr ) call CPL_proc_extents ( pcoords , cfd_realm , extents ) ! If limits passed to recv routine, use these instead ! of overlap/processor limits call CPL_proc_portion ( pcoords , cfd_realm , limits , portion , ncells ) ! Amount of data to receive from all MD processors ndata = npercell * ncells allocate ( vbuf ( ndata )); vbuf = 0. d0 endif !Get neighbours call MPI_Graph_neighbors_count ( CPL_GRAPH_COMM , myid_graph , nneighbors , ierr ) allocate ( id_neighbors ( nneighbors )) call MPI_Graph_neighbors ( CPL_GRAPH_COMM , myid_graph , nneighbors , id_neighbors , ierr ) ! Receive from all attached processors allocate ( req ( nneighbors )) allocate ( status ( MPI_STATUS_SIZE , nneighbors )) start_address = 1 do nbr = 1 , nneighbors !Get source processor from topology graph sourceid = id_neighbors ( nbr ) ! Get size of data to receive from source processors if ( realm .eq. cfd_realm ) then !CFD realm receives data based on size of MD processor domain call CPL_Cart_coords ( CPL_GRAPH_COMM , sourceid + 1 , md_realm , 3 , pcoords , ierr ) elseif ( realm .eq. md_realm ) then !MD realm receives data as big as own processor domain pcoords = ( / iblock_realm , jblock_realm , kblock_realm / ) endif ! If limits passed to recv routine, use these instead ! of overlap/processor limits call CPL_proc_portion ( pcoords , md_realm , limits , portion , ncells ) !Only receive if overlapping if ( any ( portion .eq. VOID )) then ndata = 0 req ( nbr ) = MPI_REQUEST_NULL if ( present ( recv_flag )) recv_flag = .false. else ! Amount of data to receive ndata = npercell * ncells if ( present ( recv_flag )) recv_flag = .true. if ( realm .eq. md_realm ) then !Allocate receive buffer for MD processors if ( allocated ( vbuf )) deallocate ( vbuf ); allocate ( vbuf ( ndata )); vbuf = 0. d0 endif ! Receive section of data itag = 0 call MPI_irecv ( vbuf ( start_address ), ndata , MPI_DOUBLE_PRECISION , sourceid , itag ,& CPL_GRAPH_COMM , req ( nbr ), ierr ) endif !Increment pointer ready to receive next piece of data start_address = start_address + ndata enddo call MPI_waitall ( nneighbors , req , status , ierr ) !if (rank_world .eq. 33) then !\tdo n = 1,size(vbuf) !\t\twrite(98000+rank_world,*) rank_world,n, vbuf(n) !\tenddo !endif ! ----------------- Unpack data ----------------------------- start_address = 1 do nbr = 1 , nneighbors !Get source processor from topology graph sourceid = id_neighbors ( nbr ) ! Get size of data to receive from source processors if ( realm .eq. cfd_realm ) then !CFD always receives data if ( present ( recv_flag )) recv_flag = .true. !CFD realm receives data based on size of MD processor domain call CPL_Cart_coords ( CPL_GRAPH_COMM , sourceid + 1 , md_realm , 3 , pcoords , ierr ) elseif ( realm .eq. md_realm ) then !MD realm receives data as big as own processor domain pcoords = ( / iblock_realm , jblock_realm , kblock_realm / ) !Get extents of current processor/overlap region call CPL_proc_extents ( pcoords , md_realm , extents ) endif ! If limits passed to recv routine, use these instead ! of overlap/processor limits call CPL_proc_portion ( pcoords , md_realm , limits , portion , ncells ) ! Unpack array into buffer if ( any ( portion .eq. VOID )) then !print*, 'VOID recv qqqq',realm_name(realm),rank_world,rank_realm,rank_graph2rank_world(sourceid+1),recv_flag ndata = 0 else ! Get local extents in received region pos = start_address ; ndata = npercell * ncells iclmin = portion ( 1 ) - extents ( 1 ) + 1 ; iclmax = portion ( 2 ) - extents ( 1 ) + 1 jclmin = portion ( 3 ) - extents ( 3 ) + 1 ; jclmax = portion ( 4 ) - extents ( 3 ) + 1 kclmin = portion ( 5 ) - extents ( 5 ) + 1 ; kclmax = portion ( 6 ) - extents ( 5 ) + 1 !print'(a,5i4,2i6,i4,18i4,l)', 'recv qqqq',rank_world,rank_realm,rank_olap,ndata,nbr, & !\t\t\t\t\t\t\trank_graph2rank_world(sourceid+1),size(arecv),start_address,& !\t\t\t\t\t\t\ticlmin,iclmax,jclmin,jclmax,kclmin,kclmax, & !\t\t\t\t\t\t\tportion,extents,recv_flag do kcell = kclmin , kclmax do jcell = jclmin , jclmax do icell = iclmin , iclmax do n = 1 , npercell arecv ( n , icell , jcell , kcell ) = vbuf ( pos ) !print'(6i5,f15.5)', rank_world,pos,n,icell,jcell,kcell,vbuf(pos) pos = pos + 1 end do end do end do end do endif !Increment reading position of packed data start_address = start_address + ndata enddo ! ----------------- Unpack data ----------------------------- end subroutine CPL_recv_xd !------------------------------------------------------------------- subroutine CPL_pack ( unpacked , packed , realm , icmax_pack , icmin_pack , jcmax_pack , & jcmin_pack , kcmax_pack , kcmin_pack ) use coupler_module , only : CPL_CART_COMM , rank_cart , md_realm , cfd_realm , & error_abort , CPL_GRAPH_COMM , myid_graph , realm_name implicit none integer , intent ( in ) :: realm real ( kind = kind ( 0. d0 )), dimension (:,:,:,:), intent ( in ) :: unpacked real ( kind = kind ( 0. d0 )), dimension (:), allocatable , intent ( out ) :: packed ! Optional minimum and maximum values to pack integer , intent ( in ), optional :: icmax_pack , icmin_pack integer , intent ( in ), optional :: jcmax_pack , jcmin_pack integer , intent ( in ), optional :: kcmax_pack , kcmin_pack integer :: pos , n , nbr , id_nbr , icell , jcell , kcell , ierr integer :: npercell , ncells , nneighbors integer , dimension ( 3 ) :: coord integer , dimension ( 6 ) :: extents , gextents integer , dimension (:), allocatable :: id_neighbors !Amount of data per cell npercell = size ( unpacked , 1 ) !Allocate packing buffer if ( allocated ( packed )) deallocate ( packed ) allocate ( packed ( size ( unpacked ))) ! Get neighbour topology to determine ordering of packed data call MPI_Graph_neighbors_count ( CPL_GRAPH_COMM , myid_graph , nneighbors , ierr ) allocate ( id_neighbors ( nneighbors )) call MPI_Graph_neighbors ( CPL_GRAPH_COMM , myid_graph , nneighbors , id_neighbors , ierr ) ! Loop through all neighbours which will be sent to and order the data ! appropriately to send each correctly do nbr = 1 , nneighbors if ( realm .eq. cfd_realm ) then ! Get MD neighbour id_nbr = id_neighbors ( nbr ) call CPL_Cart_coords ( CPL_GRAPH_COMM , id_nbr + 1 , md_realm , 3 , coord , ierr ) call CPL_olap_extents ( coord , md_realm , extents , ncells ) ! Get offset of neighbouring processor pos = id_nbr * npercell * ncells !print*,'Pack',rank_cart,realm_name(realm),coord,nbr,id_nbr,extents,coord elseif ( realm .eq. md_realm ) then !Get own processor coordinates call CPL_Cart_coords ( CPL_CART_COMM , rank_cart , realm , 3 , coord , ierr ) call CPL_olap_extents ( coord , realm , gextents , ncells ) ! Get local extents extents ( 1 ) = 1 ; extents ( 2 ) = gextents ( 2 ) - gextents ( 1 ) extents ( 3 ) = 1 ; extents ( 4 ) = gextents ( 4 ) - gextents ( 3 ) extents ( 5 ) = 1 ; extents ( 6 ) = gextents ( 6 ) - gextents ( 5 ) pos = 1 endif ! Pack array into buffer do kcell = extents ( 5 ), extents ( 6 ) do jcell = extents ( 3 ), extents ( 4 ) do icell = extents ( 1 ), extents ( 2 ) do n = 1 , npercell packed ( pos ) = unpacked ( n , icell , jcell , kcell ) pos = pos + 1 end do end do end do end do end do !Sanity check if ( size ( packed ) .ne. npercell * ncells ) then !print*, 'data size', size(packed), 'expected size', npercell*ncells call error_abort ( \"CPL_pack error - cell array does not match expected extents\" ) endif end subroutine CPL_pack !------------------------------------------------------------------- subroutine CPL_unpack ( packed , unpacked , realm ) use coupler_module , only : CPL_CART_COMM , rank_cart , md_realm , cfd_realm , & error_abort , CPL_GRAPH_COMM , myid_graph implicit none integer , intent ( in ) :: realm real ( kind = kind ( 0. d0 )), dimension (:,:,:,:), allocatable , intent ( out ) :: unpacked real ( kind = kind ( 0. d0 )), dimension (:), allocatable , intent ( inout ) :: packed integer :: pos , n , nbr , id_nbr , icell , jcell , kcell , ierr integer :: npercell , ncells , nneighbors integer , dimension ( 3 ) :: coord integer , dimension ( 6 ) :: extents , gextents integer , dimension (:), allocatable :: id_neighbors call CPL_Cart_coords ( CPL_CART_COMM , rank_cart , realm , 3 , coord , ierr ) call CPL_proc_extents ( coord , realm , extents , ncells ) !Amount of data per cell npercell = size ( packed ) / ncells !Allocate packing buffer if ( allocated ( unpacked )) deallocate ( unpacked ) allocate ( unpacked ( npercell , 1 : extents ( 2 ) - extents ( 1 ), & 1 : extents ( 4 ) - extents ( 3 ), & 1 : extents ( 6 ) - extents ( 5 ))) ! Get neighbour topology to determine ordering of packed data call MPI_Graph_neighbors_count ( CPL_GRAPH_COMM , myid_graph , nneighbors , ierr ) allocate ( id_neighbors ( nneighbors )) call MPI_Graph_neighbors ( CPL_GRAPH_COMM , myid_graph , nneighbors , id_neighbors , ierr ) ! Loop through all neighbours which will be sent to and order the data ! appropriately to send each correctly do nbr = 1 , nneighbors if ( realm .eq. cfd_realm ) then ! Get MD neighbour id_nbr = id_neighbors ( nbr ) call CPL_Cart_coords ( CPL_GRAPH_COMM , id_nbr , md_realm , 3 , coord , ierr ) call CPL_proc_extents ( coord , md_realm , extents , ncells ) ! Get offset of neighbouring processor pos = id_nbr * npercell * ncells !ASSUMES all ncell the same!! elseif ( realm .eq. md_realm ) then !Get own processor coordinates call CPL_Cart_coords ( CPL_CART_COMM , rank_cart , realm , 3 , coord , ierr ) call CPL_proc_extents ( coord , realm , gextents , ncells ) ! Get local extents extents ( 1 ) = 1 ; extents ( 2 ) = gextents ( 2 ) - gextents ( 1 ) extents ( 3 ) = 1 ; extents ( 4 ) = gextents ( 4 ) - gextents ( 3 ) extents ( 5 ) = 1 ; extents ( 6 ) = gextents ( 6 ) - gextents ( 5 ) pos = 1 endif ! Unpack buffer into array do kcell = extents ( 5 ), extents ( 6 ) do jcell = extents ( 3 ), extents ( 4 ) do icell = extents ( 1 ), extents ( 2 ) do n = 1 , npercell unpacked ( n , icell , jcell , kcell ) = packed ( pos ) pos = pos + 1 end do end do end do end do end do !Deallocate packed buffer deallocate ( packed ) end subroutine CPL_unpack !------------------------------------------------------------------- ! \t\t\t\t\tCPL_proc_extents  \t\t\t      - !------------------------------------------------------------------- !> !! !! Gets maximum and minimum cells for processor coordinates !! !! - Synopsis !! !!  - CPL_proc_extents(coord,realm,extents,ncells) !! !! - Input !! !!  - coord !!   - processor cartesian coordinate (3 x integer) !! !!  - realm !!   - cfd_realm (1) or md_realm (2) (integer) !! !! - Input/Output !!  - NONE !! !! - Output !! !!  - extents !!   - Six components array which defines processor extents !!     xmin,xmax,ymin,ymax,zmin,zmax (6 x integer) !! !!  - ncells (optional) !!   - number of cells on processor (integer) !! !! @author David Trevelyan subroutine CPL_proc_extents ( coord , realm , extents , ncells ) use mpi use coupler_module , only : md_realm , cfd_realm , & icPmin_md , icPmax_md , & jcPmin_md , jcPmax_md , & kcPmin_md , kcPmax_md , & icPmin_cfd , icPmax_cfd , & jcPmin_cfd , jcPmax_cfd , & kcPmin_cfd , kcPmax_cfd , & error_abort implicit none integer , intent ( in ) :: coord ( 3 ), realm integer , intent ( out ) :: extents ( 6 ) integer , optional , intent ( out ) :: ncells select case ( realm ) case ( md_realm ) extents = ( / icPmin_md ( coord ( 1 )), icPmax_md ( coord ( 1 )), & jcPmin_md ( coord ( 2 )), jcPmax_md ( coord ( 2 )), & kcPmin_md ( coord ( 3 )), kcPmax_md ( coord ( 3 )) / ) case ( cfd_realm ) extents = ( / icPmin_cfd ( coord ( 1 )), icPmax_cfd ( coord ( 1 )), & jcPmin_cfd ( coord ( 2 )), jcPmax_cfd ( coord ( 2 )), & kcPmin_cfd ( coord ( 3 )), kcPmax_cfd ( coord ( 3 )) / ) case default call error_abort ( 'Wrong realm in CPL_proc_extents' ) end select if ( present ( ncells )) then ncells = ( extents ( 2 ) - extents ( 1 ) + 1 ) * & ( extents ( 4 ) - extents ( 3 ) + 1 ) * & ( extents ( 6 ) - extents ( 5 ) + 1 ) end if end subroutine CPL_proc_extents !------------------------------------------------------------------- ! \t\t\t\t\tCPL_olap_extents  \t\t\t      - !------------------------------------------------------------------- !> !! !! Get maximum and minimum cells for current communicator within !! the overlapping region only !! !! - Synopsis !! !!  - CPL_olap_extents(coord,realm,extents,ncells) !! !! - Input !! !!  - coord !!   - processor cartesian coordinate (3 x integer) !! !!  - realm !!   - cfd_realm (1) or md_realm (2) (integer) !! !! - Input/Output !!  - NONE !! !! - Output !! !!  - extents !!   - Six components array which defines processor extents within !!     the overlap region only: xmin,xmax,ymin,ymax,zmin,zmax (6 x integer) !! !!  - ncells (optional) !!   - number of cells on processor (integer) !! !! @author David Trevelyan subroutine CPL_olap_extents ( coord , realm , extents , ncells ) use mpi use coupler_module , only : md_realm , cfd_realm , & icPmin_md , icPmax_md , & jcPmin_md , jcPmax_md , & kcPmin_md , kcPmax_md , & icPmin_cfd , icPmax_cfd , & jcPmin_cfd , jcPmax_cfd , & kcPmin_cfd , kcPmax_cfd , & icmin_olap , icmax_olap , & jcmin_olap , jcmax_olap , & kcmin_olap , kcmax_olap , & error_abort implicit none integer , intent ( in ) :: coord ( 3 ), realm integer , intent ( out ) :: extents ( 6 ) integer , optional , intent ( out ) :: ncells select case ( realm ) case ( md_realm ) extents ( 1 ) = max ( icPmin_md ( coord ( 1 )), icmin_olap ) extents ( 2 ) = min ( icPmax_md ( coord ( 1 )), icmax_olap ) extents ( 3 ) = max ( jcPmin_md ( coord ( 2 )), jcmin_olap ) extents ( 4 ) = min ( jcPmax_md ( coord ( 2 )), jcmax_olap ) extents ( 5 ) = max ( kcPmin_md ( coord ( 3 )), kcmin_olap ) extents ( 6 ) = min ( kcPmax_md ( coord ( 3 )), kcmax_olap ) case ( cfd_realm ) extents ( 1 ) = max ( icPmin_cfd ( coord ( 1 )), icmin_olap ) extents ( 2 ) = min ( icPmax_cfd ( coord ( 1 )), icmax_olap ) extents ( 3 ) = max ( jcPmin_cfd ( coord ( 2 )), jcmin_olap ) extents ( 4 ) = min ( jcPmax_cfd ( coord ( 2 )), jcmax_olap ) extents ( 5 ) = max ( kcPmin_cfd ( coord ( 3 )), kcmin_olap ) extents ( 6 ) = min ( kcPmax_cfd ( coord ( 3 )), kcmax_olap ) case default call error_abort ( 'Wrong realm in CPL_olap_extents' ) end select if ( present ( ncells )) then ncells = ( extents ( 2 ) - extents ( 1 ) + 1 ) * & ( extents ( 4 ) - extents ( 3 ) + 1 ) * & ( extents ( 6 ) - extents ( 5 ) + 1 ) end if end subroutine CPL_olap_extents !------------------------------------------------------------------- ! \t\t\t\t\tCPL_proc_portion  \t\t\t      - !------------------------------------------------------------------- !> !! Get maximum and minimum cell indices, i.e. the 'portion', of the !! input cell extents 'limits' that is contributed by the current !! overlapping processor. !! !! - Synopsis !!  - CPL_proc_portion(coord,realm,limits,portion,ncells) !! !! - Input !! !!  - coord !!   - processor cartesian coordinate (3 x integer) !! !!  - realm !!   - cfd_realm (1) or md_realm (2) (integer) !! !!  - limits(6) !!   - Array of cell extents that specify the input region. !! !! - Input/Output !!  - NONE !! !! - Output !! !!  - portion(6) !!   - Array of cell extents that define the local processor's !!     contribution to the input region 'limits'. !! !!   - ncells (optional) !!    - number of cells in portion (integer) !! !! - Note: limits(6) and portion(6) are of the form: !!   (xmin,xmax,ymin,ymax,zmin,zmax) !! !! @author David Trevelyan subroutine CPL_proc_portion ( coord , realm , limits , portion , ncells ) use mpi use coupler_module , only : VOID implicit none integer , intent ( in ) :: coord ( 3 ), limits ( 6 ), realm integer , intent ( out ) :: portion ( 6 ) integer , optional , intent ( out ) :: ncells integer :: extents ( 6 ) call CPL_proc_extents ( coord , realm , extents ) if ( extents ( 1 ) .gt. limits ( 2 ) .or. & extents ( 2 ) .lt. limits ( 1 ) .or. & extents ( 3 ) .gt. limits ( 4 ) .or. & extents ( 4 ) .lt. limits ( 3 ) .or. & extents ( 5 ) .gt. limits ( 6 ) .or. & extents ( 6 ) .lt. limits ( 5 )) then portion (:) = VOID if ( present ( ncells )) ncells = 0 return end if portion ( 1 ) = max ( extents ( 1 ), limits ( 1 )) portion ( 2 ) = min ( extents ( 2 ), limits ( 2 )) portion ( 3 ) = max ( extents ( 3 ), limits ( 3 )) portion ( 4 ) = min ( extents ( 4 ), limits ( 4 )) portion ( 5 ) = max ( extents ( 5 ), limits ( 5 )) portion ( 6 ) = min ( extents ( 6 ), limits ( 6 )) if ( present ( ncells )) then ncells = ( portion ( 2 ) - portion ( 1 ) + 1 ) * & ( portion ( 4 ) - portion ( 3 ) + 1 ) * & ( portion ( 6 ) - portion ( 5 ) + 1 ) end if end subroutine CPL_proc_portion !------------------------------------------------------------------- ! \t\t\t\t\tCPL_Cart_coords\t\t\t\t\t\t\t\t   - !------------------------------------------------------------------- !> !! Determines process coords in appropriate realm's cartesian topology !! given a rank in any communicator !! !! - Synopsis !! !!  - CPL_Cart_coords(COMM, rank, realm, maxdims, coords, ierr) !! !! - Input Parameters !! !!  - comm !!   - communicator with cartesian structure (handle) !! !!  - realm !!   - cfd_realm (1) or md_realm (2) (integer) !! !!  - rank !!   - rank of a process within group of comm (integer) !!      NOTE fortran convention rank=1 to nproc !! !!  - maxdims !!   - length of vector coords in the calling program (integer) !! !! - Output Parameter !! !!  - coords !!   - integer array (of size ndims) containing the Cartesian coordinates !!     of specified process (integer) !! !!  - ierr !!   - error flag !! @author Edward Smith subroutine CPL_Cart_coords ( COMM , rank , realm , maxdims , coords , ierr ) use coupler_module , only : CPL_WORLD_COMM , CPL_REALM_COMM , CPL_INTER_COMM , & CPL_CART_COMM , CPL_OLAP_COMM , CPL_GRAPH_COMM , & CPL_REALM_INTERSECTION_COMM , md_realm , cfd_realm , & rank_world2rank_mdcart , & rank_world2rank_cfdcart , & rank_mdrealm2rank_world , & rank_cfdrealm2rank_world , & rank_olap2rank_world , & rank_graph2rank_world , & rank2coord_cfd , rank2coord_md , & COUPLER_ERROR_CART_COMM , VOID , nproc_cfd , nproc_md , & error_abort implicit none integer , intent ( in ) :: COMM , realm , rank , maxdims integer , intent ( out ) :: coords ( maxdims ), ierr integer :: worldrank , cartrank !Get rank in world COMM from current COMM if ( COMM .eq. CPL_WORLD_COMM ) then ! -  -  -  -  -  -  -  -  -  -  -  -  - worldrank = rank ! -  -  -  -  -  -  -  -  -  -  -  -  - elseif ( COMM .eq. CPL_REALM_COMM ) then ! -  -  -  -  -  -  -  -  -  -  -  -  - if ( realm .eq. cfd_realm ) then if ( allocated ( rank_cfdrealm2rank_world ) .eqv. .false. ) then call error_abort ( \"CPL_Cart_coords Error - \" // & \"Setup not complete for CFD CPL_REALM_COMM\" ) elseif ( rank .gt. size ( rank_cfdrealm2rank_world )) then print * , 'rank = ' , rank , 'comm size = ' , size ( rank_cfdrealm2rank_world ) call error_abort ( \"CPL_Cart_coords Error -Specified rank is not in CFD realm\" ) endif worldrank = rank_cfdrealm2rank_world ( rank ) elseif ( realm .eq. md_realm ) then if ( allocated ( rank_mdrealm2rank_world ) .eqv. .false. ) then call error_abort ( \"CPL_Cart_coords Error - Setup not \" // & \"complete for MD CPL_REALM_COMM\" ) elseif ( rank .gt. size ( rank_mdrealm2rank_world )) then print * , 'rank = ' , rank , 'comm size = ' , size ( rank_cfdrealm2rank_world ) call error_abort ( \"CPL_Cart_coords Error -Specified rank \" // & \"is not in MD realm\" ) endif worldrank = rank_mdrealm2rank_world ( rank ) endif ! -  -  -  -  -  -  -  -  -  -  -  -  - elseif ( COMM .eq. CPL_CART_COMM ) then ! -  -  -  -  -  -  -  -  -  -  -  -  - if ( realm .eq. cfd_realm ) then if ( allocated ( rank2coord_cfd ) .eqv. .false. ) & call error_abort ( \"CPL_Cart_coords Error - Setup not complete\" // & \" for CFD CPL_CART_COMM\" ) coords = rank2coord_cfd (:, rank ) elseif ( realm .eq. md_realm ) then if ( allocated ( rank2coord_md ) .eqv. .false. ) & call error_abort ( \"CPL_Cart_coords Error - Setup not complete \" // & \"for MD CPL_CART_COMM\" ) coords = rank2coord_md (:, rank ) endif return ! -  -  -  -  -  -  -  -  -  -  -  -  - elseif ( COMM .eq. CPL_OLAP_COMM ) then ! -  -  -  -  -  -  -  -  -  -  -  -  - if ( allocated ( rank_olap2rank_world ) .eqv. .false. ) then call error_abort ( \"CPL_Cart_coords Error - Setup not complete for CPL_OLAP_COMM\" ) elseif ( rank .gt. size ( rank_olap2rank_world )) then print * , 'rank = ' , rank , 'comm size = ' , size ( rank_olap2rank_world ) call error_abort ( \"CPL_Cart_coords Error - Specified rank is not in overlap\" ) endif worldrank = rank_olap2rank_world ( rank ) ! -  -  -  -  -  -  -  -  -  -  -  -  - elseif ( COMM .eq. CPL_GRAPH_COMM ) then ! -  -  -  -  -  -  -  -  -  -  -  -  - if ( allocated ( rank_graph2rank_world ) .eqv. .false. ) then call error_abort ( \"CPL_Cart_coords Error - Setup not complete for CPL_GRAPH_COMM\" ) elseif ( rank .gt. size ( rank_graph2rank_world )) then call error_abort ( \"CPL_Cart_coords Error - Specified rank is not in graph\" ) endif worldrank = rank_graph2rank_world ( rank ) ! -  -  -  -  -  -  -  -  -  -  -  -  - elseif ( COMM .eq. CPL_REALM_INTERSECTION_COMM ) then ! -  -  -  -  -  -  -  -  -  -  -  -  - call error_abort ( \"CPL_Cart_coords Error - Intersection COMM not programmed\" ) ! -  -  -  -  -  -  -  -  -  -  -  -  - elseif ( COMM .eq. CPL_INTER_COMM ) then ! -  -  -  -  -  -  -  -  -  -  -  -  - call error_abort ( \"CPL_Cart_coords Error - Intercomm between realms id\" // & \" - use realm comms instead\" ) ierr = COUPLER_ERROR_CART_COMM return ! -  -  -  -  -  -  -  -  -  -  -  -  - else ! -  -  -  -  -  -  -  -  -  -  -  -  - call error_abort ( \"CPL_Cart_coords Error - Unknown COMM\" ) ierr = COUPLER_ERROR_CART_COMM return ! -  -  -  -  -  -  -  -  -  -  -  -  - endif !Get rank in realm cartesian communicator if ( realm .eq. cfd_realm ) then if ( allocated ( rank_world2rank_cfdcart ) .eqv. .false. ) then call error_abort ( \"CPL_Cart_coords Error - world to cart mapping \" // & \"not initialised correctly\" ) endif cartrank = rank_world2rank_cfdcart ( worldrank ) if ( cartrank .eq. VOID ) call error_abort ( \"CPL_Cart_coords Error - void element in mapping\" ) if ( cartrank .gt. nproc_cfd ) call error_abort ( \"CPL_Cart_coords Error - rank not in cfd realm\" ) elseif ( realm .eq. md_realm ) then if ( allocated ( rank_world2rank_mdcart ) .eqv. .false. ) then call error_abort ( \"CPL_Cart_coords Error - world to cart \" // & \"mapping not initialised correctly\" ) endif cartrank = rank_world2rank_mdcart ( worldrank ) if ( cartrank .eq. VOID ) call error_abort ( \"CPL_Cart_coords Error - void element in mapping\" ) if ( cartrank .gt. nproc_md ) call error_abort ( \"CPL_Cart_coords Error - rank not in md realm\" ) endif !Get cartesian coordinate in appropriate realm if ( realm .eq. cfd_realm ) then coords = rank2coord_cfd (:, cartrank ) elseif ( realm .eq. md_realm ) then coords = rank2coord_md (:, cartrank ) endif !Success ierr = 0 end subroutine CPL_Cart_coords !------------------------------------------------------------------- ! \t\t\t\t\tCPL_get_rank\t\t\t\t\t   - !------------------------------------------------------------------- !> !! Return rank of current processor in specified COMM !! !! - Synopsis !! !!  - CPL_get_rank(COMM, rank) !! !! - Input Parameters !! !!  - comm !!   - communicator with cartesian structure (handle) !! !! - Output Parameter !! !!  - rank !!   - rank of a process within group of comm (integer) !!      NOTE fortran convention rank=1 to nproc !! !! @author Edward Smith subroutine CPL_get_rank ( COMM , rank ) use coupler_module , only : CPL_WORLD_COMM , CPL_REALM_COMM , CPL_INTER_COMM , & CPL_CART_COMM , CPL_OLAP_COMM , CPL_GRAPH_COMM , & CPL_REALM_INTERSECTION_COMM , rank_world , & rank_realm , rank_cart , rank_olap , rank_graph , error_abort integer , intent ( in ) :: COMM integer , intent ( out ) :: rank !Get rank in world COMM from current COMM if ( COMM .eq. CPL_WORLD_COMM ) then rank = rank_world return elseif ( COMM .eq. CPL_REALM_COMM ) then rank = rank_realm return elseif ( COMM .eq. CPL_CART_COMM ) then rank = rank_cart return elseif ( COMM .eq. CPL_OLAP_COMM ) then rank = rank_olap return elseif ( COMM .eq. CPL_GRAPH_COMM ) then rank = rank_graph return elseif ( COMM .eq. CPL_REALM_INTERSECTION_COMM ) then call error_abort ( \"CPL_Cart_coords Error - Intersection COMM not programmed\" ) elseif ( COMM .eq. CPL_INTER_COMM ) then call error_abort ( \"CPL_Cart_coords Error - No rank in Intercomm\" // & \" - use realm comms instead\" ) else call error_abort ( \"CPL_Cart_coords Error - Unknown COMM\" ) endif end subroutine CPL_get_rank !------------------------------------------------------------------- ! \t\t\t\t\tCPL_olap_check\t\t\t\t\t\t\t\t\t\t   - !------------------------------------------------------------------- !> !! Check if current processor is in the overlap region !! !! - Synopsis !! !!  - CPL_olap_check() !! !! - Input Parameters !! !!  - NONE !! !! - Returns !! !!  - CPL_olap_check !!\t - True if calling processor is in the overlap region !!     and false otherwise !! !! @author Edward Smith function CPL_overlap () result ( p ) use coupler_module , only : olap_mask , rank_world implicit none logical :: p p = olap_mask ( rank_world ) end function CPL_overlap !============================================================================ ! ! Utility functions and subroutines that extract parameters from internal modules ! !----------------------------------------------------------------------------- !------------------------------------------------------------------- ! \t\t\t\t\tCPL_get\t\t\t\t\t\t\t\t\t\t   - !------------------------------------------------------------------- !> !! Wrapper to retrieve (read only) parameters from the coupler_module !! Note - this ensures all variable in the coupler are protected !! from corruption by either CFD or MD codes !! !! - Synopsis !! !!  - CPL_get([see coupler_module]) !! !! - Input Parameters !! !!  - NONE !! !! - Output Parameter !! !!  - @see coupler_module !! !! @author Edward Smith subroutine CPL_get ( icmax_olap , icmin_olap , jcmax_olap , jcmin_olap , & kcmax_olap , kcmin_olap , density_cfd , density_md , & dt_cfd , dt_MD , dx , dy , dz , ncx , ncy , ncz , xg , yg , zg , & xL_md , xL_cfd , yL_md , yL_cfd , zL_md , zL_cfd , & constraint_algo , constraint_CVflag , & constraint_OT , constraint_NCER , & constraint_Flekkoy , constraint_off , & constraint_CV , & icmin_cnst , icmax_cnst , & jcmin_cnst , jcmax_cnst , & kcmin_cnst , kcmax_cnst , & md_cfd_match_cellsize , staggered_averages , & cpl_cfd_bc_slice , cpl_md_bc_slice , & cpl_cfd_bc_x , cpl_cfd_bc_y , cpl_cfd_bc_z , & timestep_ratio , comm_style ) use coupler_module , only : icmax_olap_ => icmax_olap , & icmin_olap_ => icmin_olap , & jcmax_olap_ => jcmax_olap , & jcmin_olap_ => jcmin_olap , & kcmax_olap_ => kcmax_olap , & kcmin_olap_ => kcmin_olap , & density_cfd_ => density_cfd , & density_md_ => density_md , & dt_cfd_ => dt_cfd , dt_MD_ => dt_MD , & dx_ => dx , dy_ => dy , dz_ => dz , & ncx_ => ncx , ncy_ => ncy , ncz_ => ncz , & xL_md_ => xL_md , & yL_md_ => yL_md , & zL_md_ => zL_md , & xL_cfd_ => xL_cfd , & yL_cfd_ => yL_cfd , & zL_cfd_ => zL_cfd , & xg_ => xg , yg_ => yg , zg_ => zg , & timestep_ratio_ => timestep_ratio , & md_cfd_match_cellsize_ => md_cfd_match_cellsize , & staggered_averages_ => staggered_averages , & constraint_algo_ => constraint_algo , & constraint_CVflag_ => constraint_CVflag , & constraint_OT_ => constraint_OT , & constraint_NCER_ => constraint_NCER , & constraint_Flekkoy_ => constraint_Flekkoy , & constraint_CV_ => constraint_CV , & constraint_off_ => constraint_off , & icmin_cnst_ => icmin_cnst , & icmax_cnst_ => icmax_cnst , & jcmin_cnst_ => jcmin_cnst , & jcmax_cnst_ => jcmax_cnst , & kcmin_cnst_ => kcmin_cnst , & kcmax_cnst_ => kcmax_cnst , & cpl_cfd_bc_slice_ => cpl_cfd_bc_slice , & cpl_md_bc_slice_ => cpl_md_bc_slice , & cpl_cfd_bc_x_ => cpl_cfd_bc_x , & cpl_cfd_bc_y_ => cpl_cfd_bc_y , & cpl_cfd_bc_z_ => cpl_cfd_bc_z , & comm_style_ => comm_style , & comm_style_send_recv_ => comm_style_send_recv , & comm_style_gath_scat_ => comm_style_gath_scat implicit none logical , dimension ( 3 ), optional , intent ( out ) :: staggered_averages integer , optional , intent ( out ) :: icmax_olap , icmin_olap integer , optional , intent ( out ) :: jcmax_olap , jcmin_olap integer , optional , intent ( out ) :: kcmax_olap , kcmin_olap integer , optional , intent ( out ) :: ncx , ncy , ncz integer , optional , intent ( out ) :: md_cfd_match_cellsize , timestep_ratio integer , optional , intent ( out ) :: constraint_algo integer , optional , intent ( out ) :: constraint_CVflag integer , optional , intent ( out ) :: constraint_OT integer , optional , intent ( out ) :: constraint_NCER integer , optional , intent ( out ) :: constraint_Flekkoy integer , optional , intent ( out ) :: constraint_CV integer , optional , intent ( out ) :: constraint_off integer , optional , intent ( out ) :: comm_style integer , optional , intent ( out ) :: icmax_cnst , icmin_cnst integer , optional , intent ( out ) :: jcmax_cnst , jcmin_cnst integer , optional , intent ( out ) :: kcmax_cnst , kcmin_cnst integer , optional , intent ( out ) :: cpl_cfd_bc_slice integer , optional , intent ( out ) :: cpl_cfd_bc_x integer , optional , intent ( out ) :: cpl_cfd_bc_y integer , optional , intent ( out ) :: cpl_cfd_bc_z integer , optional , intent ( out ) :: cpl_md_bc_slice real ( kind ( 0. d0 )), optional , intent ( out ) :: density_cfd , density_md real ( kind ( 0. d0 )), optional , intent ( out ) :: dt_cfd , dt_MD real ( kind ( 0. d0 )), optional , intent ( out ) :: dx , dy , dz real ( kind ( 0. d0 )), optional , intent ( out ) :: xL_md , xL_cfd real ( kind ( 0. d0 )), optional , intent ( out ) :: yL_md , yL_cfd real ( kind ( 0. d0 )), optional , intent ( out ) :: zL_md , zL_cfd real ( kind ( 0. d0 )), dimension (:,:), allocatable , optional , intent ( out ) :: xg , yg real ( kind ( 0. d0 )), dimension (:) , allocatable , optional , intent ( out ) :: zg !Overlap extents if ( present ( icmax_olap )) icmax_olap = icmax_olap_ if ( present ( icmin_olap )) icmin_olap = icmin_olap_ if ( present ( jcmax_olap )) jcmax_olap = jcmax_olap_ if ( present ( jcmin_olap )) jcmin_olap = jcmin_olap_ if ( present ( kcmax_olap )) kcmax_olap = kcmax_olap_ if ( present ( kcmin_olap )) kcmin_olap = kcmin_olap_ !Density if ( present ( density_cfd )) density_cfd = density_cfd_ if ( present ( density_md )) density_md = density_md_ !Cells if ( present ( ncx )) ncx = ncx_ if ( present ( ncy )) ncy = ncy_ if ( present ( ncz )) ncz = ncz_ !Temporal and spatial steps if ( present ( dt_cfd )) dt_cfd = dt_cfd_ if ( present ( dt_MD )) dt_MD = dt_MD_ if ( present ( dx )) dx = dx_ if ( present ( dy )) dy = dy_ if ( present ( dz )) dz = dz_ !Domain sizes if ( present ( xL_md )) xL_md = xL_md_ if ( present ( xL_cfd )) xL_cfd = xL_cfd_ if ( present ( yL_md )) yL_md = yL_md_ if ( present ( yL_cfd )) yL_cfd = yL_cfd_ if ( present ( zL_md )) zL_md = zL_md_ if ( present ( zL_cfd )) zL_cfd = zL_cfd_ !The mesh if ( present ( xg )) then allocate ( xg ( size ( xg_ , 1 ), size ( xg_ , 2 ))) xg = xg_ endif if ( present ( yg )) then allocate ( yg ( size ( yg_ , 1 ), size ( yg_ , 2 ))) yg = yg_ endif if ( present ( zg )) then allocate ( zg ( size ( zg_ , 1 ))) zg = zg_ endif !Coupler input parameters if ( present ( staggered_averages )) staggered_averages = staggered_averages_ if ( present ( timestep_ratio )) timestep_ratio = timestep_ratio_ if ( present ( md_cfd_match_cellsize )) then md_cfd_match_cellsize = md_cfd_match_cellsize_ end if ! Constraint information if ( present ( constraint_algo )) constraint_algo = constraint_algo_ if ( present ( constraint_CVflag )) constraint_CVflag = constraint_CVflag_ if ( present ( constraint_OT )) constraint_OT = constraint_OT_ if ( present ( constraint_NCER )) constraint_NCER = constraint_NCER_ if ( present ( constraint_Flekkoy )) constraint_Flekkoy = constraint_Flekkoy_ if ( present ( constraint_CV )) constraint_CV = constraint_CV_ if ( present ( constraint_off )) constraint_off = constraint_off_ if ( present ( icmin_cnst )) icmin_cnst = icmin_cnst_ if ( present ( icmax_cnst )) icmax_cnst = icmax_cnst_ if ( present ( jcmin_cnst )) jcmin_cnst = jcmin_cnst_ if ( present ( jcmax_cnst )) jcmax_cnst = jcmax_cnst_ if ( present ( kcmin_cnst )) kcmin_cnst = kcmin_cnst_ if ( present ( kcmax_cnst )) kcmax_cnst = kcmax_cnst_ ! Communication style if ( present ( comm_style )) comm_style = comm_style_ ! Coupling styles if ( present ( cpl_cfd_bc_slice )) cpl_cfd_bc_slice = cpl_cfd_bc_slice_ if ( present ( cpl_md_bc_slice )) cpl_md_bc_slice = cpl_md_bc_slice_ if ( present ( cpl_cfd_bc_x )) cpl_cfd_bc_x = cpl_cfd_bc_x_ if ( present ( cpl_cfd_bc_y )) cpl_cfd_bc_y = cpl_cfd_bc_y_ if ( present ( cpl_cfd_bc_z )) cpl_cfd_bc_z = cpl_cfd_bc_z_ end subroutine CPL_get function CPL_comm_style () use coupler_module , only : comm_style implicit none integer :: CPL_comm_style CPL_comm_style = comm_style end function ! Function to get cuurent realm function CPL_realm () use coupler_module , only : realm implicit none integer :: CPL_realm CPL_realm = realm end function !============================================================================= !> Get molecule's global position from position local to processor. !----------------------------------------------------------------------------- function globalise ( r ) result ( rg ) use coupler_module , only : xLl , iblock_realm , npx_md , & yLl , jblock_realm , npy_md , & zLl , kblock_realm , npz_md implicit none real ( kind ( 0. d0 )), intent ( in ) :: r ( 3 ) real ( kind ( 0. d0 )) :: rg ( 3 ) rg ( 1 ) = r ( 1 ) - 0.5 d0 * xLl * ( npx_md - 1 ) + xLl * ( iblock_realm - 1 ) rg ( 2 ) = r ( 2 ) - 0.5 d0 * yLl * ( npy_md - 1 ) + yLl * ( jblock_realm - 1 ) rg ( 3 ) = r ( 3 ) - 0.5 d0 * zLl * ( npz_md - 1 ) + zLl * ( kblock_realm - 1 ) end function globalise !============================================================================= !> Get local position on processor from molecule's global position. !----------------------------------------------------------------------------- function localise ( r ) result ( rg ) use coupler_module , only : xLl , iblock_realm , npx_md , & yLl , jblock_realm , npy_md , & zLl , kblock_realm , npz_md implicit none real ( kind ( 0. d0 )), intent ( in ) :: r ( 3 ) real ( kind ( 0. d0 )) rg ( 3 ) !Global domain has origin at centre rg ( 1 ) = r ( 1 ) - xLl * ( iblock_realm - 1 ) + 0.5 d0 * xLl * ( npx_md - 1 ) rg ( 2 ) = r ( 2 ) - yLl * ( jblock_realm - 1 ) + 0.5 d0 * yLl * ( npy_md - 1 ) rg ( 3 ) = r ( 3 ) - zLl * ( kblock_realm - 1 ) + 0.5 d0 * zLl * ( npz_md - 1 ) end function localise !============================================================================= !> Map global MD position to global CFD coordinate frame !----------------------------------------------------------------------------- function map_md2cfd_global ( r ) result ( rg ) use coupler_module , only : xL_md , xg , icmin_olap , icmax_olap , & yL_md , yg , jcmin_olap , jcmax_olap , & zL_md , zg , kcmin_olap , kcmax_olap implicit none real ( kind ( 0. d0 )), intent ( in ) :: r ( 3 ) real ( kind ( 0. d0 )) :: md_only ( 3 ), rg ( 3 ) !Get size of MD domain which has no CFD cells overlapping !This should be general enough to include grid stretching !and total overlap in any directions md_only ( 1 ) = xL_md - ( xg ( icmax_olap + 1 , 1 ) - xg ( icmin_olap , 1 )) md_only ( 2 ) = yL_md - ( yg ( 1 , jcmax_olap + 1 ) - yg ( 1 , jcmin_olap )) md_only ( 3 ) = zL_md - ( zg ( kcmax_olap + 1 ) - zg ( kcmin_olap )) ! CFD has origin at bottom left while MD origin at centre rg ( 1 ) = r ( 1 ) + 0.5 d0 * xL_md - md_only ( 1 ) rg ( 2 ) = r ( 2 ) + 0.5 d0 * yL_md - md_only ( 2 ) rg ( 3 ) = r ( 3 ) + 0.5 d0 * zL_md - md_only ( 3 ) end function map_md2cfd_global !============================================================================= !> Map global CFD position in global MD coordinate frame !----------------------------------------------------------------------------- function map_cfd2md_global ( r ) result ( rg ) use coupler_module , only : xL_md , xg , icmin_olap , icmax_olap , & yL_md , yg , jcmin_olap , jcmax_olap , & zL_md , zg , kcmin_olap , kcmax_olap implicit none real ( kind ( 0. d0 )), intent ( in ) :: r ( 3 ) real ( kind ( 0. d0 )) :: md_only ( 3 ), rg ( 3 ) !Get size of MD domain which has no CFD cells overlapping !This should be general enough to include grid stretching !and total overlap in any directions md_only ( 1 ) = xL_md - ( xg ( icmax_olap + 1 , 1 ) - xg ( icmin_olap , 1 )) md_only ( 2 ) = yL_md - ( yg ( 1 , jcmax_olap + 1 ) - yg ( 1 , jcmin_olap )) md_only ( 3 ) = zL_md - ( zg ( kcmax_olap + 1 ) - zg ( kcmin_olap )) ! CFD has origin at bottom left while MD origin at centre rg ( 1 ) = r ( 1 ) - 0.5 d0 * xL_md + md_only ( 1 ) rg ( 2 ) = r ( 2 ) - 0.5 d0 * yL_md + md_only ( 2 ) rg ( 3 ) = r ( 3 ) - 0.5 d0 * zL_md + md_only ( 3 ) end function map_cfd2md_global !----------------------------------------------------------------------------- function coupler_md_get_save_period () result ( p ) use coupler_module , only : save_period implicit none integer p p = save_period end function coupler_md_get_save_period !----------------------------------------------------------------------------- function coupler_md_get_average_period () result ( p ) use coupler_module , only : average_period implicit none integer p p = average_period end function coupler_md_get_average_period !----------------------------------------------------------------------------- function coupler_md_get_md_steps_per_cfd_dt () result ( p ) use coupler_module , only : timestep_ratio implicit none integer p p = timestep_ratio end function coupler_md_get_md_steps_per_cfd_dt !----------------------------------------------------------------------------- function coupler_md_get_nsteps () result ( p ) use coupler_module , only : nsteps_md implicit none integer p p = nsteps_md end function coupler_md_get_nsteps !----------------------------------------------------------------------------- function coupler_md_get_dt_cfd () result ( p ) use coupler_module , only : dt_CFD implicit none real ( kind = kind ( 0. d0 )) p p = dt_CFD end function coupler_md_get_dt_cfd !------------------------------------------------------------------------------ end module coupler © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"sourcefile/coupler.f90.html","title":"coupler.f90 – Fortran Program"},{"text":"coupler_module.f90 Source File Source File coupler_module.f90 Modules coupler_module Source Code coupler_module.f90 All Source Files coupler.f90 coupler_module.f90 COUPLER MODULE: \n A single coupler module for both codes - this contains the same information \n on both md and cfd side Error handling MPI communicators Simulation realms MPI processor IDs Processor topologies Processor cartesian coords Global cell grid parameters Processor cell ranges Domain and cell dimensions Positions of CFD grid lines CFD to MD processor mapping Simulation parameters The data is protected so only setup routines in this module can change it Setup routines which have access to coupler parameters CPL_create_comm          (cfd+md)   splits MPI_COMM_WORLD, create inter - \n                                       communicator between CFD and MD CPL_create_map           (cfd+md)   creates correspondence maps between \n                                       the CFD grid and MD domains CPL_cfd_adjust_domain     (cfd)     adjust CFD tomain to an integer number \n                                       FCC or similar MD initial layout @author David Trevelyan, Edward Smith\n @see coupler Source Code ! !    ________/\\\\\\\\\\\\\\\\\\__/\\\\\\\\\\\\\\\\\\\\\\\\\\____/\\\\\\_____________ !     _____/\\\\\\////////__\\/\\\\\\/////////\\\\\\_\\/\\\\\\_____________ !      ___/\\\\\\/___________\\/\\\\\\_______\\/\\\\\\_\\/\\\\\\_____________ !       __/\\\\\\_____________\\/\\\\\\\\\\\\\\\\\\\\\\\\\\/__\\/\\\\\\_____________ !        _\\/\\\\\\_____________\\/\\\\\\/////////____\\/\\\\\\_____________ !         _\\//\\\\\\____________\\/\\\\\\_____________\\/\\\\\\_____________ !          __\\///\\\\\\__________\\/\\\\\\_____________\\/\\\\\\_____________ !           ____\\////\\\\\\\\\\\\\\\\\\_\\/\\\\\\_____________\\/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\_ !            _______\\/////////__\\///______________\\///////////////__ ! ! !                         C P L  -  L I B R A R Y ! !           Copyright (C) 2012-2015 Edward Smith & David Trevelyan ! !License ! !    This file is part of CPL-Library. ! !    CPL-Library is free software: you can redistribute it and/or modify !    it under the terms of the GNU General Public License as published by !    the Free Software Foundation, either version 3 of the License, or !    (at your option) any later version. ! !    CPL-Library is distributed in the hope that it will be useful, !    but WITHOUT ANY WARRANTY; without even the implied warranty of !    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the !    GNU General Public License for more details. ! !    You should have received a copy of the GNU General Public License !    along with CPL-Library.  If not, see <http://www.gnu.org/licenses/>. ! ! !Description ! ! !! COUPLER MODULE: !! A single coupler module for both codes - this contains the same information !! on both md and cfd side !! !!  - Error handling !!  - MPI communicators !!  - Simulation realms !!  - MPI processor IDs !!  - Processor topologies !!  - Processor cartesian coords !!  - Global cell grid parameters !!  - Processor cell ranges !!  - Domain and cell dimensions !!  - Positions of CFD grid lines !!  - CFD to MD processor mapping !!  - Simulation parameters !! !! The data is protected so only setup routines in this module can change it ! ! ! !  SETUP SETUP SETUP SETUP SETUP SETUP SETUP SETUP SETUP SETUP SETUP SETUP !  ----------------------------------------------------------------------- ! !! Setup routines which have access to coupler parameters !! !! - CPL_create_comm          (cfd+md)   splits MPI_COMM_WORLD, create inter - !!                                       communicator between CFD and MD !! !! - CPL_create_map           (cfd+md)   creates correspondence maps between !!                                       the CFD grid and MD domains !! !! - CPL_cfd_adjust_domain     (cfd)     adjust CFD tomain to an integer number !!                                       FCC or similar MD initial layout ! !! @author David Trevelyan, Edward Smith !! @see coupler !============================================================================= module coupler_module USE ISO_C_BINDING implicit none integer , parameter :: VOID =- 666 !!VOID value for data initialisation integer , parameter :: cfd_realm = 1 !! CFD realm identifier integer , parameter :: md_realm = 2 !! MD realm identifier character ( len =* ), parameter :: & realm_name ( 2 ) = ( / \"CFD\" , \"MD \" / ) !! Used with realm identifier to get name !! error codes integer , parameter :: & COUPLER_ERROR_REALM = 1 !! wrong realm value integer , parameter :: & COUPLER_ERROR_ONE_REALM = 2 !! one realm missing integer , parameter :: & COUPLER_ERROR_INIT = 3 !! initialisation error integer , parameter :: & COUPLER_ERROR_INPUT_FILE = 4 !! wrong value in input file integer , parameter :: & COUPLER_ERROR_READ_INPUT = 5 !! error in processing input file or data transfers integer , parameter :: & COUPLER_ERROR_CONTINUUM_FORCE = 6 !!the region in which the continuum constrain force is apply spans over two MD domains integer , parameter :: & COUPLER_ABORT_ON_REQUEST = 7 !! used in request_abort integer , parameter :: & COUPLER_ABORT_SEND_CFD = 8 !! error in coupler_cfd_send integer , parameter :: & COUPLER_ERROR_CART_COMM = 9 !! Wrong comm value in CPL_Cart_coords !! MPI error flag integer :: ierr ! MPI Communicators integer , protected :: & CPL_WORLD_COMM !! Copy of MPI_COMM_WORLD, both CFD and MD realms; integer , protected :: & CPL_REALM_COMM !! INTRA communicators within MD/CFD realms; integer , protected :: & CPL_INTER_COMM !!  CFD/MD INTER communicator between realm comms; integer , protected :: & CPL_CART_COMM !!  Comm w/cartesian topology for each realm; integer , protected :: & CPL_OLAP_COMM !!  Local comm between only overlapping MD/CFD procs; integer , protected :: & CPL_GRAPH_COMM !!  Comm w/ graph topolgy between locally olapg procs; integer , protected :: & CPL_REALM_INTERSECTION_COMM !!  Intersecting MD/CFD procs in world; !! Simulation realms integer , protected :: & realm ! MPI processor IDs integer , protected :: & myid_world !! Processor ID from 0 to nproc_world-1; integer , protected :: & rank_world !! Processor rank from 1 to nproc_world; integer , protected :: & rootid_world !! Root processor in world; integer , protected :: & myid_realm !! Processor ID from 0 to nproc_realm-1; integer , protected :: & rank_realm !! Processor rank from 1 to nproc_realm; integer , protected :: & rootid_realm !! Root processor in each realm; integer , protected :: & myid_cart !! Processor ID from 0 to nproc_cart-1; integer , protected :: & rank_cart !! Processor rank from 1 to nproc_cart; integer , protected :: & rootid_cart !! Root processor in each cart topology; integer , protected :: & myid_olap !! Processor ID from 0 to nproc_olap-1; integer , protected :: & rank_olap !! Processor rank from 1 to nproc_olap; integer , protected :: & CFDid_olap !! Root processor in overlap is the CFD processor; integer , protected :: & myid_graph !! Processor ID from 0 to nproc_graph-1; integer , protected :: & rank_graph !! Processor rank from 1 to nproc_graph; !! Get rank in CPL_world_COMM from rank in local COMM integer , protected , dimension (:), allocatable :: & rank_world2rank_mdrealm , & rank_world2rank_mdcart , & rank_world2rank_cfdrealm , & rank_world2rank_cfdcart , & rank_world2rank_olap , & rank_world2rank_graph , & rank_world2rank_inter !! Get rank in local COMM from rank in CPL_world_COMM integer , protected , dimension (:), allocatable :: & rank_mdrealm2rank_world , & rank_mdcart2rank_world , & rank_cfdrealm2rank_world , & rank_cfdcart2rank_world , & rank_olap2rank_world , & rank_graph2rank_world , & rank_inter2rank_world , & rank_olap2rank_realm ! Processor topologies integer , protected :: & nproc_md !! Total number of processor in md integer , protected :: & nproc_cfd !! Total number of processor in cfd integer , protected :: & nproc_olap !! Total number of processor in overlap region integer , protected :: & nproc_world !! Total number of processor in world integer , protected :: & npx_md !! Number of processor in x in the md integer , protected :: & npy_md !! Number of processor in y in the md integer , protected :: & npz_md !! Number of processor in z in the md integer , protected :: & npx_cfd !! Number of processor in x in the cfd integer , protected :: & npy_cfd !! Number of processor in y in the cfd integer , protected :: & npz_cfd !! Number of processor in z in the cfd logical , protected , dimension (:), allocatable :: & olap_mask !! Overlap mask specifying which processors overlap using world ranks integer , protected , dimension (:,:), allocatable :: & rank2coord_cfd , & !! Array containing coordinates for each cartesian rank rank2coord_md integer , protected , dimension (:,:,:), allocatable :: & coord2rank_cfd , & coord2rank_md !! Processor cartesian coords integer , protected :: & iblock_realm , & jblock_realm , & kblock_realm ! Global cell grid parameters integer , protected :: & ncx , & ! Number of cells in domain ncy , & ncz , & icmin , & ! Domain cell extents icmax , & jcmin , & jcmax , & kcmin , & kcmax , & icmin_olap , & ! Overlap region cell extents icmax_olap , & jcmin_olap , & jcmax_olap , & kcmin_olap , & kcmax_olap , & ncx_olap , & ! Number of cells in overlap region ncy_olap , & ncz_olap ! Constrained dynamics region flags and params integer , protected :: & constraint_algo , & constraint_CVflag ,& icmin_cnst , & ! Constrained dynamics region cell extents icmax_cnst , & jcmin_cnst , & jcmax_cnst , & kcmin_cnst , & kcmax_cnst ! Coupling CFD boundary condition direction flags integer , protected :: & cpl_cfd_bc_x , & cpl_cfd_bc_y , & cpl_cfd_bc_z ! Coupling constrained regions, average MD quantities ! in spanwise direction (flags) integer , protected :: & cpl_md_bc_slice , & cpl_cfd_bc_slice ! (average MD values, not CFD) ! Constraint parameters integer , parameter :: & constraint_off = 0 , & constraint_OT = 1 , & constraint_NCER = 2 , & constraint_Flekkoy = 3 , & constraint_CV = 4 ! Processor cell ranges integer , protected , dimension (:), allocatable :: & icPmin_md , & icPmax_md , & jcPmin_md , & jcPmax_md , & kcPmin_md , & kcPmax_md , & icPmin_cfd , & icPmax_cfd , & jcPmin_cfd , & jcPmax_cfd , & kcPmin_cfd , & kcPmax_cfd ! Domain and cell dimensions real ( kind ( 0. d0 )), protected :: & xL_md , & yL_md , & zL_md , & xL_cfd , & yL_cfd , & zL_cfd , & xL_olap , & yL_olap , & zL_olap , & xLl , & yLl , & zLl , & dx , & dy , & dz , & dymin , & dymax ! Positions of CFD grid lines real ( kind ( 0. d0 )), protected , dimension (:,:), allocatable , target :: & xg , & yg real ( kind ( 0. d0 )), protected , dimension (:) , allocatable , target :: & zg ! CFD to MD processor mapping integer , protected , dimension (:,:), allocatable :: & cfd_icoord2olap_md_icoords , & cfd_jcoord2olap_md_jcoords , & cfd_kcoord2olap_md_kcoords ! Simulation parameters integer , protected :: & nsteps_md , & !MD input steps nsteps_cfd , & !CFD input steps nsteps_coupled , & !Total number of steps for coupled simulation average_period = 1 , & ! average period for averages ( it must come from CFD !!!) save_period = 10 ! save period (corresponts to tplot in CFD, revise please !!!) real ( kind ( 0. d0 )), protected :: & dt_md , & dt_cfd , & density_md , & density_cfd integer , protected :: & timestep_ratio , & md_cfd_match_cellsize , & testval logical , protected :: & staggered_averages ( 3 ) = ( / .false. , .false. , .false. / ) ! Communications style integer , protected :: & comm_style integer , parameter :: & comm_style_send_recv = 0 , & comm_style_gath_scat = 1 !Interface for error handling functions interface error_abort module procedure error_abort_s , error_abort_si end interface error_abort private error_abort_si , error_abort_s contains !============================================================================= !                    _____      _ !                   /  ___|    | | !                   \\ `--.  ___| |_ _   _ _ __ !                    `--. \\/ _ \\ __| | | | '_ \\ !                   /\\__/ /  __/ |_| |_| | |_) | !                   \\____/ \\___|\\__|\\__,_| .__/ !                                        | | !                                        |_| !============================================================================= !============================================================================= !                           coupler_create_comm !! (cfd+md) Splits MPI_COMM_WORLD in both the CFD and MD code respectively !!         and create intercommunicator between CFD and MD !----------------------------------------------------------------------------- subroutine CPL_create_comm ( callingrealm , RETURNED_REALM_COMM , ierror ) use mpi !use coupler_module, only : rank_world,myid_world,rootid_world,nproc_world,& !                           realm, rank_realm,myid_realm,rootid_realm,ierr,& !                           CPL_WORLD_COMM, CPL_REALM_COMM, CPL_INTER_COMM implicit none integer , intent ( in ) :: callingrealm ! CFD or MD integer , intent ( out ) :: RETURNED_REALM_COMM , ierror !Get processor id in world across both realms call MPI_comm_rank ( MPI_COMM_WORLD , myid_world , ierr ) rank_world = myid_world + 1 ; rootid_world = 0 call MPI_comm_size ( MPI_COMM_WORLD , nproc_world , ierr ) if ( myid_world .eq. rootid_world ) call print_cplheader ! test if we have a CFD and a MD realm ierror = 0 ! Test realms are assigned correctly call test_realms ! Create intercommunicator between realms realm = callingrealm call create_comm contains !----------------------------------------------------------------------------- !   Test if CFD and MD realms are assigned correctly !----------------------------------------------------------------------------- subroutine print_cplheader implicit none print * , \"                                                                 \" print * , \"  ________/\\\\\\\\\\\\\\\\\\__/\\\\\\\\\\\\\\\\\\\\\\\\\\____/\\\\\\_____________        \" print * , \"   _____/\\\\\\////////__\\/\\\\\\/////////\\\\\\_\\/\\\\\\_____________       \" print * , \"    ___/\\\\\\/___________\\/\\\\\\_______\\/\\\\\\_\\/\\\\\\_____________      \" print * , \"     __/\\\\\\_____________\\/\\\\\\\\\\\\\\\\\\\\\\\\\\/__\\/\\\\\\_____________     \" print * , \"      _\\/\\\\\\_____________\\/\\\\\\/////////____\\/\\\\\\_____________    \" print * , \"       _\\//\\\\\\____________\\/\\\\\\_____________\\/\\\\\\_____________   \" print * , \"        __\\///\\\\\\__________\\/\\\\\\_____________\\/\\\\\\_____________  \" print * , \"         ____\\////\\\\\\\\\\\\\\\\\\_\\/\\\\\\_____________\\/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\_ \" print * , \"          _______\\/////////__\\///______________\\///////////////__\" print * , \"                                                                 \" print * , \"                     C P L  -  L I B R A R Y                     \" print * , \"                                                                 \" end subroutine print_cplheader subroutine test_realms implicit none integer :: i , root , nproc , ncfd , nmd integer , allocatable :: realm_list (:) ! Allocate and gather array with realm (MD or CFD) of each ! processor in the coupled domain on the root processor root = 1 if ( rank_world .eq. root ) then call MPI_comm_size ( MPI_comm_world , nproc , ierr ) allocate ( realm_list ( nproc )) else allocate ( realm_list ( 0 )) endif call MPI_gather ( callingrealm , 1 , MPI_INTEGER , realm_list , 1 , MPI_INTEGER , 0 , MPI_COMM_WORLD , ierr ) !Check through array of processors on both realms !and return error if wrong values or either is missing if ( rank_world .eq. root ) then ncfd = 0 ; nmd = 0 do i = 1 , nproc if ( realm_list ( i ) .eq. cfd_realm ) then ncfd = ncfd + 1 else if ( realm_list ( i ) .eq. md_realm ) then nmd = nmd + 1 else ierror = COUPLER_ERROR_REALM write ( * , * ) \"wrong realm value in coupler_create_comm\" call MPI_abort ( MPI_COMM_WORLD , ierror , ierr ) endif enddo if ( ncfd .eq. 0 .or. nmd .eq. 0 ) then ierror = COUPLER_ERROR_ONE_REALM write ( * , * ) \"CFD or MD realm is missing in MPI_COMM_WORLD\" call MPI_abort ( MPI_COMM_WORLD , ierror , ierr ) endif endif end subroutine test_realms !----------------------------------------------------------------------------- ! Create communicators for each realm and inter-communicator !----------------------------------------------------------------------------- subroutine create_comm implicit none integer :: callingrealm , ibuf ( 2 ), jbuf ( 2 ), remote_leader , comm_size callingrealm = realm ! Split MPI COMM WORLD ready to establish two communicators ! 1) A global intra-communicator in each realm for communication ! internally between CFD processes or between MD processes ! 2) An inter-communicator which allows communication between ! the 'groups' of processors in MD and the group in the CFD call MPI_comm_dup ( MPI_COMM_WORLD , CPL_WORLD_COMM , ierr ) RETURNED_REALM_COMM = MPI_COMM_NULL CPL_REALM_COMM = MPI_COMM_NULL !------------ create realm intra-communicators ----------------------- ! Split MPI_COMM_WORLD into an intra-communicator for each realm ! (used for any communication within each realm - e.g. broadcast from !  an md process to all other md processes) call MPI_comm_split ( CPL_WORLD_COMM , callingrealm , myid_world , RETURNED_REALM_COMM , ierr ) !------------ create realm inter-communicators ----------------------- ! Create intercommunicator between the group of processor on each realm ! (used for any communication between realms - e.g. md group rank 2 sends ! to cfd group rank 5). inter-communication is by a single processor on each group ! Split duplicate of MPI_COMM_WORLD call MPI_comm_split ( CPL_WORLD_COMM , callingrealm , myid_world , CPL_REALM_COMM , ierr ) call MPI_comm_rank ( CPL_REALM_COMM , myid_realm , ierr ) rank_realm = myid_realm + 1 ; rootid_realm = 0 ! Get the MPI_comm_world ranks that hold the largest ranks in cfd_comm and md_comm call MPI_comm_size ( CPL_REALM_COMM , comm_size , ierr ) ibuf (:) = - 1 jbuf (:) = - 1 if ( myid_realm .eq. comm_size - 1 ) then ibuf ( realm ) = myid_world endif call MPI_allreduce ( ibuf , jbuf , 2 , MPI_INTEGER , MPI_MAX , & CPL_WORLD_COMM , ierr ) !Set this largest rank on each process to be the inter-communicators (WHY NOT 0??) select case ( realm ) case ( cfd_realm ) remote_leader = jbuf ( md_realm ) case ( md_realm ) remote_leader = jbuf ( cfd_realm ) end select !print*,color, jbuf, remote_leader call MPI_intercomm_create ( CPL_REALM_COMM , comm_size - 1 , CPL_WORLD_COMM ,& remote_leader , 1 , CPL_INTER_COMM , ierr ) print * , 'Completed CPL communicator setup for ' , realm_name ( realm ), & ' , CPL_WORLD_COMM ID:' , myid_world end subroutine create_comm end subroutine CPL_create_comm !============================================================================= !! Read Coupler input file !----------------------------------------------------------------------------- subroutine read_coupler_input !use coupler_module implicit none integer :: infileid logical :: found !Check all file ids until an unused one is found infileid = 100000 do inquire ( unit = infileid , opened = found ) if ( .not. ( found )) exit infileid = infileid + 1 enddo !Open and read input file on all processes open ( infileid , file = 'cpl/COUPLER.in' , status = \"old\" , action = \"read\" , & form = \"formatted\" ) call locate ( infileid , 'DENSITY_CFD' , found ) if ( found ) then read ( infileid , * ) density_cfd else call error_abort ( \"Density not specified in coupler input file.\" ) end if call locate ( infileid , 'CONSTRAINT_INFO' , found ) if ( found ) then read ( infileid , * ) constraint_algo if ( constraint_algo .ne. 0 ) then read ( infileid , * ) constraint_CVflag read ( infileid , * ) icmin_cnst read ( infileid , * ) icmax_cnst read ( infileid , * ) jcmin_cnst read ( infileid , * ) jcmax_cnst read ( infileid , * ) kcmin_cnst read ( infileid , * ) kcmax_cnst endif else call error_abort ( \"CONSTRAINT_INFO not specified in coupler input file\" ) end if call locate ( infileid , 'OVERLAP_EXTENTS' , found ) if ( found ) then read ( infileid , * ) icmin_olap read ( infileid , * ) icmax_olap read ( infileid , * ) jcmin_olap read ( infileid , * ) jcmax_olap read ( infileid , * ) kcmin_olap read ( infileid , * ) kcmax_olap else call error_abort ( \"Ovelap extents unspecified in coupler input file.\" ) end if call locate ( infileid , 'TIMESTEP_RATIO' , found ) if ( found ) then read ( infileid , * ) timestep_ratio !TODO name change else timestep_ratio = VOID end if call locate ( infileid , 'MATCH_CELLSIZE' , found ) if ( found ) then read ( infileid , * ) md_cfd_match_cellsize else md_cfd_match_cellsize = 0 end if call locate ( infileid , 'CPL_CFD_BC_XYZ' , found ) if ( found ) then read ( infileid , * ) cpl_cfd_bc_x read ( infileid , * ) cpl_cfd_bc_y read ( infileid , * ) cpl_cfd_bc_z else cpl_cfd_bc_x = 1 cpl_cfd_bc_y = 0 cpl_cfd_bc_z = 1 end if call locate ( infileid , 'CPL_CFD_BC_SLICE' , found ) if ( found ) then read ( infileid , * ) cpl_cfd_bc_slice else cpl_cfd_bc_slice = 0 end if call locate ( infileid , 'CPL_MD_BC_SLICE' , found ) if ( found ) then read ( infileid , * ) cpl_md_bc_slice else cpl_md_bc_slice = 0 end if call locate ( infileid , 'COMM_STYLE' , found ) if ( found ) then read ( infileid , * ) comm_style else comm_style = comm_style_send_recv end if close ( infileid , status = \"keep\" ) if ( myid_world .eq. rootid_world ) then call CPL_write_header ( \"./cpl/coupler_header\" ) !todo better name/place end if end subroutine read_coupler_input !------------------------------------------------------------------------------ !                              CPL_write_header                               - !------------------------------------------------------------------------------ !> !! Writes header information to specified filename in the format !! Variable description ; variable name ; variable !! !! - Synopsis !! !!  - CPL_write_header (header_filename) !! !! - Input !! !!  - header_filename !!   - File name to write header to !! !! - Input/Output !!  - NONE !! !! - Output !!  - NONE !! !! @author Edward Smith ! ! ---------------------------------------------------------------------------- subroutine CPL_write_header ( header_filename ) implicit none character ( * ), intent ( in ) :: header_filename logical :: found integer :: infileid Character ( 8 ) :: the_date Character ( 10 ) :: the_time !Check all file ids until an unused one is found infileid = 100000 do inquire ( unit = infileid , opened = found ) if ( .not. ( found )) exit infileid = infileid + 1 enddo ! Write Simulation Parameter File contain all data required to completely recreate ! simulation and to be used for post processing call date_and_time ( the_date , the_time ) !Open and write input file on all processes !if (rank_world .eq. rootid_world+1) then open ( infileid , file = trim ( header_filename ), action = \"write\" , form = \"formatted\" ) write ( infileid , * ) 'Simulation run on Date;  sim_date ;' , the_date write ( infileid , * ) 'Simulation start time ;  sim_start_time ;' , the_time write ( infileid , * ) 'CFD density;  density_cfd ;' , density_cfd write ( infileid , * ) 'choice of constraint algorithm;  constraint_algo ;' , constraint_algo if ( constraint_algo .ne. 0 ) then write ( infileid , * ) 'CV form of constraint;  constraint_CVflag ;' , constraint_CVflag write ( infileid , * ) 'minimum x cell of constrained region;  icmin_cnst ;' , icmin_cnst write ( infileid , * ) 'maximum x cell of constrained region;  icmax_cnst ;' , icmax_cnst write ( infileid , * ) 'minimum y cell of constrained region;  jcmin_cnst ;' , jcmin_cnst write ( infileid , * ) 'maximum y cell of constrained region;  jcmax_cnst ;' , jcmax_cnst write ( infileid , * ) 'minimum z cell of constrained region;  kcmin_cnst ;' , kcmin_cnst write ( infileid , * ) 'maximum z cell of constrained region;  kcmax_cnst ;' , kcmax_cnst endif write ( infileid , * ) 'minimum x cell of overlap region;  icmin_olap ;' , icmin_olap write ( infileid , * ) 'maximum x cell of overlap region;  icmax_olap ;' , icmax_olap write ( infileid , * ) 'minimum y cell of overlap region;  jcmin_olap ;' , jcmin_olap write ( infileid , * ) 'maximum y cell of overlap region;  jcmax_olap ;' , jcmax_olap write ( infileid , * ) 'minimum z cell of overlap region;  kcmin_olap ;' , kcmin_olap write ( infileid , * ) 'maximum z cell of overlap region;  kcmax_olap ;' , kcmax_olap write ( infileid , * ) 'MD timesteps per CFD timestep;  timestep_ratio ;' , timestep_ratio !TODO name change write ( infileid , * ) 'Enforce cellsize matching;  md_cfd_match_cellsize ;' , md_cfd_match_cellsize close ( infileid , status = 'keep' ) !endif end subroutine CPL_write_header !------------------------------------------------------------------------------ !                              coupler_cfd_init                               - !------------------------------------------------------------------------------ !> !! Initialisation routine for coupler module - Every variable is sent and stored !! to ensure both md and cfd region have an identical list of parameters !! !! - Synopsis !! !!  - coupler_cfd_init(nsteps,dt_cfd,icomm_grid,icoord,npxyz_cfd,xyzL,ncxyz, !!                             density,ijkcmax,ijkcmin,iTmin,iTmax,jTmin, !!                             jTmax,kTmin,kTmax,xg,yg,zg) !! !! - Input !! !!  - nsteps !!   - Number of time steps the CFD code is expected to run for (integer) !!  - dt_cfd !!   - CFD timestep (dp real) !!  - icomm_grid !!   - The MPI communicator setup by the MPI_CART_CREATE command in the !!     CFD region (integer) !!  - icoord !!   - The three coordinate for each rank in the domain (integer array nproc by 3) !!  - npxyz_cfd !!   - Number of processors in each cartesian dimension (integer array 3) !!  - xyzL !!   - Size of domain in each cartesian dimension (dp real array 3) !!  - ncxyz !!   - Global number of cells in each cartesian dimension (integer array 3) !!  - density !!   - Density of the CFD simulation (dp_real) !!  - ijkcmax !!   - Global maximum cell in each cartesian dimension (integer array 3) !!  - ijkcmin !!   - Global minimum cell in each cartesian dimension (integer array 3) !!  - iTmin !!   - Local minimum cell for each rank (integer array no. procs in x) !!  - iTmax !!   - Local maximum cell for each rank (integer array no. procs in x) !!  - jTmin !!   - Local minimum cell for each rank (integer array no. procs in y) !!  - jTmax !!   - Local maximum cell for each rank (integer array no. procs in y) !!  - kTmin !!   - Local minimum cell for each rank (integer array no. procs in z) !!  - kTmax !!   - Local maximum cell for each rank (integer array no. procs in z) !!  - xg !!   - Array of cell vertices in the x direction (no. cells in x + 1 by !!     no. cells in y + 1) !!  - yg !!   - Array of cell vertices in the y direction (no. cells in x + 1 by !!     no. cells in y + 1) !!  - zg !!   - Array of cell vertices in the z direction (no. cells in z + 1) !! !! - Input/Output !!  - NONE !! !! - Output !!  - NONE !! !! @author Edward Smith ! ! ---------------------------------------------------------------------------- subroutine coupler_cfd_init ( nsteps , dt , icomm_grid , icoord , npxyz_cfd , xyzL , ncxyz , & density , ijkcmax , ijkcmin , iTmin , iTmax , jTmin , & jTmax , kTmin , kTmax , xgrid , ygrid , zgrid ) use mpi implicit none integer , intent ( in ) :: nsteps , icomm_grid integer , dimension ( 3 ), intent ( in ) :: ijkcmin , ijkcmax , npxyz_cfd , ncxyz integer , dimension (:), intent ( in ) :: iTmin , iTmax , jTmin , jTmax , kTmin , kTmax integer , dimension (:,:), intent ( in ) :: icoord real ( kind ( 0. d0 )), intent ( in ) :: dt , density real ( kind ( 0. d0 )), dimension ( 3 ), intent ( in ) :: xyzL real ( kind ( 0. d0 )), dimension (: ), intent ( in ) :: zgrid real ( kind ( 0. d0 )), dimension (:,:), intent ( in ) :: xgrid , ygrid integer :: i , ib , jb , kb , pcoords ( 3 ), source , nproc integer , dimension (:), allocatable :: buf , rank_world2rank_realm , rank_world2rank_cart real ( kind = kind ( 0. d0 )) :: dxmin , dxmax , dzmin , dzmax real ( kind = kind ( 0. d0 )), dimension (:), allocatable :: rbuf ! Read COUPLER.in input file call read_coupler_input ! Duplicate grid communicator for coupler use call MPI_comm_dup ( icomm_grid , CPL_CART_COMM , ierr ) call MPI_comm_rank ( CPL_CART_COMM , myid_cart , ierr ) rank_cart = myid_cart + 1 ; rootid_cart = 0 !Send only from root processor if ( myid_realm .eq. rootid_realm ) then source = MPI_ROOT else source = MPI_PROC_NULL endif ! ================ Exchange and store Data ============================== ! Data is stored to the coupler module with the same name in both realms ! Note - MPI Broadcast between intercommunicators is only supported by MPI-2 ! ------------------------ Processor Topology --------------------------- ! Store & Send CFD number of processors npx_cfd = npxyz_cfd ( 1 ) npy_cfd = npxyz_cfd ( 2 ) npz_cfd = npxyz_cfd ( 3 ) nproc_cfd = npx_cfd * npy_cfd * npz_cfd call MPI_bcast ( npxyz_cfd , 3 , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send ! Receive & Store MD number of processors allocate ( buf ( 3 )) call MPI_bcast ( buf , 3 , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive npx_md = buf ( 1 ) npy_md = buf ( 2 ) npz_md = buf ( 3 ) nproc_md = npx_md * npy_md * npz_md deallocate ( buf ) ! Store & Send CFD processor rank to coord allocate ( rank2coord_cfd ( 3 , nproc_cfd ), stat = ierr ); rank2coord_cfd = icoord !do ib=1,size(icoord,1) !do jb=1,size(icoord,2) !    print('(i1, a, i1, a, i1, a, i1)'), myid_world, ': icoord(', ib, ', ', jb, ') = ', icoord(ib,jb) !end do !end do iblock_realm = icoord ( 1 , rank_realm ) jblock_realm = icoord ( 2 , rank_realm ) kblock_realm = icoord ( 3 , rank_realm ) allocate ( buf ( 3 * nproc_cfd )); buf = reshape ( icoord , ( / 3 * nproc_cfd / ) ) call MPI_bcast ( buf , 3 * nproc_cfd , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send deallocate ( buf ) ! Receive & Store MD processor rank to coord allocate ( buf ( 3 * nproc_md )) call MPI_bcast ( buf , 3 * nproc_md , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive allocate ( rank2coord_md ( 3 , nproc_md ), stat = ierr ); rank2coord_md = reshape ( buf ,( / 3 , nproc_md / )) deallocate ( buf ) ! Setup CFD mapping from coordinate to rank ! Store & Send CFD mapping from coordinate to rank to MD allocate ( coord2rank_cfd ( npx_cfd , npy_cfd , npz_cfd )) do ib = 1 , npx_cfd do jb = 1 , npy_cfd do kb = 1 , npz_cfd pcoords = ( / ib , jb , kb / ) - 1 call MPI_Cart_rank ( CPL_CART_COMM , pcoords , i , ierr ) coord2rank_cfd ( ib , jb , kb ) = i + 1 enddo enddo enddo allocate ( buf ( nproc_cfd )); buf = reshape ( coord2rank_cfd , ( / nproc_cfd / ) ) call MPI_bcast ( coord2rank_cfd , nproc_cfd , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send deallocate ( buf ) ! Receive & Store MD coordinate to rank mapping allocate ( buf ( nproc_md )) call MPI_bcast ( buf , nproc_md , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive allocate ( coord2rank_md ( npx_md , npy_md , npz_md )) coord2rank_md = reshape ( buf ,( / npx_md , npy_md , npz_md / )) deallocate ( buf ) ! Setup CFD mapping between realm & world rank allocate ( rank_cfdrealm2rank_world ( nproc_cfd )) allocate ( rank_world2rank_realm ( nproc_world )) call CPL_rank_map ( CPL_REALM_COMM , rank_realm , nproc , & rank_cfdrealm2rank_world , rank_world2rank_realm , ierr ) !World to rank is the same on both realms allocate ( rank_world2rank_cfdrealm ( nproc_world )) allocate ( rank_world2rank_mdrealm ( nproc_world )) rank_world2rank_cfdrealm = rank_world2rank_realm rank_world2rank_mdrealm = rank_world2rank_realm ! Send CFD mapping to MD call MPI_bcast ( rank_cfdrealm2rank_world , nproc_cfd , MPI_integer , source , CPL_INTER_COMM , ierr ) !send ! Receive & Store MD mapping from realm to local rank from MD allocate ( rank_mdrealm2rank_world ( nproc_md )) call MPI_bcast ( rank_mdrealm2rank_world , nproc_md , MPI_integer , 0 , CPL_INTER_COMM , ierr ) !Receive ! Setup CFD mapping between cartesian topology & world rank allocate ( rank_cfdcart2rank_world ( nproc_cfd )) allocate ( rank_world2rank_cart ( nproc_world )) call CPL_rank_map ( CPL_CART_COMM , rank_cart , nproc , & rank_cfdcart2rank_world , rank_world2rank_cart , ierr ) !World to rank is the same on both realms cart allocate ( rank_world2rank_cfdcart ( nproc_world )) allocate ( rank_world2rank_mdcart ( nproc_world )) rank_world2rank_cfdcart = rank_world2rank_cart rank_world2rank_mdcart = rank_world2rank_cart ! Send CFD mapping to MD call MPI_bcast ( rank_cfdcart2rank_world , nproc_cfd , MPI_integer , source , CPL_INTER_COMM , ierr ) !send ! Receive & Store MD mapping from cart to local rank from MD allocate ( rank_mdcart2rank_world ( nproc_md )) call MPI_bcast ( rank_mdcart2rank_world , nproc_md , MPI_integer , 0 , CPL_INTER_COMM , ierr ) !Receive ! ------------------ Timesteps and iterations ------------------------------ ! Store & send CFD nsteps and dt_cfd nsteps_cfd = nsteps call MPI_bcast ( nsteps , 1 , MPI_integer , source , CPL_INTER_COMM , ierr ) !Send dt_cfd = dt call MPI_bcast ( dt , 1 , MPI_double_precision , source , CPL_INTER_COMM , ierr ) !Send ! Receive & store MD timestep dt_md call MPI_bcast ( dt_md , 1 , MPI_double_precision , 0 , CPL_INTER_COMM , ierr ) !Receive call MPI_bcast ( nsteps_md , 1 , MPI_integer , 0 , CPL_INTER_COMM , ierr ) !Receive ! ------------------ Send CFD grid extents ------------------------------ ! Store & send CFD density !density_cfd = density !call MPI_bcast(density_cfd,1,MPI_double_precision,source,CPL_INTER_COMM,ierr)  !Send ! Receive & store MD density !call MPI_bcast(density_md,1,MPI_double_precision,0,CPL_INTER_COMM,ierr)        !Receive ! Store & send CFD domain size xL_cfd = xyzL ( 1 ); yL_cfd = xyzL ( 2 ); zL_cfd = xyzL ( 3 ) call MPI_bcast ( xyzL , 3 , MPI_double_precision , source , CPL_INTER_COMM , ierr ) !Send ! Receive & store MD domain size allocate ( rbuf ( 3 )) call MPI_bcast ( rbuf , 3 , MPI_double_precision , 0 , CPL_INTER_COMM , ierr ) !Receive xL_md = rbuf ( 1 ); yL_md = rbuf ( 2 ); zL_md = rbuf ( 3 ); deallocate ( rbuf ) ! Store & send CFD grid extents icmin = ijkcmin ( 1 ); jcmin = ijkcmin ( 2 ); kcmin = ijkcmin ( 3 ) icmax = ijkcmax ( 1 ); jcmax = ijkcmax ( 2 ); kcmax = ijkcmax ( 3 ) allocate ( buf ( 6 )) buf = ( / icmin , icmax , jcmin , jcmax , kcmin , kcmax / ) call MPI_bcast ( buf , 6 , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send deallocate ( buf ) ! Store & send global number of cells in CFD ncx = ncxyz ( 1 ); ncy = ncxyz ( 2 ); ncz = ncxyz ( 3 ) call MPI_bcast ( ncxyz , 3 , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send ! Store & send array of global grid points allocate ( xg ( size ( xgrid , 1 ) + 1 , size ( xgrid , 2 ) + 1 ), stat = ierr ); xg = xgrid allocate ( yg ( size ( ygrid , 1 ) + 1 , size ( ygrid , 2 ) + 1 ), stat = ierr ); yg = ygrid allocate ( zg ( size ( zgrid , 1 ) + 1 ), stat = ierr ); zg = zgrid call MPI_bcast ( xgrid , size ( xgrid ), MPI_double_precision , source , CPL_INTER_COMM , ierr ) !Send call MPI_bcast ( ygrid , size ( ygrid ), MPI_double_precision , source , CPL_INTER_COMM , ierr ) !Send call MPI_bcast ( zgrid , size ( zgrid ), MPI_double_precision , source , CPL_INTER_COMM , ierr ) !Send !call write_matrix(xg,'cfd side, xg=',50+rank_realm) !call write_matrix(yg,'cfd side, yg=',50+rank_realm) !write(50+rank_realm,*), 'CFD side',rank_realm,'zg',zg ! Store & Send local (processor) CFD grid extents allocate ( icPmin_cfd ( npx_cfd ), stat = ierr ); icPmin_cfd (:) = iTmin (:) allocate ( icPmax_cfd ( npx_cfd ), stat = ierr ); icPmax_cfd (:) = iTmax (:) allocate ( jcPmin_cfd ( npy_cfd ), stat = ierr ); jcPmin_cfd (:) = jTmin (:) allocate ( jcPmax_cfd ( npy_cfd ), stat = ierr ); jcPmax_cfd (:) = jTmax (:) allocate ( kcPmin_cfd ( npz_cfd ), stat = ierr ); kcPmin_cfd (:) = kTmin (:) allocate ( kcPmax_cfd ( npz_cfd ), stat = ierr ); kcPmax_cfd (:) = kTmax (:) call MPI_bcast ( icPmin_cfd , npx_cfd , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send call MPI_bcast ( icPmax_cfd , npx_cfd , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send call MPI_bcast ( jcPmin_cfd , npy_cfd , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send call MPI_bcast ( jcPmax_cfd , npy_cfd , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send call MPI_bcast ( kcPmin_cfd , npz_cfd , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send call MPI_bcast ( kcPmax_cfd , npz_cfd , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send !Calculate the cell sizes dx,dy & dz dx = xL_cfd / ncx !xg(2,1)-xg(1,1) dy = yg ( 1 , 2 ) - yg ( 1 , 1 ) ! yL_cfd/ncy dz = zL_cfd / ncz !zg(2  )-zg(1  ) !Calculate number of cells in overlap region ncx_olap = icmax_olap - icmin_olap + 1 ncy_olap = jcmax_olap - jcmin_olap + 1 ncz_olap = kcmax_olap - kcmin_olap + 1 !Broadcast the overlap to CFD on intracommunicator call MPI_bcast ( ncy_olap , 1 , MPI_INTEGER , rootid_realm , CPL_REALM_COMM , ierr ) !Broadcast the overlap to MD over intercommunicator call MPI_bcast ( ncy_olap , 1 , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) ! Establish mapping between MD and CFD call CPL_create_map !Check for grid strectching and terminate process if found call check_mesh contains subroutine check_mesh implicit none integer :: i , j ! Check grids are the right size if ( size ( xg , 1 ) .ne. ( ncx + 1 ) .or. size ( xg , 2 ) .ne. ( ncy + 1 )) then call error_abort ( 'xg is the wrong size in cpl_cfd_init' ) end if if ( size ( yg , 1 ) .ne. ( ncx + 1 ) .or. size ( yg , 2 ) .ne. ( ncy + 1 )) then call error_abort ( 'yg is the wrong size in cpl_cfd_init' ) end if if ( size ( zg ) .ne. ( ncz + 1 )) then call error_abort ( 'zg is the wrong size in cpl_cfd_init' ) end if !Define cell sizes dx,dy & dz and check for grid stretching ! - - x - - dx = xg ( 2 , 1 ) - xg ( 1 , 1 ) dxmax = maxval ( xg ( 2 : ncx + 1 , 2 : ncy + 1 ) - xg ( 1 : ncx , 1 : ncy )) dxmin = minval ( xg ( 2 : ncx + 1 , 2 : ncy + 1 ) - xg ( 1 : ncx , 1 : ncy )) if ( dxmax - dx .gt. 0.00001 d0 .or. dx - dxmin .gt. 0.00001 d0 ) then print '(3(a,f15.7))' , 'Max dx = ' , dxmax , ' dx = ' , dx , ' Min dx = ' , dxmin call error_abort ( \"ERROR - Grid stretching in x not supported\" ) endif ! - - y - - dy = yg ( 1 , 2 ) - yg ( 1 , 1 ) dymax = maxval ( yg ( 2 : ncx + 1 , 2 : ncy + 1 ) - yg ( 1 : ncx , 1 : ncy )) dymin = minval ( yg ( 2 : ncx + 1 , 2 : ncy + 1 ) - yg ( 1 : ncx , 1 : ncy )) if ( dymax - dy .gt. 0.00001 d0 .or. dy - dymin .gt. 0.00001 d0 ) then print '(3(a,f15.7))' , 'Max dy = ' , dymax , ' dy = ' , dy , ' Min dy = ' , dymin call error_abort ( \"ERROR - Grid stretching in y not supported\" ) endif !if (dymax-dy.gt.0.0001 .or. dy-dymin.gt.0.0001) then !    write(*,*) \"********************************************************************\" !    write(*,*) \" Grid stretching employed in CFD domain - range of dy sizes:        \" !   write(*,*) \"dymin = \", dymin, \" dy = \",dy, \" dymax = \", dymax !    write(*,*) \"********************************************************************\" !    write(*,*) !endif ! - - z - - dzmax = maxval ( zg ( 2 : ncz ) - zg ( 1 : ncz - 1 )) dzmin = minval ( zg ( 2 : ncz ) - zg ( 1 : ncz - 1 )) if ( dzmax - dz .gt. 0.00001 d0 .or. dz - dzmin .gt. 0.00001 d0 ) then print '(3(a,f15.7))' , 'Max dz = ' , dzmax , ' dz = ' , dz , ' Min dz = ' , dzmin call error_abort ( \"ERROR - Grid stretching in z not supported\" ) endif end subroutine check_mesh end subroutine coupler_cfd_init !------------------------------------------------------------------------------ !                              coupler_md_init                               - !------------------------------------------------------------------------------ !> !! Initialisation routine for coupler module - Every variable is sent and stored !! to ensure both md and cfd region have an identical list of parameters !! !! - Synopsis !! !!  - coupler_mf_init(nsteps,dt_md,icomm_grid,icoord,npxyz_md,globaldomain,density) !! !! - Input !! !!  - nsteps !!   - Number of time steps the MD code is expected to run for (integer) !!  - dt_md !!   - MD timestep (dp real) !!  - icomm_grid !!   - The MPI communicator setup by the MPI_CART_CREATE command in the !!     CFD region (integer) !!  - icoord !!   - The three coordinate for each rank in the domain (integer array nproc by 3) !!  - npxyz_md !!   - Number of processors in each cartesian dimension (integer array 3) !!  - globaldomain !!   - Size of domain in each cartesian dimension (dp real array 3) !!  - density !!   - Density of the CFD simulation (dp_real) !! !! - Input/Output !!  - NONE !! !! - Output !!  - NONE !! !! @author Edward Smith ! ! ---------------------------------------------------------------------------- ! ---------------------------------------------------------------------------- ! Initialisation routine for coupler - Every variable is sent and stored ! to ensure both md and cfd region have an identical list of parameters subroutine coupler_md_init ( Nsteps , initialstep , dt , icomm_grid , icoord , npxyz_md , globaldomain , density ) use mpi implicit none integer , intent ( inout ) :: nsteps , initialstep integer , intent ( in ) :: icomm_grid integer , dimension ( 3 ), intent ( in ) :: npxyz_md integer , dimension (:,:), intent ( in ) :: icoord real ( kind ( 0. d0 )), intent ( in ) :: dt , density real ( kind = kind ( 0. d0 )), dimension ( 3 ), intent ( in ) :: globaldomain integer :: i , ib , jb , kb , pcoords ( 3 ), source , nproc integer , dimension (:), allocatable :: buf , rank_world2rank_realm , rank_world2rank_cart real ( kind = kind ( 0. d0 )), dimension (:), allocatable :: rbuf ! Read COUPLER.in input file call read_coupler_input ! Duplicate grid communicator for coupler use call MPI_comm_dup ( icomm_grid , CPL_CART_COMM , ierr ) call MPI_comm_rank ( CPL_CART_COMM , myid_cart , ierr ) rank_cart = myid_cart + 1 ; rootid_cart = 0 !Send only from root processor if ( myid_realm .eq. rootid_realm ) then source = MPI_ROOT else source = MPI_PROC_NULL endif ! ================ Exchange and store Data ============================== ! Data is stored to the coupler module with the same name in both realms ! Note - MPI Broadcast between intercommunicators is only supported by MPI-2 ! ------------------------ Processor Topology --------------------------- ! Receive & Store CFD number of processors allocate ( buf ( 3 )) call MPI_bcast ( buf , 3 , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive npx_cfd = buf ( 1 ); npy_cfd = buf ( 2 ); npz_cfd = buf ( 3 ) nproc_cfd = npx_cfd * npy_cfd * npz_cfd deallocate ( buf ) ! Store & Send MD number of processors npx_md = npxyz_md ( 1 ); npy_md = npxyz_md ( 2 ); npz_md = npxyz_md ( 3 ) nproc_md = npx_md * npy_md * npz_md call MPI_bcast ( npxyz_md , 3 , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send ! Receive & Store CFD processor rank to coord allocate ( buf ( 3 * nproc_cfd )) call MPI_bcast ( buf , 3 * nproc_cfd , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive allocate ( rank2coord_cfd ( 3 , nproc_cfd ), stat = ierr ); rank2coord_cfd = reshape ( buf ,( / 3 , nproc_cfd / )) deallocate ( buf ) ! Store & Send MD processor rank to coord allocate ( rank2coord_md ( 3 , nproc_md ), stat = ierr ); rank2coord_md = icoord iblock_realm = icoord ( 1 , rank_realm ); jblock_realm = icoord ( 2 , rank_realm ); kblock_realm = icoord ( 3 , rank_realm ) allocate ( buf ( 3 * nproc_md )); buf = reshape ( icoord ,( / 3 * nproc_md / )) call MPI_bcast ( buf , 3 * nproc_md , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send deallocate ( buf ) ! Receive & Store CFD coordinate to rank mapping allocate ( buf ( nproc_cfd )) call MPI_bcast ( buf , nproc_cfd , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive allocate ( coord2rank_cfd ( npx_cfd , npy_cfd , npz_cfd )) coord2rank_cfd = reshape ( buf ,( / npx_cfd , npy_cfd , npz_cfd / )) deallocate ( buf ) ! Setup MD mapping from coordinate to rank, ! Store & Send MD mapping from coordinate to rank to CFD allocate ( coord2rank_md ( npx_md , npy_md , npz_md )) do ib = 1 , npx_md do jb = 1 , npy_md do kb = 1 , npz_md pcoords = ( / ib , jb , kb / ) - 1 call MPI_Cart_rank ( CPL_CART_COMM , pcoords , i , ierr ) coord2rank_md ( ib , jb , kb ) = i + 1 enddo enddo enddo allocate ( buf ( nproc_md )); buf = reshape ( coord2rank_md , ( / nproc_md / ) ) call MPI_bcast ( coord2rank_md , nproc_md , MPI_INTEGER , source , CPL_INTER_COMM , ierr ) !Send deallocate ( buf ) ! Setup MD mapping between realm & world rank allocate ( rank_mdrealm2rank_world ( nproc_md )) allocate ( rank_world2rank_realm ( nproc_world )) call CPL_rank_map ( CPL_REALM_COMM , rank_realm , nproc , & rank_mdrealm2rank_world , rank_world2rank_realm , ierr ) !World to rank is the same on both realms allocate ( rank_world2rank_cfdrealm ( nproc_world )) allocate ( rank_world2rank_mdrealm ( nproc_world )) rank_world2rank_cfdrealm = rank_world2rank_realm rank_world2rank_mdrealm = rank_world2rank_realm ! Receive & Store CFD_mapping from realm to local rank allocate ( rank_cfdrealm2rank_world ( nproc_cfd )) call MPI_bcast ( rank_cfdrealm2rank_world , nproc_cfd , MPI_integer , 0 , CPL_INTER_COMM , ierr ) !Receive ! Send MD mapping from realm to local rank to CFD call MPI_bcast ( rank_mdrealm2rank_world , nproc_md , MPI_integer , source , CPL_INTER_COMM , ierr ) !send ! Setup MD mapping between cartesian topology & world rank allocate ( rank_mdcart2rank_world ( nproc_md )) allocate ( rank_world2rank_cart ( nproc_world )) call CPL_rank_map ( CPL_CART_COMM , rank_cart , nproc , & rank_mdcart2rank_world , rank_world2rank_cart , ierr ) !World to rank is the same on both realms cart allocate ( rank_world2rank_cfdcart ( nproc_world )) allocate ( rank_world2rank_mdcart ( nproc_world )) rank_world2rank_cfdcart = rank_world2rank_cart rank_world2rank_mdcart = rank_world2rank_cart ! Receive & Store CFD_mapping from cart to local rank allocate ( rank_cfdcart2rank_world ( nproc_cfd )) call MPI_bcast ( rank_cfdcart2rank_world , nproc_cfd , MPI_integer , 0 , CPL_INTER_COMM , ierr ) !Receive ! Send MD mapping from cart to local rank to CFD call MPI_bcast ( rank_mdcart2rank_world , nproc_md , MPI_integer , source , CPL_INTER_COMM , ierr ) !send ! ------------------ Timesteps and iterations ------------------------------ ! Receive & store CFD nsteps and dt_cfd call MPI_bcast ( nsteps_cfd , 1 , MPI_integer , 0 , CPL_INTER_COMM , ierr ) !Receive call MPI_bcast ( dt_cfd , 1 , MPI_double_precision , 0 , CPL_INTER_COMM , ierr ) !Receive ! Store & send MD timestep to dt_md dt_MD = dt call MPI_bcast ( dt , 1 , MPI_double_precision , source , CPL_INTER_COMM , ierr ) !Send nsteps_MD = nsteps call MPI_bcast ( nsteps , 1 , MPI_integer , source , CPL_INTER_COMM , ierr ) !Send ! ------------------ Receive CFD grid extents ------------------------------ ! Receive & store CFD density !call MPI_bcast(density_cfd,1,MPI_double_precision,0,CPL_INTER_COMM,ierr)       !Receive ! Store & send MD density !density_md = density !call MPI_bcast(density,1,MPI_double_precision,source,CPL_INTER_COMM,ierr)  !Send ! Receive & store CFD domain size allocate ( rbuf ( 3 )) call MPI_bcast ( rbuf , 3 , MPI_double_precision , 0 , CPL_INTER_COMM , ierr ) !Receive xL_cfd = rbuf ( 1 ); yL_cfd = rbuf ( 2 ); zL_cfd = rbuf ( 3 ) deallocate ( rbuf ) ! Store & send MD domain size xL_md = globaldomain ( 1 ); yL_md = globaldomain ( 2 ); zL_md = globaldomain ( 3 ) call MPI_bcast ( globaldomain , 3 , MPI_double_precision , source , CPL_INTER_COMM , ierr ) !Send ! Receive & Store global CFD grid extents allocate ( buf ( 6 )) call MPI_bcast ( buf , 6 , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Send icmin = buf ( 1 ); icmax = buf ( 2 ) jcmin = buf ( 3 ); jcmax = buf ( 4 ) kcmin = buf ( 5 ); kcmax = buf ( 6 ) deallocate ( buf ) ! Receive & Store array of global number of cells in CFD allocate ( buf ( 3 )) call MPI_bcast ( buf , 3 , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive ncx = buf ( 1 ); ncy = buf ( 2 ); ncz = buf ( 3 ) deallocate ( buf ) ! Receive & Store array of global grid points allocate ( xg ( ncx + 1 , ncy + 1 ), yg ( ncx + 1 , ncy + 1 ), zg ( ncz + 1 )) call MPI_bcast ( xg , size ( xg ), MPI_double_precision , 0 , CPL_INTER_COMM , ierr ) !Receive call MPI_bcast ( yg , size ( yg ), MPI_double_precision , 0 , CPL_INTER_COMM , ierr ) !Receive call MPI_bcast ( zg , size ( zg ), MPI_double_precision , 0 , CPL_INTER_COMM , ierr ) !Receive ! Receive & Store local (processor) CFD grid extents allocate ( icPmin_cfd ( npx_cfd )); allocate ( icPmax_cfd ( npx_cfd )); allocate ( jcPmin_cfd ( npy_cfd )); allocate ( jcPmax_cfd ( npy_cfd )); allocate ( kcPmin_cfd ( npz_cfd )); allocate ( kcPmax_cfd ( npz_cfd )); call MPI_bcast ( icPmin_cfd , npx_cfd , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive call MPI_bcast ( icPmax_cfd , npx_cfd , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive call MPI_bcast ( jcPmin_cfd , npy_cfd , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive call MPI_bcast ( jcPmax_cfd , npy_cfd , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive call MPI_bcast ( kcPmin_cfd , npz_cfd , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive call MPI_bcast ( kcPmax_cfd , npz_cfd , MPI_INTEGER , 0 , CPL_INTER_COMM , ierr ) !Receive !Calculate the cell sizes dx,dy & dz dx = xL_cfd / ncx !xg(2,1)-xg(1,1) dy = yg ( 1 , 2 ) - yg ( 1 , 1 ) ! yL_cfd/ncy dz = zL_cfd / ncz !zg(2  )-zg(1  ) !Define number of cells in overlap region ncx_olap = icmax_olap - icmin_olap + 1 ncy_olap = jcmax_olap - jcmin_olap + 1 ncz_olap = kcmax_olap - kcmin_olap + 1 ! Establish mapping between MD an CFD call CPL_create_map !if ( nsteps_md <= 0 ) then !    write(0,*) \"Number of MD steps per dt interval <= 0\" !    write(0,*) \"Coupler will not work, quitting ...\" !    call MPI_Abort(MPI_COMM_WORLD,COUPLER_ERROR_INIT,ierr) !endif ! Setup timesteps and simulation timings based on CFD/coupler call set_coupled_timing ( initialstep , Nsteps ) end subroutine coupler_md_init !----------------------------------------------------------------------------- ! Lucian(?): !   This routine should be part of the initialisation of the coupler. !   The initial times (stime for cfd & elapsedtime for md) are !   compared to see if restart is consistent and number of steps on !   both sides of the coupler are calculated and stored ! DT: This routine seems to me that it's unique to flowmol. I'll have a look !     at making it general when I do the LAMMPS socket. TODO(djt06@ic.ac.uk) subroutine set_coupled_timing ( initialstep , Nsteps ) implicit none integer , intent ( in ) :: initialstep integer , intent ( out ) :: Nsteps real ( kind = kind ( 0. d0 )) :: elapsedtime integer :: Nsteps_MDperCFD !Set number of MD timesteps per CFD using ratio of timestep or coupler value if ( timestep_ratio .eq. VOID ) then Nsteps_MDperCFD = int ( dt_cfd / dt_MD ) else Nsteps_MDperCFD = timestep_ratio endif Nsteps_coupled = Nsteps_cfd !Set number of steps in MD simulation and final time elapsed Nsteps_md = initialstep + Nsteps_cfd * Nsteps_MDperCFD elapsedtime = Nsteps_md * dt_MD if ( rank_realm .eq. 1 ) then print * , 'Nsteps in CFD is ' , Nsteps_cfd print * , 'Nsteps in MD reset from ' , Nsteps , ' to ' , Nsteps_md print * , 'Total simulation time will be ' , elapsedtime , ' in LJ units' endif !Set corrected nsteps returned to MD Nsteps = Nsteps_md end subroutine set_coupled_timing !============================================================================= !! Establish for all MD processors the mapping (if any) !! to coupled CFD processors !----------------------------------------------------------------------------- subroutine CPL_create_map use mpi implicit none ! Check (as best as one can) that the inputs will work call check_config_feasibility ! Get ranges of cells on each MD processor call get_md_cell_ranges ! Get overlapping mapping for MD to CFD call get_overlap_blocks ! Setup overlap communicators call prepare_overlap_comms ! Setup graph topology call CPL_overlap_topology contains subroutine check_config_feasibility implicit none integer :: ival real ( kind ( 0. d0 )) :: rval , rtoler character ( len = 256 ) :: string ! Check that CFD and MD domains agree in x and z directions rtoler = 1. d - 4 rval = 0. d0 rval = rval + abs ( xL_md - xL_cfd ) rval = rval + abs ( zL_md - zL_cfd ) if ( rval .gt. rtoler ) then string = \"MD/CFD domain sizes do not match in both x and z \" // & \"directions. Aborting simulation. \" print * , \"xL_md = \" , xL_md print * , \"xL_cfd = \" , xL_cfd print * , \"zL_md = \" , zL_md print * , \"zL_cfd = \" , zL_cfd call error_abort ( string ) end if ! Check there is only one overlap CFD proc in y ival = nint ( dble ( ncy ) / dble ( npy_cfd ) ) if ( ncy_olap .gt. ival ) then string = \"This coupler will not work if there is more than one \" // & \"CFD (y-coordinate) in the overlapping region. \" // & \"Aborting simulation.\" call error_abort ( string ) end if ! Check that MD processor size is an integer multiple of CFD cell size ! This test doesn't work if xL_xyz is slightly less than a multiple of dxyz ! We avoid this by adding the required tolerence to the mod and taking away after rtoler = 1. d - 4 rval = 0. d0 rval = rval + abs ( mod ( xL_md + rtoler , dx ) - rtoler ) rval = rval + abs ( mod ( yL_md + rtoler , dy ) - rtoler ) rval = rval + abs ( mod ( zL_md + rtoler , dz ) - rtoler ) if ( rval .gt. rtoler ) then print '(6(a,f10.5))' , ' xL_md/dx = ' , xL_md / dx , 'dx =' , dx , & ' yL_md/dy = ' , yL_md / dy , 'dy =' , dy , & ' zL_md/dz = ' , zL_md / dz , 'dz =' , dz string = \"MD region lengths must be an integer number of CFD \" // & \"cell sizes (i.e. xL_md must be an integer multiple \" // & \"of dx, etc. ), aborting simulation.\" call error_abort ( string ) end if ! Check whether ncx,ncy,ncz are an integer multiple of npx_md, etc. ! - N.B. no need to check ncy/npy_md. ival = 0 ival = ival + mod ( ncx , npx_cfd ) ival = ival + mod ( ncy , npy_cfd ) ival = ival + mod ( ncz , npz_cfd ) ival = ival + mod ( ncx , npx_md ) ival = ival + mod ( ncz , npz_md ) if ( ival .ne. 0 ) then string = \"The number of cells in the cfd domain is not an \" // & \"integer multiple of the number of processors in \" // & \"the x and z directions. Aborting simulation.\" call error_abort ( string ) end if ! Check that the MD region is large enough to cover overlap xL_olap = ncx_olap * dx yL_olap = ncy_olap * dy zL_olap = ncz_olap * dz ! Tolerance of half a cell width if ( xL_md .lt. ( xL_olap - dx / 2. d0 ) .or. & yL_md .lt. ( yL_olap - dy / 2. d0 ) .or. & zL_md .lt. ( zL_olap - dz / 2. d0 ) ) then string = \"Overlap region is larger than the MD region. \" // & \"Aborting simulation.\" call error_abort ( string ) end if ! Check overlap cells are within CFD extents ival = 0 if ( icmin_olap .lt. icmin ) ival = ival + 1 if ( icmax_olap .gt. icmax ) ival = ival + 1 if ( jcmin_olap .lt. jcmin ) ival = ival + 1 if ( jcmax_olap .gt. jcmax ) ival = ival + 1 if ( kcmin_olap .lt. kcmin ) ival = ival + 1 if ( kcmax_olap .gt. kcmax ) ival = ival + 1 if ( ival .ne. 0 ) then print '(a,6i10)' , 'ijkcmin,ijkcmax = ' , icmin , icmax & , jcmin , jcmax & , kcmin , kcmax print '(a,6i10)' , 'olap extents    = ' , icmin_olap , icmax_olap , jcmin_olap , & jcmax_olap , kcmin_olap , kcmax_olap string = \"Overlap region has been specified outside of the \" // & \"CFD region. Aborting simulation.\" call error_abort ( string ) end if ! Check MD/CFD ratios are integers in x and z if ( mod ( npx_md , npx_cfd ) .ne. 0 ) then print '(a,i8,a,i8)' , ' number of MD processors in x ' , npx_md , & ' number of CFD processors in x ' , npx_cfd call error_abort ( \"get_overlap_blocks error - number of MD \" // & \"processors in x must be an integer multiple \" // & \"of number of CFD processors in x\" ) elseif ( mod ( npz_md , npz_cfd ) .ne. 0 ) then print '(a,i8,a,i8)' , ' number of MD processors in z ' , npz_md , & ' number of CFD processors in z ' , npz_cfd call error_abort ( \"get_overlap_blocks error - number of MD \" // & \"processors in z must be an integer multiple \" // & \"of number of CFD processors in z\" ) endif end subroutine check_config_feasibility !------------------------------------------------------------ !Calculate processor cell ranges of MD code on all processors !------------------------------------------------------------------------------ !                      GET MD CELL RANGES                                     - !------------------------------------------------------------------------------ !> !! Store the minimum and maximum CFD cell coordinates that overlap each !! MD processor. !! !! - Synopsis !! !!  - get_md_cell_ranges() !! !! - Input !!  - NONE !! - Input/Output !!  - NONE !! - Output !!  - NONE !! !! @author David Trevelyan subroutine get_md_cell_ranges implicit none integer :: n integer :: olap_jmin_mdcoord integer :: ncxl , nczl !, ncyl integer :: ncy_mdonly , ncy_md , ncyP_md integer :: funit allocate ( icPmin_md ( npx_md )); icPmin_md = VOID allocate ( jcPmin_md ( npy_md )); jcPmin_md = VOID allocate ( kcPmin_md ( npz_md )); kcPmin_md = VOID allocate ( icPmax_md ( npx_md )); icPmax_md = VOID allocate ( jcPmax_md ( npy_md )); jcPmax_md = VOID allocate ( kcPmax_md ( npz_md )); kcPmax_md = VOID ! - - x - - ncxl = ceiling ( dble ( ncx ) / dble ( npx_md )) do n = 1 , npx_md icPmax_md ( n ) = n * ncxl icPmin_md ( n ) = icPmax_md ( n ) - ncxl + 1 end do ! - - y - - ncy_md = nint ( yL_md / dy ) ncy_mdonly = ncy_md - ncy_olap ncyP_md = ncy_md / npy_md olap_jmin_mdcoord = npy_md - ceiling ( dble ( ncy_olap ) / dble ( ncyP_md )) + 1 do n = olap_jmin_mdcoord , npy_md jcPmax_md ( n ) = n * ncyP_md - ncy_mdonly jcPmin_md ( n ) = jcPmax_md ( n ) - ncyP_md + 1 if ( jcPmin_md ( n ) .le. 0 ) jcPmin_md ( n ) = 1 end do ! - - z - - nczl = ceiling ( dble ( ncz ) / dble ( npz_md )) do n = 1 , npz_md kcPmax_md ( n ) = n * nczl kcPmin_md ( n ) = kcPmax_md ( n ) - nczl + 1 end do if ( myid_world .eq. rootid_world ) then funit = CPL_new_fileunit () open ( funit , file = \"cpl/map_MD\" , action = \"write\" , status = \"replace\" ) write ( funit , * ), '' write ( funit , * ), '===========================================' write ( funit , * ), '------------ M D   M A P ------------------' write ( funit , * ), '===========================================' write ( funit , * ), 'npx_md = ' , npx_md write ( funit , * ), 'ncx    = ' , ncx write ( funit , * ), 'ncxl   = ' , ncxl write ( funit , * ), '-------------------------------------------' write ( funit , * ), '  icoord_md     icPmin_md     icPmax_md    ' write ( funit , * ), '-------------------------------------------' do n = 1 , npx_md write ( funit , '(1x,3i11)' ), n , icPmin_md ( n ), icPmax_md ( n ) end do write ( funit , * ), '-------------------------------------------' write ( funit , * ), 'npy_md     = ' , npy_md write ( funit , * ), 'ncy_md     = ' , ncy_md write ( funit , * ), 'ncyP_md    = ' , ncyP_md write ( funit , * ), 'ncy_olap   = ' , ncy_olap write ( funit , * ), 'ncy_mdonly = ' , ncy_mdonly write ( funit , * ), 'olap_jmin_mdcoord = ' , olap_jmin_mdcoord write ( funit , * ), 'dy         = ' , dy write ( funit , * ), '-------------------------------------------' write ( funit , * ), '  jcoord_md     jcPmin_md       jcPmax_md  ' write ( funit , * ), '-------------------------------------------' do n = 1 , npy_md write ( funit , '(1x,3i11)' ) n , jcPmin_md ( n ), jcPmax_md ( n ) end do write ( funit , * ), '-------------------------------------------' write ( funit , * ), 'npz_md = ' , npz_md write ( funit , * ), 'ncz    = ' , ncz write ( funit , * ), 'nczl   = ' , nczl write ( funit , * ), '-------------------------------------------' write ( funit , * ), '  kcoord_md     kcPmin_md       kcPmax_md  ' write ( funit , * ), '-------------------------------------------' do n = 1 , npz_md write ( funit , '(1x,3i11)' ), n , kcPmin_md ( n ), kcPmax_md ( n ) end do write ( funit , * ), '-------------------------------------------' close ( funit , status = \"keep\" ) endif !Sanity Check min not less than max if ( icPmin_md ( iblock_realm ) .gt. icPmax_md ( iblock_realm )) & call error_abort ( \"Error in get_md_cell_ranges - mapping failure imin greater than imax\" ) if ( jcPmin_md ( jblock_realm ) .gt. jcPmax_md ( jblock_realm )) & call error_abort ( \"Error in get_md_cell_ranges - mapping failure jmin greater than jmax\" ) if ( kcPmin_md ( kblock_realm ) .gt. kcPmax_md ( kblock_realm )) & call error_abort ( \"Error in get_md_cell_ranges - mapping failure kmin greater than kmax\" ) end subroutine get_md_cell_ranges !------------------------------------------------------------ !Calculate processor overlap between CFD/MD on all processors !------------------------------------------------------------------------------ !                          GET OVERLAP BLOCKS                                 - !------------------------------------------------------------------------------ !> !! Store MD processor coordinates that overlap each CFD processor coordinate. !! !! - Synopsis !! !!  - get_overlap_blocks() !! !! - Input !!  - NONE !! - Input/Output !!  - NONE !! - Output !!  - NONE !! !! @author David Trevelyan subroutine get_overlap_blocks implicit none integer :: i , n , endproc , nolapsx , nolapsy , nolapsz integer , dimension ( 3 ) :: pcoords integer :: funit real ( kind ( 0. d0 )) :: xLl_md , yLl_md , zLl_md , yLl_cfd xL_olap = ncx_olap * dx yL_olap = ncy_olap * dy zL_olap = ncz_olap * dz xLl_md = xL_md / npx_md yLl_md = yL_md / npy_md zLl_md = zL_md / npz_md if ( realm .eq. md_realm ) then xLl = xLl_md ; yLl = yLl_md ; zLl = zLl_md endif nolapsx = nint ( dble ( npx_md ) / dble ( npx_cfd ) ) nolapsy = ceiling ( yL_olap / yLl_md ) nolapsz = nint ( dble ( npz_md ) / dble ( npz_cfd ) ) !Get cartesian coordinate of overlapping md cells & cfd cells allocate ( cfd_icoord2olap_md_icoords ( npx_cfd , nolapsx )) allocate ( cfd_jcoord2olap_md_jcoords ( npy_cfd , nolapsy )) allocate ( cfd_kcoord2olap_md_kcoords ( npz_cfd , nolapsz )) cfd_icoord2olap_md_icoords = VOID cfd_jcoord2olap_md_jcoords = VOID cfd_kcoord2olap_md_kcoords = VOID ! - - x - - do n = 1 , npx_cfd do i = 1 , nolapsx cfd_icoord2olap_md_icoords ( n , i ) = ( n - 1 ) * nolapsx + i end do end do ! - - y - - yLl_cfd = yL_cfd / npy_cfd endproc = ceiling ( yL_olap / yLl_cfd ) if ( endproc .gt. npy_cfd ) then print * , \"Warning in get_overlap_blocks -- top processor in CFD greater than number\" print * , \"  of processors. This may be correct if some MD domain exists above CFD.\" endproc = npy_cfd nolapsy = 1 print * , endproc , nolapsy endif do n = 1 , endproc do i = 1 , nolapsy cfd_jcoord2olap_md_jcoords ( n , i ) = ( n - 1 ) * nolapsy + i & + ( npy_md - nolapsy ) end do end do ! - - z - - do n = 1 , npz_cfd do i = 1 , nolapsz cfd_kcoord2olap_md_kcoords ( n , i ) = ( n - 1 ) * nolapsz + i end do end do if ( myid_world .eq. rootid_world ) then funit = CPL_new_fileunit () open ( funit , file = \"cpl/map_CFD\" , action = \"write\" , status = \"replace\" ) write ( funit , * ), '' write ( funit , * ), '===========================================' write ( funit , * ), '------------ C F D   M A P ----------------' write ( funit , * ), '===========================================' write ( funit , * ), 'npx_cfd = ' , npx_cfd write ( funit , * ), 'nolapsx = ' , nolapsx write ( funit , * ), '-------------------------------------------' write ( funit , * ), '  icoord_cfd       olapmin     olapmax     ' write ( funit , * ), '-------------------------------------------' do n = 1 , npx_cfd write ( funit , '(1x,3i11)' ), n , & cfd_icoord2olap_md_icoords ( n , 1 ), & cfd_icoord2olap_md_icoords ( n , nolapsx ) end do write ( funit , * ), '-------------------------------------------' write ( funit , * ), 'npy_cfd = ' , npy_cfd write ( funit , * ), 'nolapsy = ' , nolapsy write ( funit , * ), '-------------------------------------------' write ( funit , * ), '  jcoord_cfd       olapmin     olapmax     ' write ( funit , * ), '-------------------------------------------' do n = 1 , npy_cfd write ( funit , '(1x,3i11)' ), n , & cfd_jcoord2olap_md_jcoords ( n , 1 ), & cfd_jcoord2olap_md_jcoords ( n , nolapsy ) end do write ( funit , * ), '-------------------------------------------' write ( funit , * ), 'npz_cfd = ' , npz_cfd write ( funit , * ), 'nolapsz = ' , nolapsz write ( funit , * ), '-------------------------------------------' write ( funit , * ), '  kcoord_cfd       olapmin     olapmax     ' write ( funit , * ), '-------------------------------------------' do n = 1 , npz_cfd write ( funit , '(1x,3i11)' ), n , & cfd_kcoord2olap_md_kcoords ( n , 1 ), & cfd_kcoord2olap_md_kcoords ( n , nolapsz ) end do write ( funit , * ), '-------------------------------------------' close ( funit , status = \"keep\" ) endif end subroutine get_overlap_blocks !subroutine intersect_comm !   use coupler_module !   use mpi !   implicit none !   integer :: n !   integer,dimension(3)   :: pcoords !Create communicator for all intersecting processors !   if (realm .eq. cfd_realm) then !       map%n = npx_md/npx_cfd !       allocate(map%rank_list(map%n)) !Get rank(s) of overlapping MD processor(s) !       do n = 1,map%n !           pcoords(1)=cfd_icoord2olap_md_icoords(rank2coord_cfd(1,rank_realm),n) !           pcoords(2)=cfd_jcoord2olap_md_jcoords(rank2coord_cfd(2,rank_realm),n) !           pcoords(3)=cfd_kcoord2olap_md_kcoords(rank2coord_cfd(3,rank_realm),n) !           if (any(pcoords(:).eq.VOID)) then !               map%n = 0; map%rank_list(:) = VOID !           else !               map%rank_list(n) = coord2rank_md(pcoords(1),pcoords(2),pcoords(3)) !           endif !           write(250+rank_realm,'(2a,6i5)'), 'overlap',realm_name(realm),rank_realm,map%n,map%rank_list(n),pcoords !       enddo !   else if (realm .eq. md_realm) then !       map%n = 1 !       allocate(map%rank_list(map%n)) !Get rank of overlapping CFD processor !       pcoords(1) = rank2coord_md(1,rank_realm)*(dble(npx_cfd)/dble(npx_md)) !       pcoords(2) = npy_cfd !rank2coord_md(2,rank_realm)*(dble(npy_cfd)/dble(npy_md)) !       pcoords(3) = rank2coord_md(3,rank_realm)*(dble(npz_cfd)/dble(npz_md)) !!      map%rank_list(1) = coord2rank_cfd(pcoords(1),pcoords(2),pcoords(3)) !       write(300+rank_realm,'(2a,6i5)'), 'overlap',realm_name(realm),rank_realm,map%n,map%rank_list(1),pcoords !   endif !end subroutine intersect_comm !========================================================================= !------------------------------------------------------------------------------ !                    PREPARE OVERLAP COMMS                                    - !------------------------------------------------------------------------------ !> !! @author David Trevelyan !! Splits the world communicator into \"overlap\" communicators. Each overlap !! communicator consists of a CFD root processor and the MD processors which !! lie on top of it in the domain. !! !! - Synopsis !! !!  - prepare_overlap_comms() !! !! - Input !!  - NONE !! - Input/Output !!  - NONE !! - Output !!  - NONE !! subroutine prepare_overlap_comms use mpi implicit none !General idea: !   1. loop over cfd cart ranks !   2. find cfd cart coords from cfd cart rank !   3. find overlapping md cart coords (from cfd_icoord2olap_md_icoords) !   4. find md cart rank from md cart coords (coord2rank_md) !   5. find md world rank from md cart rank (rank_mdcart2rank_world) !   6. set group(md_world_rank) to cfd cart rank !   7. split world comm according to groups !      if group(world_rank) == 0, set olap_comm to null integer :: i , j , k , ic , jc , kc integer :: trank_md , trank_cfd , trank_world , nolap integer , dimension (:), allocatable :: mdicoords , mdjcoords , mdkcoords integer , parameter :: olap_null = - 666 integer :: group ( nproc_world ) integer :: cfdcoord ( 3 ) integer :: tempsize tempsize = size ( cfd_icoord2olap_md_icoords , 2 ) allocate ( mdicoords ( tempsize )) tempsize = size ( cfd_jcoord2olap_md_jcoords , 2 ) allocate ( mdjcoords ( tempsize )) tempsize = size ( cfd_kcoord2olap_md_kcoords , 2 ) allocate ( mdkcoords ( tempsize )) allocate ( olap_mask ( nproc_world )) !Set default values, must be done because coord2rank_md cannot !take \"null\" coordinates. group (:) = olap_null olap_mask (:) = .false. nolap = 0 ! Every process loop over all cfd ranks do trank_cfd = 1 , nproc_cfd ! Get cart coords of cfd rank cfdcoord (:) = rank2coord_cfd (:, trank_cfd ) ! Get md cart coords overlapping cfd proc mdicoords (:) = cfd_icoord2olap_md_icoords ( cfdcoord ( 1 ),:) mdjcoords (:) = cfd_jcoord2olap_md_jcoords ( cfdcoord ( 2 ),:) mdkcoords (:) = cfd_kcoord2olap_md_kcoords ( cfdcoord ( 3 ),:) ! Set group and olap_mask for CFD processor if it overlaps if ( any ( mdicoords .ne. olap_null ) .and. & any ( mdjcoords .ne. olap_null ) .and. & any ( mdkcoords .ne. olap_null )) then trank_world = rank_cfdcart2rank_world ( trank_cfd ) olap_mask ( trank_world ) = .true. group ( trank_world ) = trank_cfd end if ! Set group and olap_mask for MD processors do i = 1 , size ( mdicoords ) do j = 1 , size ( mdjcoords ) do k = 1 , size ( mdkcoords ) ic = mdicoords ( i ) jc = mdjcoords ( j ) kc = mdkcoords ( k ) if ( any (( / ic , jc , kc / ) .eq. olap_null )) cycle trank_md = coord2rank_md ( ic , jc , kc ) trank_world = rank_mdcart2rank_world ( trank_md ) olap_mask ( trank_world ) = .true. group ( trank_world ) = trank_cfd end do end do end do end do call MPI_Barrier ( CPL_REALM_COMM , ierr ) ! Split world Comm into a set of comms for overlapping processors call MPI_comm_split ( CPL_WORLD_COMM , group ( rank_world ), realm , & CPL_OLAP_COMM , ierr ) !Setup Overlap comm sizes and id if ( realm .eq. cfd_realm ) CFDid_olap = myid_olap call MPI_bcast ( CFDid_olap , 1 , MPI_INTEGER , CFDid_olap , CPL_OLAP_COMM , ierr ) ! USED ONLY FOR OUTPUT/TESTING?? !if (myid_olap .eq. CFDid_olap) testval = group(rank_world) !call MPI_bcast(testval,1,MPI_INTEGER,CFDid_olap,CPL_OLAP_COMM,ierr) ! Set all non-overlapping processors to MPI_COMM_NULL if ( olap_mask ( rank_world ) .eqv. .false. ) then myid_olap = olap_null rank_olap = olap_null CPL_OLAP_COMM = MPI_COMM_NULL end if !Setup overlap map call CPL_rank_map ( CPL_OLAP_COMM , rank_olap , nproc_olap , & rank_olap2rank_world , rank_world2rank_olap , ierr ) myid_olap = rank_olap - 1 deallocate ( mdicoords ) deallocate ( mdjcoords ) deallocate ( mdkcoords ) !if (realm.eq.md_realm) call write_overlap_comms_md end subroutine prepare_overlap_comms !========================================================================= !Setup topology graph of overlaps between CFD & MD processors subroutine CPL_overlap_topology use mpi implicit none integer :: i , n , nconnections integer , dimension (:), allocatable :: index , edges logical :: reorder !Allow optimisations of ordering reorder = .true. !Get number of processors in communicating overlap region if ( olap_mask ( rank_world ) .eqv. .true. ) then !CFD processor is root and has mapping to all MD processors allocate ( index ( nproc_olap )) ! Index for each processor allocate ( edges ( 2 * ( nproc_olap ) - 1 )) ! nproc_olap-1 for CFD and one for ! each of nproc_olap MD processors index = 0 ; edges = 0 !CFD processor has connections to nproc_olap MD processors nconnections = nproc_olap - 1 index ( 1 ) = nconnections do n = 1 , nconnections edges ( n ) = n !olap_list(n+1) !CFD connected to all MD processors 1 to nconnections enddo !MD processor has a single connection to CFD nconnections = 1 ; i = 2 do n = nproc_olap + 1 , 2 * ( nproc_olap ) - 1 index ( i ) = index ( i - 1 ) + nconnections !Each successive index incremented by one edges ( n ) = CFDid_olap !Connected to CFD processor i = i + 1 enddo !Create graph topology for overlap region call MPI_Graph_create ( CPL_OLAP_COMM , nproc_olap , index , edges , reorder , CPL_GRAPH_COMM , ierr ) else CPL_GRAPH_COMM = MPI_COMM_NULL endif ! Setup graph map call CPL_rank_map ( CPL_GRAPH_COMM , rank_graph , nproc_olap , & rank_graph2rank_world , rank_world2rank_graph , ierr ) myid_graph = rank_graph - 1 end subroutine CPL_overlap_topology subroutine print_overlap_comms use mpi implicit none integer :: trank if ( myid_world .eq. 0 ) then write ( 7500 + rank_realm , * ), '' write ( 7500 + rank_realm , * ), '----------- OVERLAP COMMS INFO ------------' write ( 7500 + rank_realm , * ), '-------------------------------------------' write ( 7500 + rank_realm , * ), '        RANKS              BROADCAST TEST  ' write ( 7500 + rank_realm , * ), '  world  realm  olap      testval( = group)' write ( 7500 + rank_realm , * ), '-------------------------------------------' end if do trank = 1 , nproc_world if ( rank_world .eq. trank ) then write ( 7500 + rank_realm , '(3i7,i16)' ), rank_world , rank_realm , & rank_olap , testval end if end do if ( myid_world .eq. 0 ) then write ( 7500 + rank_realm , * ), '-------- END OVERLAP COMMS INFO  ----------' write ( 7500 + rank_realm , * ), '===========================================' end if end subroutine print_overlap_comms end subroutine CPL_create_map !============================================================================= !   Adjust CFD domain size to an integer number of lattice units used by !   MD if sizes are given in sigma units !----------------------------------------------------------------------------- subroutine CPL_cfd_adjust_domain ( xL , yL , zL , nx , ny , nz , density_output ) use mpi !use coupler_module, only : density_cfd,CPL_REALM_COMM, rank_realm, ierr implicit none integer , optional , intent ( inout ) :: nx , ny , nz real ( kind ( 0. d0 )), optional , intent ( inout ) :: xL , yL , zL real ( kind ( 0. d0 )), optional , intent ( inout ) :: density_output ! Internal variables integer :: ierror , root !character(1)                                :: direction logical :: changed !Define root processes root = 1 density_output = density_cfd ! Check CFD domain and MD domain are compatible sizes to allow a ! stable initial MD lattice structure - resize if possible or ! stop code and demand a regeneration of grid if vary by more than 0.01 changed = .false. if ( present ( xL )) then call init_length ( xL , resize = .true. , direction = 'x' , & print_warning = changed ) endif ! No need to adjust y because we can adjust DY in MD to ! have an integer number of FCC units. ??????? What if ( present ( zL )) then call init_length ( zL , resize = .true. , direction = 'z' , & print_warning = changed ) endif if ( changed ) then print * , \"Regenerate Grid with corrected sizes as above\" call MPI_Abort ( MPI_COMM_WORLD , ierror , ierr ) endif ! check id CFD cell sizes are larger than 2*sigma call test_cfd_cell_sizes contains !----------------------------------------------------------------------------- subroutine init_length ( rout , resize , direction , print_warning ) !use coupler_module, only: dx,dy,dz,error_abort implicit none real ( kind = kind ( 0. d0 )), intent ( inout ) :: rout logical , intent ( in ) :: resize character ( * ), intent ( in ) :: direction logical , intent ( out ) :: print_warning real ( kind ( 0. d0 )) :: dxyz ! dx, dy or dz real ( kind ( 0. d0 )) :: rinit ! initial val of rout or rin for print print_warning = .false. select case ( direction ) case ( 'x' , 'X' ) dxyz = dx case ( 'y' , 'Y' ) dxyz = dy case ( 'z' , 'Z' ) dxyz = dz case default call error_abort ( 'Wrong direction specified in init_length' ) end select if ( resize ) then rinit = rout rout = real ( nint ( rout / dxyz ), kind ( 0. d0 )) * dxyz print_warning = .true. print * , direction , 'dxyz = ' , dxyz endif if ( print_warning ) then !if (rank_realm .eq. root) then write ( * , '(3(a,/),3a,/,2(a,f20.10),/a,/,a)' ) & \"*********************************************************************\" , & \"WARNING - this is a coupled run which resets CFD domain size         \" , & \" to an integer number of MD initial cells:                           \" , & \"   Domain resized in the \" , direction , \" direction                   \" , & \" inital size =\" , rinit , \" resized \" , rout , & \"                                                                     \" , & \"*********************************************************************\" !endif !If resize is insignificant then return flag print_warning as false if ( abs ( rinit - rout ) .lt. 0.01 ) print_warning = .false. end if end subroutine init_length !----------------------------------------------------------------------------- subroutine test_cfd_cell_sizes implicit none if ( rank_realm .eq. root ) then if ( present ( xL ) .and. present ( nx )) then if ( xL / nx < 2.0 d0 ) then write ( 0 , * ) \" WARNING: CFD cell size in x direction is less that 2 * sigma. Does this make sense?\" write ( 0 , * ) \"          xL=\" , xL , \"nx=\" , nx endif endif if ( present ( yL ) .and. present ( ny )) then if ( yL / ny < 2.0 d0 ) then write ( 0 , * ) \" WARNING: CFD cell size in y direction is less that 2 * sigma. Does this make sense?\" write ( 0 , * ) \"          yL=\" , yL , \"nx=\" , ny endif endif if ( present ( zL ) .and. present ( nz )) then if ( zL / nz < 2.0 d0 ) then write ( 0 , * ) \" WARNING: CFD cell size in z direction is less that 2 * sigma. Does this make sense?\" write ( 0 , * ) \"          zL=\" , zL , \"nx=\" , nz endif endif end if end subroutine test_cfd_cell_sizes end subroutine CPL_cfd_adjust_domain !------------------------------------------------------------------- !                   CPL_rank_map                                   - !------------------------------------------------------------------- ! Get COMM map for current communicator and relationship to ! world rank used to link to others in the coupler hierachy ! - - - Synopsis - - - ! CPL_rank_map(COMM, rank, comm2world, world2comm, ierr) ! - - - Input Parameters - - - !comm !    communicator with cartesian structure (handle) ! - - - Output Parameter - - - !rank !    rank of a process within group of comm (integer) !    NOTE - fortran convention rank=1 to nproc !nproc !    number of processes within group of comm (integer) !comm2world !   Array of size nproc_world which for element at !   world_rank has local rank in COMM !world2comm !   Array of size nproc_COMM which for element at !   for local ranks in COMM has world rank !ierr !    error flag subroutine CPL_rank_map ( COMM , rank , nproc , comm2world , world2comm , ierr ) !use coupler_module, only : rank_world, nproc_world, CPL_WORLD_COMM, VOID use mpi implicit none integer , intent ( in ) :: COMM integer , intent ( out ) :: rank , nproc , ierr integer , dimension (:), allocatable , intent ( out ) :: comm2world , world2comm allocate ( world2comm ( nproc_world )) world2comm ( nproc_world ) = VOID if ( COMM .ne. MPI_COMM_NULL ) then !Mapping from comm rank to world rank call MPI_comm_rank ( COMM , rank , ierr ) rank = rank + 1 call MPI_comm_size ( COMM , nproc , ierr ) allocate ( comm2world ( nproc )) call MPI_allgather ( rank_world , 1 , MPI_INTEGER , & comm2world , 1 , MPI_INTEGER , COMM , ierr ) else rank = VOID allocate ( comm2world ( 0 )) endif !Mapping from world rank to comm rank call MPI_allgather ( rank , 1 , MPI_INTEGER , & world2comm , 1 , MPI_INTEGER , CPL_WORLD_COMM , ierr ) end subroutine CPL_rank_map !--------------------------------------------------- ! Locate file in input subroutine locate ( fileid , keyword , have_data ) implicit none integer , intent ( in ) :: fileid ! File unit number character ( len =* ), intent ( in ) :: keyword ! Input keyword logical , intent ( out ) :: have_data ! Flag: input found character * ( 100 ) :: linestring ! First 100 chars integer :: keyword_length ! Length of keyword integer :: io ! File status flag keyword_length = len ( keyword ) rewind ( fileid ) ! Loop until end of file or keyword found do ! Read first 100 characters of line read ( fileid , '(a)' , iostat = io ) linestring ! If end of file is reached, exit if ( io .ne. 0 ) then have_data = .false. exit end if ! If the first characters match keyword, exit if ( linestring ( 1 : keyword_length ) .eq. keyword ) then have_data = .true. exit endif end do end subroutine locate !=========================================================================== !Error handling subroutines subroutine error_abort_s ( msg ) use mpi implicit none character ( len =* ), intent ( in ), optional :: msg integer errcode , ierr if ( present ( msg )) then write ( * , * ) msg endif call MPI_Abort ( MPI_COMM_WORLD , errcode , ierr ) end subroutine error_abort_s subroutine error_abort_si ( msg , i ) use mpi implicit none character ( len =* ), intent ( in ) :: msg integer , intent ( in ) :: i integer errcode , ierr write ( * , * ) msg , i call MPI_Abort ( MPI_COMM_WORLD , errcode , ierr ) end subroutine error_abort_si subroutine messenger_lasterrorcheck use mpi implicit none integer resultlen character * 12 err_buffer call MPI_Error_string ( ierr , err_buffer , resultlen , ierr ) print * , err_buffer end subroutine messenger_lasterrorcheck !-------------------------------------------------------------------------------------- ! Prints formatted debug statements subroutine printf ( buf , dplaces_in ) implicit none real ( kind ( 0. d0 )), dimension (:), intent ( in ) :: buf integer , intent ( in ), optional :: dplaces_in integer :: n , dplaces , space real ( kind ( 0. d0 )) :: maxbuf , minbuf , order character * 19 :: string character * 42 :: buf_precision space = 2 !Default number of decimal places if not supplied if ( present ( dplaces_in )) then if ( dplaces_in .le. 9 ) then dplaces = dplaces_in else print * , 'Number of decimal places in printf if limited to 9' dplaces = 9 !Maximum endif else dplaces = 4 endif !Find out required format to display maximum element in buffer maxbuf = maxval ( buf ); minbuf = minval ( buf ) maxbuf = max ( maxbuf , 10 * abs ( minbuf )) !10*Ensures extra space for minus sign order = 1. d0 ; n = 1 do while ( max ( maxbuf , order ) .ne. order ) order = order * 1 0. d0 n = n + 1 enddo if ( maxbuf .lt. 0. d0 .and. maxbuf .gt. - 1. d0 ) then n = n + 1 !For the case of -0.something endif if ( n + dplaces + space .le. 9 ) then write ( buf_precision , '(a,i1,a,i1)' ), 'f' , n + dplaces + space , '.' , dplaces else write ( buf_precision , '(a,i2,a,i1)' ), 'f' , n + dplaces + space , '.' , dplaces endif ! Build up format specifier string based on size of passed array string = '(a6,i3,   ' // trim ( buf_precision ) // ')' write ( string ( 8 : 10 ), '(i3)' ), size ( buf ) !Write formatted data print ( string ), 'printf' , rank_world , buf end subroutine printf !-------------------------------------------------------------------------------------- !Write matrix in correct format subroutine write_matrix_int ( a , varname , fh ) implicit none integer :: i , j , fh character ( * ) :: varname integer , dimension (:,:) :: a write ( fh , * ) varname do i = lbound ( a , 1 ), ubound ( a , 1 ) write ( fh , * ) ( a ( i , j ), j = lbound ( a , 2 ), ubound ( a , 2 )) end do end subroutine write_matrix_int subroutine write_matrix ( a , varname , fh ) implicit none integer :: i , j , fh character ( * ) :: varname real ( kind ( 0. d0 )), dimension (:,:) :: a write ( fh , * ) varname do i = lbound ( a , 1 ), ubound ( a , 1 ) write ( fh , * ) ( a ( i , j ), j = lbound ( a , 2 ), ubound ( a , 2 )) end do end subroutine write_matrix !=========================================================================== ! Subroutine that can be used to stop the code when reaching a given ! point in coupler -- useful when coupling new codes !--------------------------------------------------------------------------- !subroutine request_stop(tag) !    use mpi !    implicit none ! !    character(len=*),intent(in) ::tag !    integer myid, ierr ! !    ! do nothing, get out quick !    if(.not. stop_request_activated ) return ! !    if (tag /= stop_request_name) return ! !    select case(stop_request_name) !    case(\"create_comm\",\"CREATE_COMM\") !        call mpi_comm_rank(CPL_REALM_COMM, myid,ierr) !        write(0,*) 'stop as requested at ', trim(stop_request_name), ', realm',realm, 'rank', myid !        call MPI_Finalize(ierr) !        stop !    case(\"create_map\",\"CREATE_MAP\") !        call mpi_comm_rank(CPL_REALM_COMM, myid,ierr) !        write(0,*) 'stop as requested at ', trim(stop_request_name), ', realm',realm, 'rank', myid !        call MPI_Finalize(ierr) !        stop !    case default !        write(0,*) \"WARNING: request abort activated, but the tag is unrecognized, check COUPLER.in\" !        write(0,*) \"         accepted stop tags are: create_comm\" !    end select ! !end subroutine request_stop function CPL_new_fileunit () result ( f ) implicit none logical :: op integer :: f f = 1 do inquire ( f , opened = op ) if ( op .eqv. .false. ) exit f = f + 1 enddo end function end module coupler_module © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"sourcefile/coupler_module.f90.html","title":"coupler_module.f90 – Fortran Program"},{"text":"CPL_overlap Function Source File coupler.f90 coupler CPL_overlap All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function CPL_overlap() Uses: coupler_module Arguments None Return Value logical Description Check if current processor is in the overlap region Synopsis CPL_olap_check() Input Parameters NONE Returns CPL_olap_check True if calling processor is in the overlap region\n and false otherwise @author Edward Smith © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_overlap.html","title":"CPL_overlap – Fortran Program"},{"text":"CPL_comm_style Function Source File coupler.f90 coupler CPL_comm_style All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function CPL_comm_style() Uses: coupler_module Arguments None Return Value integer © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_comm_style.html","title":"CPL_comm_style – Fortran Program"},{"text":"CPL_realm Function Source File coupler.f90 coupler CPL_realm All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function CPL_realm() Uses: coupler_module Arguments None Return Value integer © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_realm.html","title":"CPL_realm – Fortran Program"},{"text":"globalise Function Source File coupler.f90 coupler globalise All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function globalise(r) Uses: coupler_module Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in) :: r (3) Return Value real(kind=kind(0.d0)) Description Get molecule's global position from position local to processor. © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/globalise.html","title":"globalise – Fortran Program"},{"text":"localise Function Source File coupler.f90 coupler localise All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function localise(r) Uses: coupler_module Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in) :: r (3) Return Value real(kind=kind(0.d0)) Description Get local position on processor from molecule's global position. © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/localise.html","title":"localise – Fortran Program"},{"text":"map_md2cfd_global Function Source File coupler.f90 coupler map_md2cfd_global Variables md_only All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function map_md2cfd_global(r) Uses: coupler_module Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in) :: r (3) Return Value real(kind=kind(0.d0)) Description Map global MD position to global CFD coordinate frame Variables Type Visibility Attributes Name Initial real(kind=kind(0.d0)), public :: md_only (3) © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/map_md2cfd_global.html","title":"map_md2cfd_global – Fortran Program"},{"text":"map_cfd2md_global Function Source File coupler.f90 coupler map_cfd2md_global Variables md_only All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function map_cfd2md_global(r) Uses: coupler_module Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in) :: r (3) Return Value real(kind=kind(0.d0)) Description Map global CFD position in global MD coordinate frame Variables Type Visibility Attributes Name Initial real(kind=kind(0.d0)), public :: md_only (3) © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/map_cfd2md_global.html","title":"map_cfd2md_global – Fortran Program"},{"text":"coupler_md_get_save_period Function Source File coupler.f90 coupler coupler_md_get_save_period All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function coupler_md_get_save_period() Uses: coupler_module Arguments None Return Value integer © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/coupler_md_get_save_period.html","title":"coupler_md_get_save_period – Fortran Program"},{"text":"coupler_md_get_average_period Function Source File coupler.f90 coupler coupler_md_get_average_period All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function coupler_md_get_average_period() Uses: coupler_module Arguments None Return Value integer © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/coupler_md_get_average_period.html","title":"coupler_md_get_average_period – Fortran Program"},{"text":"coupler_md_get_md_steps_per_cfd_dt Function Source File coupler.f90 coupler coupler_md_get_md_steps_per_cfd_dt All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function coupler_md_get_md_steps_per_cfd_dt() Uses: coupler_module Arguments None Return Value integer © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/coupler_md_get_md_steps_per_cfd_dt.html","title":"coupler_md_get_md_steps_per_cfd_dt – Fortran Program"},{"text":"coupler_md_get_nsteps Function Source File coupler.f90 coupler coupler_md_get_nsteps All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function coupler_md_get_nsteps() Uses: coupler_module Arguments None Return Value integer © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/coupler_md_get_nsteps.html","title":"coupler_md_get_nsteps – Fortran Program"},{"text":"coupler_md_get_dt_cfd Function Source File coupler.f90 coupler coupler_md_get_dt_cfd All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function coupler_md_get_dt_cfd() Uses: coupler_module Arguments None Return Value real(kind=kind(0.d0)) © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/coupler_md_get_dt_cfd.html","title":"coupler_md_get_dt_cfd – Fortran Program"},{"text":"CPL_gather Subroutine Source File coupler.f90 coupler CPL_gather Variables sendcount recvcounts displs sendbuf recvbuf Subroutines check_limits_consistency prepare_gatherv_parameters pack_sendbuf unpack_recvbuf deallocate_gather_u All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_gather(gatherarray, npercell, limits, recvarray) Uses: mpi coupler_module Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:,:,:,:) :: gatherarray integer, intent(in) :: npercell integer, intent(in) :: limits (6) real(kind=kind(0.d0)), intent(inout), dimension(:,:,:,:) :: recvarray Description Perform gather operation on CPL_OLAP_COMM communicator. The CFD processor\n is the root process. The gathered data is effectively \"slotted\" into the\n correct part of the recvarray, and is intented for use in providing the\n CFD simulation boundary conditions with data obtained from the MD\n simulation. Synopsis CPL_gather(gatherarray,npercell,limits,recvarray) Input gatherarray Assumed shape array of data to be gathered from each MD processor\n     in the overlap communicator. limits Integer array of length 6, specifying the global cell extents of the\n     region to be gathered, is the same on ALL processors. npercell number of data points per cell to be gathered (integer)\n     Note: should be the same as size(gatherarray(1)) for MD\n     processor. E.G. npercell = 3 for gathering 3D velocities. Input/Output recvarray The array in which the gathered values are to be stored on the CFD\n     processor. The only values to be changed in recvarray are:\n     recvarray(limits(1):limits(2),limits(3):limits(4),limits(5):limits(6)) Output Parameters NONE @author David Trevelyan Variables Type Visibility Attributes Name Initial integer, public :: sendcount integer, public, dimension(:), allocatable :: recvcounts integer, public, dimension(:), allocatable :: displs real(kind=kind(0.d0)), public, dimension(:), allocatable :: sendbuf real(kind=kind(0.d0)), public, dimension(:), allocatable :: recvbuf Subroutines subroutine check_limits_consistency() Arguments None Description subroutine prepare_gatherv_parameters() Arguments None Description subroutine pack_sendbuf() Arguments None Description subroutine unpack_recvbuf() Arguments None Description subroutine deallocate_gather_u() Arguments None Description © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_gather.html","title":"CPL_gather – Fortran Program"},{"text":"CPL_scatter Subroutine Source File coupler.f90 coupler CPL_scatter Variables recvcount displs sendcounts recvbuf scatterbuf Subroutines prepare_scatterv_parameters pack_scatterbuf unpack_scatterbuf deallocate_scatter_s All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_scatter(scatterarray, npercell, limits, recvarray) Uses: coupler_module mpi Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:,:,:,:) :: scatterarray integer, intent(in) :: npercell integer, intent(in) :: limits (6) real(kind=kind(0.d0)), intent(inout), dimension(:,:,:,:) :: recvarray Description Scatter cell-wise data from CFD processor to corresponding MD processors\n on the overlap communicator CPL_OLAP_COMM. Synopsis CPL_scatter(scatterarray,npercell,limits,recvarray) Input scatterarray assumed shape array of data to be scattered (real(kind(0.d0))) limits integer array of length 6, specifying the global cell extents of the\n     region to be scattered, is the same on all processors. npercell number of data points per cell to be scattered (integer).\n     Note: should be the same as size(scatterarray(1)) for CFD proc Input/Output recvarray the array in which the scattered values are stored on the MD\n     processors. Output NONE @author David Trevelyan Variables Type Visibility Attributes Name Initial integer, public :: recvcount integer, public, dimension(:), allocatable :: displs integer, public, dimension(:), allocatable :: sendcounts real(kind=kind(0.d0)), public, dimension(:), allocatable :: recvbuf real(kind=kind(0.d0)), public, dimension(:), allocatable :: scatterbuf Subroutines subroutine prepare_scatterv_parameters() Arguments None Description subroutine pack_scatterbuf() Arguments None Description subroutine unpack_scatterbuf() Arguments None Description subroutine deallocate_scatter_s() Arguments None Description © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_scatter.html","title":"CPL_scatter – Fortran Program"},{"text":"CPL_pack Subroutine Source File coupler.f90 coupler CPL_pack Variables pos n nbr id_nbr icell jcell kcell ierr npercell ncells nneighbors coord extents gextents id_neighbors All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_pack(unpacked, packed, realm, icmax_pack, icmin_pack, jcmax_pack, jcmin_pack, kcmax_pack, kcmin_pack) Uses: coupler_module Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:,:,:,:) :: unpacked real(kind=kind(0.d0)), intent(out), dimension(:), allocatable :: packed integer, intent(in) :: realm integer, intent(in), optional :: icmax_pack integer, intent(in), optional :: icmin_pack integer, intent(in), optional :: jcmax_pack integer, intent(in), optional :: jcmin_pack integer, intent(in), optional :: kcmax_pack integer, intent(in), optional :: kcmin_pack Variables Type Visibility Attributes Name Initial integer, public :: pos integer, public :: n integer, public :: nbr integer, public :: id_nbr integer, public :: icell integer, public :: jcell integer, public :: kcell integer, public :: ierr integer, public :: npercell integer, public :: ncells integer, public :: nneighbors integer, public, dimension(3) :: coord integer, public, dimension(6) :: extents integer, public, dimension(6) :: gextents integer, public, dimension(:), allocatable :: id_neighbors © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_pack.html","title":"CPL_pack – Fortran Program"},{"text":"CPL_unpack Subroutine Source File coupler.f90 coupler CPL_unpack Variables pos n nbr id_nbr icell jcell kcell ierr npercell ncells nneighbors coord extents gextents id_neighbors All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_unpack(packed, unpacked, realm) Uses: coupler_module Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(inout), dimension(:), allocatable :: packed real(kind=kind(0.d0)), intent(out), dimension(:,:,:,:), allocatable :: unpacked integer, intent(in) :: realm Variables Type Visibility Attributes Name Initial integer, public :: pos integer, public :: n integer, public :: nbr integer, public :: id_nbr integer, public :: icell integer, public :: jcell integer, public :: kcell integer, public :: ierr integer, public :: npercell integer, public :: ncells integer, public :: nneighbors integer, public, dimension(3) :: coord integer, public, dimension(6) :: extents integer, public, dimension(6) :: gextents integer, public, dimension(:), allocatable :: id_neighbors © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_unpack.html","title":"CPL_unpack – Fortran Program"},{"text":"CPL_proc_extents Subroutine Source File coupler.f90 coupler CPL_proc_extents All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_proc_extents(coord, realm, extents, ncells) Uses: mpi coupler_module Arguments Type Intent Optional Attributes Name integer, intent(in) :: coord (3) integer, intent(in) :: realm integer, intent(out) :: extents (6) integer, intent(out), optional :: ncells Description Gets maximum and minimum cells for processor coordinates Synopsis CPL_proc_extents(coord,realm,extents,ncells) Input coord processor cartesian coordinate (3 x integer) realm cfd_realm (1) or md_realm (2) (integer) Input/Output NONE Output extents Six components array which defines processor extents\n     xmin,xmax,ymin,ymax,zmin,zmax (6 x integer) ncells (optional) number of cells on processor (integer) @author David Trevelyan © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_proc_extents.html","title":"CPL_proc_extents – Fortran Program"},{"text":"CPL_olap_extents Subroutine Source File coupler.f90 coupler CPL_olap_extents All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_olap_extents(coord, realm, extents, ncells) Uses: mpi coupler_module Arguments Type Intent Optional Attributes Name integer, intent(in) :: coord (3) integer, intent(in) :: realm integer, intent(out) :: extents (6) integer, intent(out), optional :: ncells Description Get maximum and minimum cells for current communicator within\n the overlapping region only Synopsis CPL_olap_extents(coord,realm,extents,ncells) Input coord processor cartesian coordinate (3 x integer) realm cfd_realm (1) or md_realm (2) (integer) Input/Output NONE Output extents Six components array which defines processor extents within\n     the overlap region only: xmin,xmax,ymin,ymax,zmin,zmax (6 x integer) ncells (optional) number of cells on processor (integer) @author David Trevelyan © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_olap_extents.html","title":"CPL_olap_extents – Fortran Program"},{"text":"CPL_proc_portion Subroutine Source File coupler.f90 coupler CPL_proc_portion Variables extents All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_proc_portion(coord, realm, limits, portion, ncells) Uses: mpi coupler_module Arguments Type Intent Optional Attributes Name integer, intent(in) :: coord (3) integer, intent(in) :: realm integer, intent(in) :: limits (6) integer, intent(out) :: portion (6) integer, intent(out), optional :: ncells Description Get maximum and minimum cell indices, i.e. the 'portion', of the\n input cell extents 'limits' that is contributed by the current\n overlapping processor. Synopsis CPL_proc_portion(coord,realm,limits,portion,ncells) Input coord processor cartesian coordinate (3 x integer) realm cfd_realm (1) or md_realm (2) (integer) limits(6) Array of cell extents that specify the input region. Input/Output NONE Output portion(6) Array of cell extents that define the local processor's\n     contribution to the input region 'limits'. ncells (optional) number of cells in portion (integer) Note: limits(6) and portion(6) are of the form:\n   (xmin,xmax,ymin,ymax,zmin,zmax) @author David Trevelyan Variables Type Visibility Attributes Name Initial integer, public :: extents (6) © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_proc_portion.html","title":"CPL_proc_portion – Fortran Program"},{"text":"CPL_Cart_coords Subroutine Source File coupler.f90 coupler CPL_Cart_coords Variables worldrank cartrank All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_Cart_coords(COMM, rank, realm, maxdims, coords, ierr) Uses: coupler_module Arguments Type Intent Optional Attributes Name integer, intent(in) :: COMM integer, intent(in) :: rank integer, intent(in) :: realm integer, intent(in) :: maxdims integer, intent(out) :: coords (maxdims) integer, intent(out) :: ierr Description Determines process coords in appropriate realm's cartesian topology \n given a rank in any communicator Synopsis CPL_Cart_coords(COMM, rank, realm, maxdims, coords, ierr) Input Parameters comm communicator with cartesian structure (handle) realm cfd_realm (1) or md_realm (2) (integer) rank rank of a process within group of comm (integer) \n      NOTE fortran convention rank=1 to nproc maxdims length of vector coords in the calling program (integer) Output Parameter coords integer array (of size ndims) containing the Cartesian coordinates \n     of specified process (integer) ierr error flag\n @author Edward Smith Variables Type Visibility Attributes Name Initial integer, public :: worldrank integer, public :: cartrank © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_cart_coords.html","title":"CPL_Cart_coords – Fortran Program"},{"text":"CPL_get_rank Subroutine Source File coupler.f90 coupler CPL_get_rank All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_get_rank(COMM, rank) Uses: coupler_module Arguments Type Intent Optional Attributes Name integer, intent(in) :: COMM integer, intent(out) :: rank Description Return rank of current processor in specified COMM Synopsis CPL_get_rank(COMM, rank) Input Parameters comm communicator with cartesian structure (handle) Output Parameter rank rank of a process within group of comm (integer) \n      NOTE fortran convention rank=1 to nproc @author Edward Smith © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_get_rank.html","title":"CPL_get_rank – Fortran Program"},{"text":"CPL_get Subroutine Source File coupler.f90 coupler CPL_get All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_get(icmax_olap, icmin_olap, jcmax_olap, jcmin_olap, kcmax_olap, kcmin_olap, density_cfd, density_md, dt_cfd, dt_MD, dx, dy, dz, ncx, ncy, ncz, xg, yg, zg, xL_md, xL_cfd, yL_md, yL_cfd, zL_md, zL_cfd, constraint_algo, constraint_CVflag, constraint_OT, constraint_NCER, constraint_Flekkoy, constraint_off, constraint_CV, icmin_cnst, icmax_cnst, jcmin_cnst, jcmax_cnst, kcmin_cnst, kcmax_cnst, md_cfd_match_cellsize, staggered_averages, cpl_cfd_bc_slice, cpl_md_bc_slice, cpl_cfd_bc_x, cpl_cfd_bc_y, cpl_cfd_bc_z, timestep_ratio, comm_style) Uses: coupler_module Arguments Type Intent Optional Attributes Name integer, intent(out), optional :: icmax_olap integer, intent(out), optional :: icmin_olap integer, intent(out), optional :: jcmax_olap integer, intent(out), optional :: jcmin_olap integer, intent(out), optional :: kcmax_olap integer, intent(out), optional :: kcmin_olap real(kind=kind(0.d0)), intent(out), optional :: density_cfd real(kind=kind(0.d0)), intent(out), optional :: density_md real(kind=kind(0.d0)), intent(out), optional :: dt_cfd real(kind=kind(0.d0)), intent(out), optional :: dt_MD real(kind=kind(0.d0)), intent(out), optional :: dx real(kind=kind(0.d0)), intent(out), optional :: dy real(kind=kind(0.d0)), intent(out), optional :: dz integer, intent(out), optional :: ncx integer, intent(out), optional :: ncy integer, intent(out), optional :: ncz real(kind=kind(0.d0)), intent(out), optional dimension(:,:), allocatable :: xg real(kind=kind(0.d0)), intent(out), optional dimension(:,:), allocatable :: yg real(kind=kind(0.d0)), intent(out), optional dimension(:), allocatable :: zg real(kind=kind(0.d0)), intent(out), optional :: xL_md real(kind=kind(0.d0)), intent(out), optional :: xL_cfd real(kind=kind(0.d0)), intent(out), optional :: yL_md real(kind=kind(0.d0)), intent(out), optional :: yL_cfd real(kind=kind(0.d0)), intent(out), optional :: zL_md real(kind=kind(0.d0)), intent(out), optional :: zL_cfd integer, intent(out), optional :: constraint_algo integer, intent(out), optional :: constraint_CVflag integer, intent(out), optional :: constraint_OT integer, intent(out), optional :: constraint_NCER integer, intent(out), optional :: constraint_Flekkoy integer, intent(out), optional :: constraint_off integer, intent(out), optional :: constraint_CV integer, intent(out), optional :: icmin_cnst integer, intent(out), optional :: icmax_cnst integer, intent(out), optional :: jcmin_cnst integer, intent(out), optional :: jcmax_cnst integer, intent(out), optional :: kcmin_cnst integer, intent(out), optional :: kcmax_cnst integer, intent(out), optional :: md_cfd_match_cellsize logical, intent(out), optional dimension(3) :: staggered_averages integer, intent(out), optional :: cpl_cfd_bc_slice integer, intent(out), optional :: cpl_md_bc_slice integer, intent(out), optional :: cpl_cfd_bc_x integer, intent(out), optional :: cpl_cfd_bc_y integer, intent(out), optional :: cpl_cfd_bc_z integer, intent(out), optional :: timestep_ratio integer, intent(out), optional :: comm_style Description Wrapper to retrieve (read only) parameters from the coupler_module \n Note - this ensures all variable in the coupler are protected\n from corruption by either CFD or MD codes Synopsis CPL_get([see coupler_module]) Input Parameters NONE Output Parameter @see coupler_module @author Edward Smith © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_get.html","title":"CPL_get – Fortran Program"},{"text":"CPL_send Interface Source File coupler.f90 coupler CPL_send Module Procedures CPL_send_3d CPL_send_4d All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public interface CPL_send Module Procedures private  subroutine CPL_send_3d(temp, icmin_send, icmax_send, jcmin_send, jcmax_send, kcmin_send, kcmax_send, send_flag) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:,:,:) :: temp integer, intent(in), optional :: icmin_send integer, intent(in), optional :: icmax_send integer, intent(in), optional :: jcmin_send integer, intent(in), optional :: jcmax_send integer, intent(in), optional :: kcmin_send integer, intent(in), optional :: kcmax_send logical, intent(out), optional :: send_flag Description CPL_send_data wrapper for 3d arrays\n see CPL_send_xd for input description\n @see coupler#subroutine_CPL_send_xd private  subroutine CPL_send_4d(asend, icmin_send, icmax_send, jcmin_send, jcmax_send, kcmin_send, kcmax_send, send_flag) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:,:,:,:) :: asend integer, intent(in), optional :: icmin_send integer, intent(in), optional :: icmax_send integer, intent(in), optional :: jcmin_send integer, intent(in), optional :: jcmax_send integer, intent(in), optional :: kcmin_send integer, intent(in), optional :: kcmax_send logical, intent(out), optional :: send_flag Description CPL_send_data wrapper for 4d arrays\n see CPL_send_xd for input description\n @see coupler#subroutine_CPL_send_xd © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"interface/cpl_send.html","title":"CPL_send – Fortran Program"},{"text":"CPL_recv Interface Source File coupler.f90 coupler CPL_recv Module Procedures CPL_recv_3d CPL_recv_4d All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public interface CPL_recv Module Procedures private  subroutine CPL_recv_3d(temp, icmin_recv, icmax_recv, jcmin_recv, jcmax_recv, kcmin_recv, kcmax_recv, recv_flag) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(inout), dimension(:,:,:) :: temp integer, intent(in), optional :: icmin_recv integer, intent(in), optional :: icmax_recv integer, intent(in), optional :: jcmin_recv integer, intent(in), optional :: jcmax_recv integer, intent(in), optional :: kcmin_recv integer, intent(in), optional :: kcmax_recv logical, intent(out), optional :: recv_flag Description CPL_recv_xd wrapper for 3d arrays\n see CPL_recv_xd for input description\n @see coupler#subroutine_CPL_recv_xd private  subroutine CPL_recv_4d(arecv, icmin_recv, icmax_recv, jcmin_recv, jcmax_recv, kcmin_recv, kcmax_recv, recv_flag) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(out), dimension(:,:,:,:) :: arecv integer, intent(in), optional :: icmin_recv integer, intent(in), optional :: icmax_recv integer, intent(in), optional :: jcmin_recv integer, intent(in), optional :: jcmax_recv integer, intent(in), optional :: kcmin_recv integer, intent(in), optional :: kcmax_recv logical, intent(out), optional :: recv_flag Description CPL_recv_xd  wrapper for 4d arrays\n See CPL_recv_xd for input description\n @see coupler#subroutine_CPL_recv_xd © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"interface/cpl_recv.html","title":"CPL_recv – Fortran Program"},{"text":"CPL_new_fileunit Function Source File coupler_module.f90 coupler_module CPL_new_fileunit Variables op All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  function CPL_new_fileunit() Arguments None Return Value integer Variables Type Visibility Attributes Name Initial logical, public :: op © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_new_fileunit.html","title":"CPL_new_fileunit – Fortran Program"},{"text":"CPL_create_comm Subroutine Source File coupler_module.f90 coupler_module CPL_create_comm Subroutines print_cplheader test_realms create_comm All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_create_comm(callingrealm, RETURNED_REALM_COMM, ierror) Uses: mpi Arguments Type Intent Optional Attributes Name integer, intent(in) :: callingrealm integer, intent(out) :: RETURNED_REALM_COMM integer, intent(out) :: ierror Subroutines subroutine print_cplheader() Arguments None Description subroutine test_realms() Arguments None Description subroutine create_comm() Arguments None Description © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_create_comm.html","title":"CPL_create_comm – Fortran Program"},{"text":"read_coupler_input Subroutine Source File coupler_module.f90 coupler_module read_coupler_input Variables infileid found All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine read_coupler_input() Arguments None Variables Type Visibility Attributes Name Initial integer, public :: infileid logical, public :: found © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/read_coupler_input.html","title":"read_coupler_input – Fortran Program"},{"text":"CPL_write_header Subroutine Source File coupler_module.f90 coupler_module CPL_write_header Variables found infileid the_date the_time All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_write_header(header_filename) Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: header_filename Description Writes header information to specified filename in the format\n Variable description ; variable name ; variable Synopsis CPL_write_header (header_filename) Input header_filename File name to write header to Input/Output NONE Output NONE @author Edward Smith Variables Type Visibility Attributes Name Initial logical, public :: found integer, public :: infileid character(len=8), public :: the_date character(len=10), public :: the_time © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_write_header.html","title":"CPL_write_header – Fortran Program"},{"text":"coupler_cfd_init Subroutine Source File coupler_module.f90 coupler_module coupler_cfd_init Variables i ib jb kb pcoords source nproc buf rank_world2rank_realm rank_world2rank_cart dxmin dxmax dzmin dzmax rbuf Subroutines check_mesh All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine coupler_cfd_init(nsteps, dt, icomm_grid, icoord, npxyz_cfd, xyzL, ncxyz, density, ijkcmax, ijkcmin, iTmin, iTmax, jTmin, jTmax, kTmin, kTmax, xgrid, ygrid, zgrid) Uses: mpi Arguments Type Intent Optional Attributes Name integer, intent(in) :: nsteps real(kind=kind(0.d0)), intent(in) :: dt integer, intent(in) :: icomm_grid integer, intent(in), dimension(:,:) :: icoord integer, intent(in), dimension(3) :: npxyz_cfd real(kind=kind(0.d0)), intent(in), dimension(3) :: xyzL integer, intent(in), dimension(3) :: ncxyz real(kind=kind(0.d0)), intent(in) :: density integer, intent(in), dimension(3) :: ijkcmax integer, intent(in), dimension(3) :: ijkcmin integer, intent(in), dimension(:) :: iTmin integer, intent(in), dimension(:) :: iTmax integer, intent(in), dimension(:) :: jTmin integer, intent(in), dimension(:) :: jTmax integer, intent(in), dimension(:) :: kTmin integer, intent(in), dimension(:) :: kTmax real(kind=kind(0.d0)), intent(in), dimension(:,:) :: xgrid real(kind=kind(0.d0)), intent(in), dimension(:,:) :: ygrid real(kind=kind(0.d0)), intent(in), dimension(:  ) :: zgrid Description Initialisation routine for coupler module - Every variable is sent and stored\n to ensure both md and cfd region have an identical list of parameters Synopsis coupler_cfd_init(nsteps,dt_cfd,icomm_grid,icoord,npxyz_cfd,xyzL,ncxyz,\n                             density,ijkcmax,ijkcmin,iTmin,iTmax,jTmin,\n                             jTmax,kTmin,kTmax,xg,yg,zg) Input nsteps Number of time steps the CFD code is expected to run for (integer) dt_cfd CFD timestep (dp real) icomm_grid The MPI communicator setup by the MPI_CART_CREATE command in the \n     CFD region (integer) icoord The three coordinate for each rank in the domain (integer array nproc by 3) npxyz_cfd Number of processors in each cartesian dimension (integer array 3) xyzL Size of domain in each cartesian dimension (dp real array 3) ncxyz Global number of cells in each cartesian dimension (integer array 3) density Density of the CFD simulation (dp_real) ijkcmax Global maximum cell in each cartesian dimension (integer array 3) ijkcmin Global minimum cell in each cartesian dimension (integer array 3) iTmin Local minimum cell for each rank (integer array no. procs in x) iTmax Local maximum cell for each rank (integer array no. procs in x) jTmin Local minimum cell for each rank (integer array no. procs in y) jTmax Local maximum cell for each rank (integer array no. procs in y) kTmin Local minimum cell for each rank (integer array no. procs in z) kTmax Local maximum cell for each rank (integer array no. procs in z) xg Array of cell vertices in the x direction (no. cells in x + 1 by \n     no. cells in y + 1) yg Array of cell vertices in the y direction (no. cells in x + 1 by \n     no. cells in y + 1) zg Array of cell vertices in the z direction (no. cells in z + 1) Input/Output NONE Output NONE @author Edward Smith Variables Type Visibility Attributes Name Initial integer, public :: i integer, public :: ib integer, public :: jb integer, public :: kb integer, public :: pcoords (3) integer, public :: source integer, public :: nproc integer, public, dimension(:), allocatable :: buf integer, public, dimension(:), allocatable :: rank_world2rank_realm integer, public, dimension(:), allocatable :: rank_world2rank_cart real(kind=kind(0.d0)), public :: dxmin real(kind=kind(0.d0)), public :: dxmax real(kind=kind(0.d0)), public :: dzmin real(kind=kind(0.d0)), public :: dzmax real(kind=kind(0.d0)), public, dimension(:), allocatable :: rbuf Subroutines subroutine check_mesh() Arguments None Description © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/coupler_cfd_init.html","title":"coupler_cfd_init – Fortran Program"},{"text":"coupler_md_init Subroutine Source File coupler_module.f90 coupler_module coupler_md_init Variables i ib jb kb pcoords source nproc buf rank_world2rank_realm rank_world2rank_cart rbuf All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine coupler_md_init(nsteps, initialstep, dt, icomm_grid, icoord, npxyz_md, globaldomain, density) Uses: mpi Arguments Type Intent Optional Attributes Name integer, intent(inout) :: nsteps integer, intent(inout) :: initialstep real(kind=kind(0.d0)), intent(in) :: dt integer, intent(in) :: icomm_grid integer, intent(in), dimension(:,:) :: icoord integer, intent(in), dimension(3) :: npxyz_md real(kind=kind(0.d0)), intent(in), dimension(3) :: globaldomain real(kind=kind(0.d0)), intent(in) :: density Description Initialisation routine for coupler module - Every variable is sent and stored\n to ensure both md and cfd region have an identical list of parameters Synopsis coupler_mf_init(nsteps,dt_md,icomm_grid,icoord,npxyz_md,globaldomain,density) Input nsteps Number of time steps the MD code is expected to run for (integer) dt_md MD timestep (dp real) icomm_grid The MPI communicator setup by the MPI_CART_CREATE command in the \n     CFD region (integer) icoord The three coordinate for each rank in the domain (integer array nproc by 3) npxyz_md Number of processors in each cartesian dimension (integer array 3) globaldomain Size of domain in each cartesian dimension (dp real array 3) density Density of the CFD simulation (dp_real) Input/Output NONE Output NONE @author Edward Smith Variables Type Visibility Attributes Name Initial integer, public :: i integer, public :: ib integer, public :: jb integer, public :: kb integer, public :: pcoords (3) integer, public :: source integer, public :: nproc integer, public, dimension(:), allocatable :: buf integer, public, dimension(:), allocatable :: rank_world2rank_realm integer, public, dimension(:), allocatable :: rank_world2rank_cart real(kind=kind(0.d0)), public, dimension(:), allocatable :: rbuf © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/coupler_md_init.html","title":"coupler_md_init – Fortran Program"},{"text":"set_coupled_timing Subroutine Source File coupler_module.f90 coupler_module set_coupled_timing Variables elapsedtime Nsteps_MDperCFD All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine set_coupled_timing(initialstep, Nsteps) Arguments Type Intent Optional Attributes Name integer, intent(in) :: initialstep integer, intent(out) :: Nsteps Variables Type Visibility Attributes Name Initial real(kind=kind(0.d0)), public :: elapsedtime integer, public :: Nsteps_MDperCFD © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/set_coupled_timing.html","title":"set_coupled_timing – Fortran Program"},{"text":"CPL_create_map Subroutine Source File coupler_module.f90 coupler_module CPL_create_map Subroutines check_config_feasibility get_md_cell_ranges get_overlap_blocks prepare_overlap_comms CPL_overlap_topology print_overlap_comms All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_create_map() Uses: mpi Arguments None Description map %rank_list ( 1 ) = coord2rank_cfd ( pcoords ( 1 ), pcoords ( 2 ), pcoords ( 3 )) Subroutines subroutine check_config_feasibility() Arguments None Description subroutine get_md_cell_ranges() Arguments None Description Store the minimum and maximum CFD cell coordinates that overlap each\n MD processor. subroutine get_overlap_blocks() Arguments None Description Store MD processor coordinates that overlap each CFD processor coordinate. subroutine prepare_overlap_comms() Arguments None Description @author David Trevelyan \n Splits the world communicator into \"overlap\" communicators. Each overlap \n communicator consists of a CFD root processor and the MD processors which\n lie on top of it in the domain. subroutine CPL_overlap_topology() Arguments None Description subroutine print_overlap_comms() Arguments None Description © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_create_map.html","title":"CPL_create_map – Fortran Program"},{"text":"CPL_cfd_adjust_domain Subroutine Source File coupler_module.f90 coupler_module CPL_cfd_adjust_domain Variables ierror root changed Subroutines init_length test_cfd_cell_sizes All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_cfd_adjust_domain(xL, yL, zL, nx, ny, nz, density_output) Uses: mpi Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(inout), optional :: xL real(kind=kind(0.d0)), intent(inout), optional :: yL real(kind=kind(0.d0)), intent(inout), optional :: zL integer, intent(inout), optional :: nx integer, intent(inout), optional :: ny integer, intent(inout), optional :: nz real(kind=kind(0.d0)), intent(inout), optional :: density_output Variables Type Visibility Attributes Name Initial integer, public :: ierror integer, public :: root logical, public :: changed Subroutines subroutine init_length(rout, resize, direction, print_warning) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(inout) :: rout logical, intent(in) :: resize character(len=*), intent(in) :: direction logical, intent(out) :: print_warning Description subroutine test_cfd_cell_sizes() Arguments None Description © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_cfd_adjust_domain.html","title":"CPL_cfd_adjust_domain – Fortran Program"},{"text":"CPL_rank_map Subroutine Source File coupler_module.f90 coupler_module CPL_rank_map All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine CPL_rank_map(COMM, rank, nproc, comm2world, world2comm, ierr) Uses: mpi Arguments Type Intent Optional Attributes Name integer, intent(in) :: COMM integer, intent(out) :: rank integer, intent(out) :: nproc integer, intent(out), dimension(:), allocatable :: comm2world integer, intent(out), dimension(:), allocatable :: world2comm integer, intent(out) :: ierr © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/cpl_rank_map.html","title":"CPL_rank_map – Fortran Program"},{"text":"locate Subroutine Source File coupler_module.f90 coupler_module locate Variables linestring keyword_length io All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine locate(fileid, keyword, have_data) Arguments Type Intent Optional Attributes Name integer, intent(in) :: fileid character(len=*), intent(in) :: keyword logical, intent(out) :: have_data Variables Type Visibility Attributes Name Initial character(len=1), public :: linestring integer, public :: keyword_length integer, public :: io © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/locate.html","title":"locate – Fortran Program"},{"text":"messenger_lasterrorcheck Subroutine Source File coupler_module.f90 coupler_module messenger_lasterrorcheck Variables resultlen err_buffer All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine messenger_lasterrorcheck() Uses: mpi Arguments None Variables Type Visibility Attributes Name Initial integer, public :: resultlen character(len=2), public :: err_buffer © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/messenger_lasterrorcheck.html","title":"messenger_lasterrorcheck – Fortran Program"},{"text":"printf Subroutine Source File coupler_module.f90 coupler_module printf Variables n dplaces space maxbuf minbuf order string buf_precision All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine printf(buf, dplaces_in) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:) :: buf integer, intent(in), optional :: dplaces_in Variables Type Visibility Attributes Name Initial integer, public :: n integer, public :: dplaces integer, public :: space real(kind=kind(0.d0)), public :: maxbuf real(kind=kind(0.d0)), public :: minbuf real(kind=kind(0.d0)), public :: order character(len=9), public :: string character(len=2), public :: buf_precision © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/printf.html","title":"printf – Fortran Program"},{"text":"write_matrix_int Subroutine Source File coupler_module.f90 coupler_module write_matrix_int Variables i j All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine write_matrix_int(a, varname, fh) Arguments Type Intent Optional Attributes Name integer, intent(inout), dimension(:,:) :: a character(len=*), intent(inout) :: varname integer, intent(inout) :: fh Variables Type Visibility Attributes Name Initial integer, public :: i integer, public :: j © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/write_matrix_int.html","title":"write_matrix_int – Fortran Program"},{"text":"write_matrix Subroutine Source File coupler_module.f90 coupler_module write_matrix Variables i j All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public  subroutine write_matrix(a, varname, fh) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(inout), dimension(:,:) :: a character(len=*), intent(inout) :: varname integer, intent(inout) :: fh Variables Type Visibility Attributes Name Initial integer, public :: i integer, public :: j © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"proc/write_matrix.html","title":"write_matrix – Fortran Program"},{"text":"error_abort Interface Source File coupler_module.f90 coupler_module error_abort Module Procedures error_abort_s error_abort_si All Procedures coupler_cfd_init coupler_md_get_average_period coupler_md_get_dt_cfd coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_save_period coupler_md_init CPL_Cart_coords CPL_cfd_adjust_domain CPL_comm_style CPL_create_comm CPL_create_map CPL_gather CPL_get CPL_get_rank CPL_new_fileunit CPL_olap_extents CPL_overlap CPL_pack CPL_proc_extents CPL_proc_portion CPL_rank_map CPL_realm CPL_recv CPL_scatter CPL_send CPL_unpack CPL_write_header error_abort globalise localise locate map_cfd2md_global map_md2cfd_global messenger_lasterrorcheck printf read_coupler_input set_coupled_timing write_matrix write_matrix_int public interface error_abort Module Procedures private  subroutine error_abort_s(msg) Arguments Type Intent Optional Attributes Name character(len=*), intent(in), optional :: msg Description private  subroutine error_abort_si(msg, i) Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: msg integer, intent(in) :: i Description © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"interface/error_abort.html","title":"error_abort – Fortran Program"},{"text":"coupler Module Source File coupler.f90 coupler Interfaces CPL_send CPL_recv Functions CPL_overlap CPL_comm_style CPL_realm globalise localise map_md2cfd_global map_cfd2md_global coupler_md_get_save_period coupler_md_get_average_period coupler_md_get_md_steps_per_cfd_dt coupler_md_get_nsteps coupler_md_get_dt_cfd Subroutines CPL_gather CPL_scatter CPL_pack CPL_unpack CPL_proc_extents CPL_olap_extents CPL_proc_portion CPL_Cart_coords CPL_get_rank CPL_get All Modules coupler coupler_module Uses: ISO_C_BINDING mpi coupler_module coupler_module mpi coupler_module coupler_module mpi coupler_module coupler_module coupler_module mpi coupler_module coupler_module coupler_module mpi coupler_module mpi coupler_module mpi coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module coupler_module Interfaces public interface CPL_send private  subroutine CPL_send_3d(temp, icmin_send, icmax_send, jcmin_send, jcmax_send, kcmin_send, kcmax_send, send_flag) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:,:,:) :: temp integer, intent(in), optional :: icmin_send integer, intent(in), optional :: icmax_send integer, intent(in), optional :: jcmin_send integer, intent(in), optional :: jcmax_send integer, intent(in), optional :: kcmin_send integer, intent(in), optional :: kcmax_send logical, intent(out), optional :: send_flag Description CPL_send_data wrapper for 3d arrays\n see CPL_send_xd for input description\n @see coupler#subroutine_CPL_send_xd private  subroutine CPL_send_4d(asend, icmin_send, icmax_send, jcmin_send, jcmax_send, kcmin_send, kcmax_send, send_flag) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:,:,:,:) :: asend integer, intent(in), optional :: icmin_send integer, intent(in), optional :: icmax_send integer, intent(in), optional :: jcmin_send integer, intent(in), optional :: jcmax_send integer, intent(in), optional :: kcmin_send integer, intent(in), optional :: kcmax_send logical, intent(out), optional :: send_flag Description CPL_send_data wrapper for 4d arrays\n see CPL_send_xd for input description\n @see coupler#subroutine_CPL_send_xd public interface CPL_recv private  subroutine CPL_recv_3d(temp, icmin_recv, icmax_recv, jcmin_recv, jcmax_recv, kcmin_recv, kcmax_recv, recv_flag) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(inout), dimension(:,:,:) :: temp integer, intent(in), optional :: icmin_recv integer, intent(in), optional :: icmax_recv integer, intent(in), optional :: jcmin_recv integer, intent(in), optional :: jcmax_recv integer, intent(in), optional :: kcmin_recv integer, intent(in), optional :: kcmax_recv logical, intent(out), optional :: recv_flag Description CPL_recv_xd wrapper for 3d arrays\n see CPL_recv_xd for input description\n @see coupler#subroutine_CPL_recv_xd private  subroutine CPL_recv_4d(arecv, icmin_recv, icmax_recv, jcmin_recv, jcmax_recv, kcmin_recv, kcmax_recv, recv_flag) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(out), dimension(:,:,:,:) :: arecv integer, intent(in), optional :: icmin_recv integer, intent(in), optional :: icmax_recv integer, intent(in), optional :: jcmin_recv integer, intent(in), optional :: jcmax_recv integer, intent(in), optional :: kcmin_recv integer, intent(in), optional :: kcmax_recv logical, intent(out), optional :: recv_flag Description CPL_recv_xd  wrapper for 4d arrays\n See CPL_recv_xd for input description\n @see coupler#subroutine_CPL_recv_xd Functions public  function CPL_overlap () Arguments None Return Value logical Description Check if current processor is in the overlap region public  function CPL_comm_style () Arguments None Return Value integer Description public  function CPL_realm () Arguments None Return Value integer Description public  function globalise (r) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in) :: r (3) Return Value real(kind=kind(0.d0))\n  (3) Description Get molecule's global position from position local to processor. public  function localise (r) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in) :: r (3) Return Value real(kind=kind(0.d0))\n  (3) Description Get local position on processor from molecule's global position. public  function map_md2cfd_global (r) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in) :: r (3) Return Value real(kind=kind(0.d0))\n  (3) Description Map global MD position to global CFD coordinate frame public  function map_cfd2md_global (r) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in) :: r (3) Return Value real(kind=kind(0.d0))\n  (3) Description Map global CFD position in global MD coordinate frame public  function coupler_md_get_save_period () Arguments None Return Value integer Description public  function coupler_md_get_average_period () Arguments None Return Value integer Description public  function coupler_md_get_md_steps_per_cfd_dt () Arguments None Return Value integer Description public  function coupler_md_get_nsteps () Arguments None Return Value integer Description public  function coupler_md_get_dt_cfd () Arguments None Return Value real(kind=kind(0.d0)) Description Subroutines public  subroutine CPL_gather (gatherarray, npercell, limits, recvarray) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:,:,:,:) :: gatherarray integer, intent(in) :: npercell integer, intent(in) :: limits (6) real(kind=kind(0.d0)), intent(inout), dimension(:,:,:,:) :: recvarray Description Perform gather operation on CPL_OLAP_COMM communicator. The CFD processor\n is the root process. The gathered data is effectively \"slotted\" into the\n correct part of the recvarray, and is intented for use in providing the\n CFD simulation boundary conditions with data obtained from the MD\n simulation. public  subroutine CPL_scatter (scatterarray, npercell, limits, recvarray) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:,:,:,:) :: scatterarray integer, intent(in) :: npercell integer, intent(in) :: limits (6) real(kind=kind(0.d0)), intent(inout), dimension(:,:,:,:) :: recvarray Description Scatter cell-wise data from CFD processor to corresponding MD processors\n on the overlap communicator CPL_OLAP_COMM. public  subroutine CPL_pack (unpacked, packed, realm, icmax_pack, icmin_pack, jcmax_pack, jcmin_pack, kcmax_pack, kcmin_pack) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:,:,:,:) :: unpacked real(kind=kind(0.d0)), intent(out), dimension(:), allocatable :: packed integer, intent(in) :: realm integer, intent(in), optional :: icmax_pack integer, intent(in), optional :: icmin_pack integer, intent(in), optional :: jcmax_pack integer, intent(in), optional :: jcmin_pack integer, intent(in), optional :: kcmax_pack integer, intent(in), optional :: kcmin_pack Description public  subroutine CPL_unpack (packed, unpacked, realm) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(inout), dimension(:), allocatable :: packed real(kind=kind(0.d0)), intent(out), dimension(:,:,:,:), allocatable :: unpacked integer, intent(in) :: realm Description public  subroutine CPL_proc_extents (coord, realm, extents, ncells) Arguments Type Intent Optional Attributes Name integer, intent(in) :: coord (3) integer, intent(in) :: realm integer, intent(out) :: extents (6) integer, intent(out), optional :: ncells Description Gets maximum and minimum cells for processor coordinates public  subroutine CPL_olap_extents (coord, realm, extents, ncells) Arguments Type Intent Optional Attributes Name integer, intent(in) :: coord (3) integer, intent(in) :: realm integer, intent(out) :: extents (6) integer, intent(out), optional :: ncells Description Get maximum and minimum cells for current communicator within\n the overlapping region only public  subroutine CPL_proc_portion (coord, realm, limits, portion, ncells) Arguments Type Intent Optional Attributes Name integer, intent(in) :: coord (3) integer, intent(in) :: realm integer, intent(in) :: limits (6) integer, intent(out) :: portion (6) integer, intent(out), optional :: ncells Description Get maximum and minimum cell indices, i.e. the 'portion', of the\n input cell extents 'limits' that is contributed by the current\n overlapping processor. public  subroutine CPL_Cart_coords (COMM, rank, realm, maxdims, coords, ierr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: COMM integer, intent(in) :: rank integer, intent(in) :: realm integer, intent(in) :: maxdims integer, intent(out) :: coords (maxdims) integer, intent(out) :: ierr Description Determines process coords in appropriate realm's cartesian topology \n given a rank in any communicator public  subroutine CPL_get_rank (COMM, rank) Arguments Type Intent Optional Attributes Name integer, intent(in) :: COMM integer, intent(out) :: rank Description Return rank of current processor in specified COMM public  subroutine CPL_get (icmax_olap, icmin_olap, jcmax_olap, jcmin_olap, kcmax_olap, kcmin_olap, density_cfd, density_md, dt_cfd, dt_MD, dx, dy, dz, ncx, ncy, ncz, xg, yg, zg, xL_md, xL_cfd, yL_md, yL_cfd, zL_md, zL_cfd, constraint_algo, constraint_CVflag, constraint_OT, constraint_NCER, constraint_Flekkoy, constraint_off, constraint_CV, icmin_cnst, icmax_cnst, jcmin_cnst, jcmax_cnst, kcmin_cnst, kcmax_cnst, md_cfd_match_cellsize, staggered_averages, cpl_cfd_bc_slice, cpl_md_bc_slice, cpl_cfd_bc_x, cpl_cfd_bc_y, cpl_cfd_bc_z, timestep_ratio, comm_style) Arguments Type Intent Optional Attributes Name integer, intent(out), optional :: icmax_olap integer, intent(out), optional :: icmin_olap integer, intent(out), optional :: jcmax_olap integer, intent(out), optional :: jcmin_olap integer, intent(out), optional :: kcmax_olap integer, intent(out), optional :: kcmin_olap real(kind=kind(0.d0)), intent(out), optional :: density_cfd real(kind=kind(0.d0)), intent(out), optional :: density_md real(kind=kind(0.d0)), intent(out), optional :: dt_cfd real(kind=kind(0.d0)), intent(out), optional :: dt_MD real(kind=kind(0.d0)), intent(out), optional :: dx real(kind=kind(0.d0)), intent(out), optional :: dy real(kind=kind(0.d0)), intent(out), optional :: dz integer, intent(out), optional :: ncx integer, intent(out), optional :: ncy integer, intent(out), optional :: ncz real(kind=kind(0.d0)), intent(out), optional dimension(:,:), allocatable :: xg real(kind=kind(0.d0)), intent(out), optional dimension(:,:), allocatable :: yg real(kind=kind(0.d0)), intent(out), optional dimension(:), allocatable :: zg real(kind=kind(0.d0)), intent(out), optional :: xL_md real(kind=kind(0.d0)), intent(out), optional :: xL_cfd real(kind=kind(0.d0)), intent(out), optional :: yL_md real(kind=kind(0.d0)), intent(out), optional :: yL_cfd real(kind=kind(0.d0)), intent(out), optional :: zL_md real(kind=kind(0.d0)), intent(out), optional :: zL_cfd integer, intent(out), optional :: constraint_algo integer, intent(out), optional :: constraint_CVflag integer, intent(out), optional :: constraint_OT integer, intent(out), optional :: constraint_NCER integer, intent(out), optional :: constraint_Flekkoy integer, intent(out), optional :: constraint_off integer, intent(out), optional :: constraint_CV integer, intent(out), optional :: icmin_cnst integer, intent(out), optional :: icmax_cnst integer, intent(out), optional :: jcmin_cnst integer, intent(out), optional :: jcmax_cnst integer, intent(out), optional :: kcmin_cnst integer, intent(out), optional :: kcmax_cnst integer, intent(out), optional :: md_cfd_match_cellsize logical, intent(out), optional dimension(3) :: staggered_averages integer, intent(out), optional :: cpl_cfd_bc_slice integer, intent(out), optional :: cpl_md_bc_slice integer, intent(out), optional :: cpl_cfd_bc_x integer, intent(out), optional :: cpl_cfd_bc_y integer, intent(out), optional :: cpl_cfd_bc_z integer, intent(out), optional :: timestep_ratio integer, intent(out), optional :: comm_style Description Wrapper to retrieve (read only) parameters from the coupler_module \n Note - this ensures all variable in the coupler are protected\n from corruption by either CFD or MD codes © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"module/coupler.html","title":"coupler – Fortran Program"},{"text":"coupler_module Module Source File coupler_module.f90 coupler_module Variables VOID cfd_realm md_realm realm_name COUPLER_ERROR_REALM COUPLER_ERROR_ONE_REALM COUPLER_ERROR_INIT COUPLER_ERROR_INPUT_FILE COUPLER_ERROR_READ_INPUT COUPLER_ERROR_CONTINUUM_FORCE COUPLER_ABORT_ON_REQUEST COUPLER_ABORT_SEND_CFD COUPLER_ERROR_CART_COMM ierr CPL_WORLD_COMM CPL_REALM_COMM CPL_INTER_COMM CPL_CART_COMM CPL_OLAP_COMM CPL_GRAPH_COMM CPL_REALM_INTERSECTION_COMM realm myid_world rank_world rootid_world myid_realm rank_realm rootid_realm myid_cart rank_cart rootid_cart myid_olap rank_olap CFDid_olap myid_graph rank_graph rank_world2rank_mdrealm rank_world2rank_mdcart rank_world2rank_cfdrealm rank_world2rank_cfdcart rank_world2rank_olap rank_world2rank_graph rank_world2rank_inter rank_mdrealm2rank_world rank_mdcart2rank_world rank_cfdrealm2rank_world rank_cfdcart2rank_world rank_olap2rank_world rank_graph2rank_world rank_inter2rank_world rank_olap2rank_realm nproc_md nproc_cfd nproc_olap nproc_world npx_md npy_md npz_md npx_cfd npy_cfd npz_cfd olap_mask rank2coord_cfd rank2coord_md coord2rank_cfd coord2rank_md iblock_realm jblock_realm kblock_realm ncx ncy ncz icmin icmax jcmin jcmax kcmin kcmax icmin_olap icmax_olap jcmin_olap jcmax_olap kcmin_olap kcmax_olap ncx_olap ncy_olap ncz_olap constraint_algo constraint_CVflag icmin_cnst icmax_cnst jcmin_cnst jcmax_cnst kcmin_cnst kcmax_cnst cpl_cfd_bc_x cpl_cfd_bc_y cpl_cfd_bc_z cpl_md_bc_slice cpl_cfd_bc_slice constraint_off constraint_OT constraint_NCER constraint_Flekkoy constraint_CV icPmin_md icPmax_md jcPmin_md jcPmax_md kcPmin_md kcPmax_md icPmin_cfd icPmax_cfd jcPmin_cfd jcPmax_cfd kcPmin_cfd kcPmax_cfd xL_md yL_md zL_md xL_cfd yL_cfd zL_cfd xL_olap yL_olap zL_olap xLl yLl zLl dx dy dz dymin dymax xg yg zg cfd_icoord2olap_md_icoords cfd_jcoord2olap_md_jcoords cfd_kcoord2olap_md_kcoords nsteps_md nsteps_cfd nsteps_coupled average_period save_period dt_md dt_cfd density_md density_cfd timestep_ratio md_cfd_match_cellsize testval staggered_averages comm_style comm_style_send_recv comm_style_gath_scat Interfaces error_abort Functions CPL_new_fileunit Subroutines CPL_create_comm read_coupler_input CPL_write_header coupler_cfd_init coupler_md_init set_coupled_timing CPL_create_map CPL_cfd_adjust_domain CPL_rank_map locate messenger_lasterrorcheck printf write_matrix_int write_matrix All Modules coupler coupler_module Uses: ISO_C_BINDING mpi mpi mpi mpi mpi mpi mpi mpi mpi (cfd+md) Splits MPI_COMM_WORLD in both the CFD and MD code respectively\n         and create intercommunicator between CFD and MD Read Coupler input file Establish for all MD processors the mapping (if any) \n to coupled CFD processors Variables Type Visibility Attributes Name Initial integer, public, parameter :: VOID = -666 VOID value for data initialisation integer, public, parameter :: cfd_realm = 1 CFD realm identifier integer, public, parameter :: md_realm = 2 MD realm identifier character(len=*), public, parameter :: realm_name (2) = (/\"CFD\",\"CFD\"/) Used with realm identifier to get name error codes integer, public, parameter :: COUPLER_ERROR_REALM = 1 wrong realm value integer, public, parameter :: COUPLER_ERROR_ONE_REALM = 2 one realm missing integer, public, parameter :: COUPLER_ERROR_INIT = 3 initialisation error integer, public, parameter :: COUPLER_ERROR_INPUT_FILE = 4 wrong value in input file integer, public, parameter :: COUPLER_ERROR_READ_INPUT = 5 error in processing input file or data transfers integer, public, parameter :: COUPLER_ERROR_CONTINUUM_FORCE = 6 the region in which the continuum constrain force is apply spans over two MD domains integer, public, parameter :: COUPLER_ABORT_ON_REQUEST = 7 used in request_abort integer, public, parameter :: COUPLER_ABORT_SEND_CFD = 8 error in coupler_cfd_send integer, public, parameter :: COUPLER_ERROR_CART_COMM = 9 Wrong comm value in CPL_Cart_coords MPI error flag integer, public :: ierr integer, protected :: CPL_WORLD_COMM Copy of MPI_COMM_WORLD, both CFD and MD realms; integer, protected :: CPL_REALM_COMM INTRA communicators within MD/CFD realms; integer, protected :: CPL_INTER_COMM CFD/MD INTER communicator between realm comms; integer, protected :: CPL_CART_COMM Comm w/cartesian topology for each realm; integer, protected :: CPL_OLAP_COMM Local comm between only overlapping MD/CFD procs; integer, protected :: CPL_GRAPH_COMM Comm w/ graph topolgy between locally olapg procs; integer, protected :: CPL_REALM_INTERSECTION_COMM Intersecting MD/CFD procs in world; Simulation realms integer, protected :: realm integer, protected :: myid_world Processor ID from 0 to nproc_world-1; integer, protected :: rank_world Processor rank from 1 to nproc_world; integer, protected :: rootid_world Root processor in world; integer, protected :: myid_realm Processor ID from 0 to nproc_realm-1; integer, protected :: rank_realm Processor rank from 1 to nproc_realm; integer, protected :: rootid_realm Root processor in each realm; integer, protected :: myid_cart Processor ID from 0 to nproc_cart-1; integer, protected :: rank_cart Processor rank from 1 to nproc_cart; integer, protected :: rootid_cart Root processor in each cart topology; integer, protected :: myid_olap Processor ID from 0 to nproc_olap-1; integer, protected :: rank_olap Processor rank from 1 to nproc_olap; integer, protected :: CFDid_olap Root processor in overlap is the CFD processor; integer, protected :: myid_graph Processor ID from 0 to nproc_graph-1; integer, protected :: rank_graph Processor rank from 1 to nproc_graph; Get rank in CPL_world_COMM from rank in local COMM integer, protected, dimension(:), allocatable :: rank_world2rank_mdrealm integer, protected, dimension(:), allocatable :: rank_world2rank_mdcart integer, protected, dimension(:), allocatable :: rank_world2rank_cfdrealm integer, protected, dimension(:), allocatable :: rank_world2rank_cfdcart integer, protected, dimension(:), allocatable :: rank_world2rank_olap integer, protected, dimension(:), allocatable :: rank_world2rank_graph integer, protected, dimension(:), allocatable :: rank_world2rank_inter Get rank in local COMM from rank in CPL_world_COMM integer, protected, dimension(:), allocatable :: rank_mdrealm2rank_world integer, protected, dimension(:), allocatable :: rank_mdcart2rank_world integer, protected, dimension(:), allocatable :: rank_cfdrealm2rank_world integer, protected, dimension(:), allocatable :: rank_cfdcart2rank_world integer, protected, dimension(:), allocatable :: rank_olap2rank_world integer, protected, dimension(:), allocatable :: rank_graph2rank_world integer, protected, dimension(:), allocatable :: rank_inter2rank_world integer, protected, dimension(:), allocatable :: rank_olap2rank_realm integer, protected :: nproc_md Total number of processor in md integer, protected :: nproc_cfd Total number of processor in cfd integer, protected :: nproc_olap Total number of processor in overlap region integer, protected :: nproc_world Total number of processor in world integer, protected :: npx_md Number of processor in x in the md integer, protected :: npy_md Number of processor in y in the md integer, protected :: npz_md Number of processor in z in the md integer, protected :: npx_cfd Number of processor in x in the cfd integer, protected :: npy_cfd Number of processor in y in the cfd integer, protected :: npz_cfd Number of processor in z in the cfd logical, protected, dimension(:), allocatable :: olap_mask Overlap mask specifying which processors overlap using world ranks integer, protected, dimension(:,:), allocatable :: rank2coord_cfd integer, protected, dimension(:,:), allocatable :: rank2coord_md Array containing coordinates for each cartesian rank integer, protected, dimension(:,:,:), allocatable :: coord2rank_cfd integer, protected, dimension(:,:,:), allocatable :: coord2rank_md Processor cartesian coords integer, protected :: iblock_realm integer, protected :: jblock_realm integer, protected :: kblock_realm integer, protected :: ncx integer, protected :: ncy integer, protected :: ncz integer, protected :: icmin integer, protected :: icmax integer, protected :: jcmin integer, protected :: jcmax integer, protected :: kcmin integer, protected :: kcmax integer, protected :: icmin_olap integer, protected :: icmax_olap integer, protected :: jcmin_olap integer, protected :: jcmax_olap integer, protected :: kcmin_olap integer, protected :: kcmax_olap integer, protected :: ncx_olap integer, protected :: ncy_olap integer, protected :: ncz_olap integer, protected :: constraint_algo integer, protected :: constraint_CVflag integer, protected :: icmin_cnst integer, protected :: icmax_cnst integer, protected :: jcmin_cnst integer, protected :: jcmax_cnst integer, protected :: kcmin_cnst integer, protected :: kcmax_cnst integer, protected :: cpl_cfd_bc_x integer, protected :: cpl_cfd_bc_y integer, protected :: cpl_cfd_bc_z integer, protected :: cpl_md_bc_slice integer, protected :: cpl_cfd_bc_slice integer, public, parameter :: constraint_off = 0 integer, public, parameter :: constraint_OT = 1 integer, public, parameter :: constraint_NCER = 2 integer, public, parameter :: constraint_Flekkoy = 3 integer, public, parameter :: constraint_CV = 4 integer, protected, dimension(:), allocatable :: icPmin_md integer, protected, dimension(:), allocatable :: icPmax_md integer, protected, dimension(:), allocatable :: jcPmin_md integer, protected, dimension(:), allocatable :: jcPmax_md integer, protected, dimension(:), allocatable :: kcPmin_md integer, protected, dimension(:), allocatable :: kcPmax_md integer, protected, dimension(:), allocatable :: icPmin_cfd integer, protected, dimension(:), allocatable :: icPmax_cfd integer, protected, dimension(:), allocatable :: jcPmin_cfd integer, protected, dimension(:), allocatable :: jcPmax_cfd integer, protected, dimension(:), allocatable :: kcPmin_cfd integer, protected, dimension(:), allocatable :: kcPmax_cfd real(kind=kind(0.d0)), protected :: xL_md real(kind=kind(0.d0)), protected :: yL_md real(kind=kind(0.d0)), protected :: zL_md real(kind=kind(0.d0)), protected :: xL_cfd real(kind=kind(0.d0)), protected :: yL_cfd real(kind=kind(0.d0)), protected :: zL_cfd real(kind=kind(0.d0)), protected :: xL_olap real(kind=kind(0.d0)), protected :: yL_olap real(kind=kind(0.d0)), protected :: zL_olap real(kind=kind(0.d0)), protected :: xLl real(kind=kind(0.d0)), protected :: yLl real(kind=kind(0.d0)), protected :: zLl real(kind=kind(0.d0)), protected :: dx real(kind=kind(0.d0)), protected :: dy real(kind=kind(0.d0)), protected :: dz real(kind=kind(0.d0)), protected :: dymin real(kind=kind(0.d0)), protected :: dymax real(kind=kind(0.d0)), protected, dimension(:,:), allocatable, target :: xg real(kind=kind(0.d0)), protected, dimension(:,:), allocatable, target :: yg real(kind=kind(0.d0)), protected, dimension(:), allocatable, target :: zg integer, protected, dimension(:,:), allocatable :: cfd_icoord2olap_md_icoords integer, protected, dimension(:,:), allocatable :: cfd_jcoord2olap_md_jcoords integer, protected, dimension(:,:), allocatable :: cfd_kcoord2olap_md_kcoords integer, protected :: nsteps_md integer, protected :: nsteps_cfd integer, protected :: nsteps_coupled integer, protected :: average_period = 1 integer, protected :: save_period = 10 real(kind=kind(0.d0)), protected :: dt_md real(kind=kind(0.d0)), protected :: dt_cfd real(kind=kind(0.d0)), protected :: density_md real(kind=kind(0.d0)), protected :: density_cfd integer, protected :: timestep_ratio integer, protected :: md_cfd_match_cellsize integer, protected :: testval logical, protected :: staggered_averages (3) = (/.false.,.false.,.false./) integer, protected :: comm_style integer, public, parameter :: comm_style_send_recv = 0 integer, public, parameter :: comm_style_gath_scat = 1 Interfaces public interface error_abort private  subroutine error_abort_s(msg) Arguments Type Intent Optional Attributes Name character(len=*), intent(in), optional :: msg Description private  subroutine error_abort_si(msg, i) Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: msg integer, intent(in) :: i Description Functions public  function CPL_new_fileunit () Arguments None Return Value integer Description Subroutines public  subroutine CPL_create_comm (callingrealm, RETURNED_REALM_COMM, ierror) Arguments Type Intent Optional Attributes Name integer, intent(in) :: callingrealm integer, intent(out) :: RETURNED_REALM_COMM integer, intent(out) :: ierror Description public  subroutine read_coupler_input () Arguments None Description public  subroutine CPL_write_header (header_filename) Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: header_filename Description Writes header information to specified filename in the format\n Variable description ; variable name ; variable public  subroutine coupler_cfd_init (nsteps, dt, icomm_grid, icoord, npxyz_cfd, xyzL, ncxyz, density, ijkcmax, ijkcmin, iTmin, iTmax, jTmin, jTmax, kTmin, kTmax, xgrid, ygrid, zgrid) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nsteps real(kind=kind(0.d0)), intent(in) :: dt integer, intent(in) :: icomm_grid integer, intent(in), dimension(:,:) :: icoord integer, intent(in), dimension(3) :: npxyz_cfd real(kind=kind(0.d0)), intent(in), dimension(3) :: xyzL integer, intent(in), dimension(3) :: ncxyz real(kind=kind(0.d0)), intent(in) :: density integer, intent(in), dimension(3) :: ijkcmax integer, intent(in), dimension(3) :: ijkcmin integer, intent(in), dimension(:) :: iTmin integer, intent(in), dimension(:) :: iTmax integer, intent(in), dimension(:) :: jTmin integer, intent(in), dimension(:) :: jTmax integer, intent(in), dimension(:) :: kTmin integer, intent(in), dimension(:) :: kTmax real(kind=kind(0.d0)), intent(in), dimension(:,:) :: xgrid real(kind=kind(0.d0)), intent(in), dimension(:,:) :: ygrid real(kind=kind(0.d0)), intent(in), dimension(:  ) :: zgrid Description Initialisation routine for coupler module - Every variable is sent and stored\n to ensure both md and cfd region have an identical list of parameters public  subroutine coupler_md_init (nsteps, initialstep, dt, icomm_grid, icoord, npxyz_md, globaldomain, density) Arguments Type Intent Optional Attributes Name integer, intent(inout) :: nsteps integer, intent(inout) :: initialstep real(kind=kind(0.d0)), intent(in) :: dt integer, intent(in) :: icomm_grid integer, intent(in), dimension(:,:) :: icoord integer, intent(in), dimension(3) :: npxyz_md real(kind=kind(0.d0)), intent(in), dimension(3) :: globaldomain real(kind=kind(0.d0)), intent(in) :: density Description Initialisation routine for coupler module - Every variable is sent and stored\n to ensure both md and cfd region have an identical list of parameters public  subroutine set_coupled_timing (initialstep, Nsteps) Arguments Type Intent Optional Attributes Name integer, intent(in) :: initialstep integer, intent(out) :: Nsteps Description public  subroutine CPL_create_map () Arguments None Description public  subroutine CPL_cfd_adjust_domain (xL, yL, zL, nx, ny, nz, density_output) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(inout), optional :: xL real(kind=kind(0.d0)), intent(inout), optional :: yL real(kind=kind(0.d0)), intent(inout), optional :: zL integer, intent(inout), optional :: nx integer, intent(inout), optional :: ny integer, intent(inout), optional :: nz real(kind=kind(0.d0)), intent(inout), optional :: density_output Description public  subroutine CPL_rank_map (COMM, rank, nproc, comm2world, world2comm, ierr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: COMM integer, intent(out) :: rank integer, intent(out) :: nproc integer, intent(out), dimension(:), allocatable :: comm2world integer, intent(out), dimension(:), allocatable :: world2comm integer, intent(out) :: ierr Description public  subroutine locate (fileid, keyword, have_data) Arguments Type Intent Optional Attributes Name integer, intent(in) :: fileid character(len=*), intent(in) :: keyword logical, intent(out) :: have_data Description public  subroutine messenger_lasterrorcheck () Arguments None Description public  subroutine printf (buf, dplaces_in) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(in), dimension(:) :: buf integer, intent(in), optional :: dplaces_in Description public  subroutine write_matrix_int (a, varname, fh) Arguments Type Intent Optional Attributes Name integer, intent(inout), dimension(:,:) :: a character(len=*), intent(inout) :: varname integer, intent(inout) :: fh Description public  subroutine write_matrix (a, varname, fh) Arguments Type Intent Optional Attributes Name real(kind=kind(0.d0)), intent(inout), dimension(:,:) :: a character(len=*), intent(inout) :: varname integer, intent(inout) :: fh Description © 2015 Fortran Program was written by Edward Smith\nDavid Trevelyan. Documentation generated by FORD .","tags":"","loc":"module/coupler_module.html","title":"coupler_module – Fortran Program"}]}