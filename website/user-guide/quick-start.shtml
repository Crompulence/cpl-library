<!DOCTYPE html>
<html lang="en-UK">

<head>
    <link rel="stylesheet" href="../styles.css">
    <script type="text/javascript" src="/details-shim.js"></script>
</head>

<body>

    <div class="center">

        <!--#include virtual="/nav.html"-->

        <h2>Quick-start guide: coupling with CPL Library</h2>
        The aim of the following text is to lead the user, by example, from the
        start to the end; from downloading <mark class="CPL">CPL library</mark>, all the way through to
        running and analysing a coupled simulation. 

        Please <a href="/contact.shtml">get in touch</a> if something is
        unclear.

        <p>
            Guide contents:
        </p>

        <ul>
            <a href=#5min><li> Five Minute Quickstart</li></a>
            <a href=#prerequisites><li> Pre-requisites</li></a>
            <a href=#download ><li>Download</li></a>
            <a href=#compilation ><li>Compilation</li></a>
            <a href=#MLE><li>Demo: a minimal linking example</li></a>
            <a href=#linking><li>Demo: a minimum send/recv example</li></a>
            <a href=#mock><li>CPL Mocks</li></a>
            <a href=#analysing><li>Demo: visualising a coupled simulation</li></a>
            <a href=#interactive><li>Demo: Interactive GUI using CPL library</li></a>
            <a href=#MD_CFD><li>Minimal CFD and MD code with coupling</li></a>
        </ul>


        <h2 id="5min">Five Minute Quickstart</h2>

        <p>
            Assuming you have <a href="https://www.docker.com/">Docker</a>, the easiest way to try out
            <mark class="CPL">CPL library</mark> on your local linux computer is with the following command,

            <div class="code">
                sudo docker run --name cplrun -it cpllibrary/cpl-library 
            </div>

            more details of running <mark class="CPL">CPL library</mark> with Docker can be found on the
            <a href=http://www.cpl-library.org/wiki/index.php/CPL_library_in_Docker> wiki</a>.
        </p>


        <details>
            <summary> I don't use Docker </summary>
            <p>
                If you don't use Docker, or prefer not to, <mark class="CPL">CPL library</mark> should be fairly easy to 
                install on a unix based system, add the <a href=#prerequisites>Pre-requisites</a> 
                and then follow the install instructions in the 
                <a href=#download>download</a> and
                <a href=#compilation>compilation</a> sections. 
                Make sure /lib/libcpl.so exists and you have called 
                <div class="code">
                    source SOURCEME.sh
                </div>
                in the cpl-library folder and then return to this section and continue.
            </p>
        </details>

        <p>
            In the Docker container (or installed <mark class="CPL">CPL library</mark> folder), 
            change directory to the examples,
        </p>
            <div class="code">
                cd ./examples/minimal_send_recv_mocks 
            </div>
        <p>
            These examples show the minimal code needed to run a coupled simulation in the
            three supported languages, Fortran, C++ and Python. 
            Have a look at the code in these files, you will see the CPL_init, CPL_setup, 
            CPL_send and CPL_recv commands, as well as boilerplate to setup MPI and
            create a cartesian topology.
            You can compile Fortran code using,
        </p>
        <div class="code">
            cplf90 minimal_CFD.f90 -o ./CFD
        </div>
        <p>
            for C++ code, you compile using,
        </p>
            <div class="code">
                cplc++ minimal_MD.cpp -o ./MD
            </div>
        <p>
            A coupled run is then performed using,
        </p>
            <div class="code">
                cplexec -c 1 ./CFD -m 1 ./MD
            </div>   
        <p>
            This will run each executable on one processes and link them together,
            exchanging information using MPI and printing to screen the output from both codes.
            Similarly, the other Python
        </p>
            <div class="code">
                cplexec -c 1 ./minimal_CFD.py -m 1 ./MD
            </div>   
        <p>
            or the other Fortran or C++ codes could be compiled and used instead.
            The runner <code> cplexec </code> is simply a Python wrapper which runs both excutables as separate MPI
            processes with their own MPI_COMM_WORLDS, which and then linked together using MPI Port. 
            Alternatively, the two executables can be run in MPMD mode,
            using,
        </p>
            <div class="code">
                mpiexec -n 1 ./CFD : -n 1 ./MD
            </div>
        <p>
            where they share a single MPI_COMM_WORLD.
        </p>

        <p>
            That's it, this example contains the basic premise of <mark class="CPL">CPL library</mark>,
            run two codes, link them together, setup a topological mapping and exchange information.
            Next step can be to adjust the input parameter in any of the dummy codes to see what happens,
            run with different numbers of processes by adjusting the input values.
        </p>

        <p>
            Once you understand the way <mark class="CPL">CPL library</mark> links two any 
            executables in Fortran, C++ or Python and exchanges information through CPL send 
            and CPL recv, you can move on to the other examples. The interactive examples may
            need display setup in Docker, see the
            <a href=http://www.cpl-library.org/wiki/index.php/CPL_library_in_Docker> wiki</a>).
            You can then look into OpenFOAM and LAMMPS coupling,
        </p>
            <div class="code">
                sudo docker run --name cplopenfoamlammps -it cpllibrary/cpl-openfoam-lammps
            </div>
        <p>
            And check out the examples in cpl-library/examples/coupled, as well as test folders
            in the two coupling APPS, /CPL_APP_LAMMPS-DEV, 
            <a href=https://github.com/Crompulence/CPL_APP_LAMMPS-DEV/tree/master/test/> here </a> 
            and /OpenFOAM/CPL_APP_OPENFOAM-3.0.1
            <a href=https://github.com/Crompulence/CPL_APP_OPENFOAM-3.0.1/tree/master/examples> here</a>.
            The APPS use the minimal dummy scripts above like
            <a href=https://en.wikipedia.org/wiki/Mock_object> mocks </a> to test
            coupling and develop new code. Combined with assert statements and testing
            functions, it is possible to build up a range of automated tests for coupled
            codes to divide and validate a coupled simulation. 
        </p>   

        <h2 id="prerequisites">Pre-requisites</h2>
        Before you begin installing <mark class="CPL">CPL library</mark> either locally or
        on your supercomputer, make sure you have the following installed on your
        system:
        <ul>
            <li>A Fortran compiler that supports the F2008 standard</li>
            <li>A C/C++ compiler that supports the C++11 standard </li>
            <li>An MPI library - currently MPICH is supported</li>
        </ul>

        <mark class="CPL">CPL library</mark> was developed and tested using the
        <a href="https://gcc.gnu.org/">GCC compiler collection </a>
        and <a href="https://www.mpich.org">MPICH</a>,
        which are both free and open-source. The intel compiler collection
        and associated intel mpi (derived from MPICH) also work as expected.
        The following guide assumes that you're working with a Unix-like operating
        system like Linux or OS X. Docker may work cross-platform but has not been
        tested so please <a href="/contact.shtml">email us</a>
        if you'd like help getting it working for a new system.

        <p>
        <em> 
            IMPORTANT NOTE: To minimise compatibility problems, 
            you are advised to install gcc-5 and gfortran-5 or later
            (from a <a href="/faq.shtml">ppa</a> or build it yourself).
            If you have a modern linux distribution (e.g. 16.04 or later)
            then MPICH can also be got from a ppa. 
            On older systems, it may be best to download the latest version of MPICH and
            build this new version with a version of gcc and gfortran later than 5. 
        </em>

        <details>
            <summary> MPICH with gcc-5 setup </summary>

             <div class="code"> $ ./configure --prefix \ <br>
                                /PATH/TO/INSTALL/mpich_gcc5/ \ <br>
                                              CXX=gcc-5 FC=gfortran-5 <br>
                                $make <br>
                                $make install <br>
                                $ export PATH:/PATH/TO/INSTALL/mpich_gcc5/:$PATH <br>
             </div>
        </details>



        <em>
            You should then build <mark class="CPL">CPL library</mark> and ALL the codes you 
            plan to couple with the same compiler and version of mpic++ and mpif90.
            Although this sounds like a lot of work, especially if you
            have custom versions of codes already installed, it is
            absolutely essential to guarantee correct behaviour in a coupled 
            simulation.
            The advice to to find out the version of mpi used by the code which is slowest to build
            (e.g. OpenFOAM can take up to 8 hours) and use this to build everything else.
            For supercomputers, you will need to load from modules compatible versions
            of MPI and GCC (or intel), and ensure that the two programs that you plan to couple
            are built with these same packages. Please see the 
            <a href=http://www.cpl-library.org/wiki/index.php?title=Setup_of_OpenFOAM&section=3#Coupled_OpenFOAM_on_HPC> wiki </a> 
            for more discussion of using installed modeul code in a coupled simulation on a supercomputer (an OpenFOAM example).
        </em>
        </P>

        <p>
            For testing and visualisation, you will optionally need a range of Python tools,
            these include,

            <ul>
                <li>The following packages for Python2.7 from a package manager: python-dev, python-pip, python-tk and python-wxgtk3.0, </li>
                <li>From pip, install matplotlib, numpy, scipy, pytest and mpi4py 
                    (ensure mpi4py is using the same MPI as <mark class="CPL">CPL library</mark> and all other coupled codes)</li>
            </ul>
        </p>

        <h2 id="download">Download</h2>

        <p>
            To keep up-to-date with bug-fixes and future developments, the
            user is encouraged to download the git version:
            <div class="code">
                <p>
                    $  git clone https://github.com/Crompulence/cpl-library.git
                </p>
            </div>
            which will download to a new folder in the current directory named
            "cpl-library". 
        </p>

        <p> 
            If you don't want to use git, You can also download from github as a zip file,
            <p>
                https://github.com/Crompulence/cpl-library/archive/master.zip
            </p>
            Then unzip the file and rename
            the folder to "cpl-library": 
            <div class="code"> $  unzip cpl-library-master.zip </div> 
            <div class="code"> $  mv cpl-library-master cpl-library</div> 

            Older release versions with Zenodo DOI links are also available on the
            href="/download.shtml">download page</a>. 
        </p>


        <h2 id="compilation">Compilation</h2>

        <p>
            Compiling <mark class="CPL">CPL library</mark> itself, as a shared library, is
            likely to work with a single Gnu make command:

            <div class="code"> $  cd PATH/TO/cpl-library/</div>
            <div class="code"> $  make PLATFORM=gcc </div>
        </p>

        <p>
            The platform commands specifies the platform specific options,
            a list of which are available in the cpl-library/make folder.
            This build has been verified for the GCC compiler collection and
            MPICH. Support for Intel (R)
            compiler suites have also been tested on various platforms. 
            For more specific build requirements, the user should copy and change
            the compiler template file as outlined in cpl-library/README.            

            Note that <mark class="CPL">CPL library</mark> is not installed. 
            Instead, a library will be created at the path,
        </p>

            <div class="code"> cpl-library/lib/ </div>
    
        <p>
            C++ headers and Fortran use-modules are placed in
        </p>
            
        <div class="code"> cpl-library/include/ </div>

        <p>
            The wrapper scripts for cplf90, cplc++ and cplexec are located in
        </p>
            
        <div class="code"> cpl-library/bin/ </div>

        <p>
            These can be added to the user's path by calling,
        </p>

        <div class="code"> $  source SOURCEME.sh </div>

        <p>
            which can be added to the user .bashrc if desired, and 
            is preferred to installing into the system directory as
            different versions of CPL library may be used with different 
            version of MPI or compilers. Note, a make install option is
            provided but has not been fully tested for all versions of linux.
        </p>


        <h2 id="MLE">Demo: a minimal linking example</h2>


        <p>
            All examples are all included in the /cpl-library/examples folder
            with the <mark class="CPL">CPL library</mark> software.
        </p>


        <p>
            This simple example shows you how to link two massively-parallel 
            codes with <mark class="CPL">CPL library</mark>. 
            The example MD code is written in Fortran,
            and the CFD code in C++. Both programs may run with any number of
            processes. Take a look at the simple Fortran example code:
        </p>

        <div class="code">
            <!--#include virtual="demo-codes/md_init_fortran.html"-->
        </div>

        <p>
            This code initialises MPI, and gets the MD communicator from
            <mark class="CPL">CPL library</mark>. It must, therefore, be linked
            to both the MPI and CPL libraries when it is compiled. The 
            compilation can be performed with a call to mpif90 of the form:
        </p>
 

        <div class="code"> 

            $  mpif90 ./cfd_init.f90 \ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                      -I${CPL_PATH}/include/cpl \ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      -L${CPL_PATH}/lib  \ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      -Wl,-rpath=${CPL_PATH}/lib/ \ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      -o ./mdProgram -lcpl

        </div>

        <p>
            If you're not familiar with the processes involved in linking
            shared libraries to software, there are a few things to notice
            in the above command. 
            The variable CPL_PATH should be set to the location of the base
            directory of the <mark class="CPL">CPL library</mark> downloaded,
            unzipped and build in the previous section. 
            First, we've specified the location of the 
            Fortran module files with the <code>-I</code> flag. 
            Second, the location of the cpl-library shared library, 
            <code>libcpl.so</code>, is specified with <code>-L</code>. 
            Because the library is a dynamic one, not in the standard
            <code>/usr/lib</code> location, we also provide the built 
            executable with a reference to the library location with <br>
            <code>-Wl,-rpath=${CPL_PATH}/lib/</code>. 
            Note: all compiler flags for the MPI library
            are handled automatically by the <code>mpic++</code> and
            <code>mpif90</code> compiler wrappers. 
<!--
            Finally, because the library is
            written with both C++ and Fortran, <code>-lc</code> and
            <code>-lstdc++</code> are required
            to link to the C++ standard libraries with the Fortran compiler.
            The executable is saved to <code>md/mdProgram</code>. 
-->
            Two helper scripts similar to MPI are provided to simplify linking in
            ${CPL_PATH}/bin folder. These can be included in the path using,
        </p>

        <div class="code">
            $ source SOURCEME.sh
        </div>

        <p>
            Note if you want to specify the cpl path to a different directory than the location
            of SOURCEME.sh, you need to set
        </p>

        <div class="code"> $  CPL_PATH=/PATH/TO/cpl-library/</div>

        <p>
            manually. After adding  <mark class="CPL">CPL library</mark>  to your
            path with SOURCEME.sh, the previous build statement then simply becomes,
        </p>

        <div class="code"> cplf90 ./md_init.f90 -o ./mdProgram </div>

        <p>
            Next we build an equivalent CFD demo program written in C++:
        </p>

        <div class="code">
            <!--#include virtual="demo-codes/cfd_init_cpp.html"-->
        </div>

        <p>
            Like the Fortran example above, the compilation command 
            is of the form:
        </p>
 
        <div class="code"> 

            $ mpic++ ./cfd_init.cpp \ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                     -I${CPL_PATH}/include/cpl \ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  
                     -L${CPL_PATH}/lib  \ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                     -Wl,-rpath=${CPL_PATH}/lib/ \ <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                     -o ./cfdProgram -lcpl  <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 

        </div>

        <p>
            This time, the <code>-I</code> flag tells the compiler where to
            find the <mark class="CPL">CPL library</mark> C++ header files, and we must link to the 
            <code>-lgfortran</code> and <code>-lmpifort</code> Fortran libraries.
            Alternativly using cplc++ wrapper,
        </p>

        <div class="code"> cplc++ ./cfd_init.cpp -o ./mdProgram </div>

        <p>
            Now the demo codes are compiled, they can be executed together in 
            the run directory with:
        </p>
     
        <div class="code"> 
            mpiexec -n 8 ./mdProgram : -n 4 ./cfdProgram
        </div>
    
        <p>
            That's it! The codes should execute, and <mark class="CPL">CPL library</mark> will tell you
            that its internal communicators have been initialised.
            Obviously it is rarely that simple, if you get any problems,
            please check out the <a class="links" href='/faq.shtml'>faq</a>
            section and if this doesn't help, please get in touch.
        </p>

        <p>
            The fortran code can also be easily replaced with a python example,
        </p>

        <div class="code">
            <!--#include virtual="demo-codes/md_init_python.html"-->
        </div>

        <p>
            there is no need to compile, but the
            path must be correctly set. This can also be done using,
        </p>

        <div class="code">
            $ source SOURCEME.sh
        </div>

        <p>
            if you haven't already. 
            SOURCEME.sh appends the library location and python ctype bindings 
            to the PYTHONPATH variable. A python and C++ coupled
            example can then be run using,
        </p>

        <div class="code"> 
            mpiexec -n 8 python ./md_init.py : -n 4 ./cfdProgram
        </div>
 
        <p>
            These examples don't actually do anything interesting, and so
            in the next section, the minimal code is presented to send and recieve data.
        </p>

        <h2 id="linking">Demo: a minimum send/recv example</h2>

        <p>
            This section outlines a first and simplest example of 
            coupling. An array of cells with three components per cell
            is packed with the x, y and z global index of the cell,
            shown here for 2D indices only,
        </p>

        <img width=80% src="./images/cfd_md_cell_index.png" >

        <p>
            This is sent, from CFD to MD and checked.
            Although this isn't the most exciting example, it does
            provide a clear picture of exactly how <mark class="CPL">CPL library</mark> works.
            The global system of cells is setup and split over many processors
            in both the MD and CFD codes. The coupled simulation has some 
            overlap, specified in the COUPLER.in file in the cpl folder. This
            determines how the two codes overlap and the topological
            relationship between them. The mapping of processors is handled
            by <mark class="CPL">CPL library</mark> using a minimal set of inputs.
            First, initialisation is performed with a call to 
            <a class="links" href='/api-docs/fortran_api.shtml#f/_/cpl_init'>CPL_init</a>.
            This splits MPI_COMM_WORLD into two MPI communicators for the MD and CFD realms. 
            The grid information and topological mapping is performed using calls to
            <a class="links" href='/api-docs/fortran_api.shtml#f/_/cpl_setup_md'>CPL_setup_md</a> on the MD side and 
            <a class="links" href='/api-docs/fortran_api.shtml#f/_/cpl_setup_cfd'>CPL_setup_cfd</a> for the CFD. 
            The 
            <a class="links" href='/api-docs/fortran_api.shtml#f/_/cpl_send'>CPL_send</a>
            and 
            <a class="links" href='/api-docs/fortran_api.shtml#f/_/cpl_recv'>CPL_recv</a>
            functions then exchange all 
            information for the grid in the overlap region with all information
            exchanged only between processes that are topological overlapping.
            This avoids any global communications and maintains the scaling of 
            the two codes. The send/recv limits are specified in 
            global grid coordinates and should be consistent between codes.
            This means the user does not need to know about the 
            processor topology of the two codes.

            The examples in this section start with fortran, then c++ and
            finally python. 
            
             
        </p>


        <h3 id="linking_fortran">Fortran</h3>
        <p>
            This section outlines a minimal example in Fortan which will send
            information between an MD and CFD code. 
        <p>
            The CFD code is first,
        </p>

            <details>
                <summary> See CFD Fortran Code </summary>
                    <!--#include virtual="demo-codes/cfd_send_cells_fortran.html"-->
            </details>
 

        <p>
            Now the MD code which receives data from the CFD code
            and 
        </p>

            <details>
                <summary> See MD Fortran Code </summary>
                 <!--#include virtual="demo-codes/md_recv_cells_fortran.html"-->
            </details>   


        <p>
            This can be built using,
        </p>

        <div class="code">
           $  cplf90 ./md_recv_cells.f90 -o ./md.exe
 
        </div>

        <p>
            and similar for cfd executable,
        </p>

      <div class="code">
           $  cplf90 ./cfd_send_cells.f90   -o ./cfd.exe
        </div>

        <p>
            and run using
        </p>

        <div class="code">
            mpiexec -n 4 ./cfd.exe : -n 16 ./md.exe
        </div>

        <p>
            The coupled simulation sends data from the 16 MD 
            processors to the 4 CFD processors. These are then
            checked to ensure they correspond the correct
            locations in space and any errors are printed.
            A correct output will contain something like,
        </p>

        <div class="code">
            MD -- (rank= 2) CELLS HAVE BEEN RECEIVED CORRECTLY. <br>&nbsp;&nbsp;
            MD -- (rank= 3) CELLS HAVE BEEN RECEIVED CORRECTLY. <br>&nbsp;&nbsp;
            MD -- (rank= 6) CELLS HAVE BEEN RECEIVED CORRECTLY. <br>&nbsp;&nbsp;
            MD -- (rank= 7) CELLS HAVE BEEN RECEIVED CORRECTLY. <br>&nbsp;&nbsp;
            MD -- (rank=10) CELLS HAVE BEEN RECEIVED CORRECTLY. <br>&nbsp;&nbsp;
            MD -- (rank=11) CELLS HAVE BEEN RECEIVED CORRECTLY. <br>&nbsp;&nbsp;
            MD -- (rank=14) CELLS HAVE BEEN RECEIVED CORRECTLY. <br>&nbsp;&nbsp;
            MD -- (rank=15) CELLS HAVE BEEN RECEIVED CORRECTLY. 
        </div>

        <p>
            The examples for C++ and python are included next.
        </p>


        <h3 id="linking_cpp">C++</h3>

        <p>
            This section outlines a minimal example in C++ which will send
            information between an MD and CFD code. The C bindings are
            similar.
        </p>
            <details>
                <summary> See CFD C++ code </summary>
                 <!--#include virtual="demo-codes/cfd_send_cells_cpp.html"-->
            </details>   

        <p>
            and the MD code,
        </p>

            <details>
                <summary> See MD C++ code </summary>
                 <!--#include virtual="demo-codes/md_recv_cells_cpp.html"-->
            </details>

         This is what I want to copy




        <h3 id="linking_python">python</h3>
        <p>
            This section outlines a minimal example in python which will send
            information between an MD and CFD code. The CFD code is first.
        </p>

        <details>
            <summary> See CFD python Code </summary>
             <!--#include virtual="demo-codes/cfd_send_cells_python.html"-->
        </details>   

        <p>
            and the MD code,
        </p>

            <details>
                <summary> See MD python Code </summary>
                 <!--#include virtual="demo-codes/md_recv_cells_python.html"-->
            </details>   


        <p>
            This minimal send and receive example forms part of the testing 
            framework of the <mark class="CPL">CPL library</mark> and represents the core functionality
            of coupled simulation. We use this idea in the next section 
            to develop a mock framework to allow testing of coupled 
            simulations.
        </p>


        <h2 id="mock">CPL Mocks</h2>


        <p>
            The aim of coupling is to link two highly non-linear codes,
            as a result, ensuring correct behaviour is highly non-trivial.
            Even if the two linked codes are each free of bugs, there
            are issues of numerical stability, resolution, overlap size,
            averaging time, choice of exchanged variable and much more.
            For this reason, the proposed method of development of any
            coupling application is through the use of 
            <mark class="CPL">CPL Mocks</mark>, minimal Python scripts
            used to inject known values and check the response.
            In this way, the MD and CFD codes can be separated and tested in 
            isolation, with the expected response used to automate this 
            test as part of a continuous testing framework (on e.g. Travis CI).
            This also provides a way to explore the functionality of either code,
            and ensures that when we link them together, we can quickly identify
            if the problem is in the CFD, MD or due to their (highly 
            non-linear) interaction. An example of a
            <mark class="CPL">CPL Mocks</mark> code is shown here, first the 
            minimal MD <mark class="CPL">CPL Mocks</mark> code:
        </p>

        <div class="code">
            <!--#include virtual="demo-codes/minimal_CFD.html"-->
        </div>

        <p>
            The minimal CFD <mark class="CPL">CPL Mocks</mark> code is as
            follows:
        </p>

        <div class="code">
            <!--#include virtual="demo-codes/minimal_MD.html"-->
        </div>


       <p>
            Running these codes together will print the send and recv values.
            These should be turned into assert statements as part of a testing
            to ensure that for a given CPL_send value, the coupled code
            is working as expected. For examples of how we use these mocks, see the test
            folders under the various
             <a class="links" href="https://github.com/Crompulence/CPL_APP_LAMMPS-DEV/tree/master/test/constant_force">APPS</a>.


            Note a difficulty of MPI runs in that they must be started with mpiexec, so 
            it is not possible to bootstrap the complete test from the mock code.
            Instead, the mocks can be used to trigger an error which is picked up
            by the higher level script which creates the mpiexec 
            instance between the code and the mock.


            Even with minimal mock scripts, designing coupled simulation can still be
            tricky, owing to the different possible
            data formats in the various codes as well as different grid index systems.
            In the next section, visualisation of a coupled code is presented to show
            how design and development of coupled simulation can be aided by 
            minimal CFD and MD code with <mark class="CPL">CPL library</mark> tools.
        </p>




        <!--<div class="code">
            make PLATFORM=template cpl_
        </div>

        <p>
            where "template" refers to the name of the included file /make/template.inc, in
            which the compilers and flags are defined. The template provided is likely to
            work for most machines, but the user is advised to create their own version.
        </p>-->
        <h2 id="analysing">Demo: visualising a coupled simulation</h2>

        <p>
            The previous section, although an important test is not particularly exciting.
            In this section, a more interesting visual example is presented which uses
            the python interface and matplotlib to plot results. A fortran
            and C++ example MD code specifies a test sinusoidal function in x, y and z of the form
            and sends this to a python CFD code which plots the results. The fortran and
            C++ codes are almost identical to the <a href=#linking>previous example</a> , 
            although this time the MD code sends to the CFD. 
        </p>

        <details>
            <summary> MD fortran send code </summary>
             <!--#include virtual="demo-codes/md_send_topo_fortran.html"-->
        </details>

        <details>
            <summary> MD C++ send code </summary>
             <!--#include virtual="demo-codes/md_send_topo_cpp.html"-->
        </details> 

        <p>
            The python code requires matplotlib to be <a href=http://matplotlib.org/users/installing.html>installed</a>,
            which for debian based systems uses,
        </p> 

        <div class="code">
            sudo apt-get build-dep python-matplotlib
        </div>

        <p>
            please see the <a href=http://matplotlib.org/index.html>official website </a> for more information.
            The python CFD code which uses matplotlib to display the data received is as follows:
        </p> 

        <details>
            <summary> CFD python receive and plot </summary>
             <!--#include virtual="demo-codes/CFD_recv_and_plot_python.html"-->
        </details>

        <p>
            Setting the cells to be a combination of sines and cosines for every cell in the domain, here set to be 16 by 16 in 
            x and z. The CFD code has 6 cell with an overlap size specified to be 4 cells,
        </p> 

        <img height=50px src="./images/sincos.png"
             attribute="A(1,i,j,k) = sin\left(\frac{2\pi i}{N_x}-0.25j\pi\right)cos\left(\frac{2\pi k}{N_z}\right)" >

        <p>
            This plots a figure of the form,
        </p> 

        <img width=80% src="./images/topology_plot.png" >

        <p>
            The grid is set to a function of x, y and z based on cell numbers.
            The above plot shows the x and y dependence of the functions on the top 
            figure and the x z dependence on the bottom layer.
            In order to understand this in the context of the coupler grid, a python
            function can be used to add a grid to the plot,
        </p> 


        <details>
            <summary> Python Grid plotting function </summary>
             <!--#include virtual="demo-codes/draw_grid_python.html"-->
        </details>

        <p>
            There are no changes needed to the sending code. 
            The python code to use this in the plot introduced the cpl_get 
            function in order to obtain the grid properties. All details of the
            coupled configuration are stored locally on every processor so no
            communication is required for a call to CPL_get. The code receive data
            and plots over the grid resulting the following,
        </p> 

        <img width=80% src="./images/topology_grid_plot.png" >

        <p>
            Notice that the MD code is running on four processor, highlighted on the grid plot and the
            sine function is clearly continuous and correct over the processor boundaries.
            A single CFD processor receives and plots the data for simplicity here.
            The CPL could be used in this manner on an MD or CFD simulation running on
            thousands of processors and the <mark class="CPL">CPL library</mark> linked only to display current values.
            By linking to a particular location, a slice or plot along an axis could be used
            without requiring global communications. In this way, the current state of the simulation can
            be checked while maintaining the scaling of the codes.
            This is a very important function to use when developing coupled simulation.
            As typically you will be linking two highly complex and non-linear codes, they MUST
            be verified individually before attempting to link. Using the python interface with
            plotting, the two codes can be visualised, unit-tested and verified individually.
            The simple plotting interface can then be simply swapped out for the other code.
            Both for debugging and controlling simulation, <mark class="CPL">CPL library</mark> can be used to not only display the 
            current state of the simulation, but also to control it by specifying a boundary condition.
            In the next section, the plotting code sends back a user changeable value to interactively
            develop a coupled simulation. 
        </p> 



        <h2 id="interactive">Demo: Interactive GUI using CPL library</h2>


        <p>
            This allows an interactive code to adjust the various values, usign a send and
            receive. The python code to do this uses matplotlib widgets and is as follows,
        </p> 


        <details>
            <summary> Interactive python code </summary>
             <!--#include virtual="demo-codes/Interactive_python.html"-->
        </details>

        <p>
            The previous MD code needs to be adapted to receive data from the
            CFD code, as well as evolve in time to reflect the potentially changing
            boundary. 

        <details>
            <summary> Interactive fortran code </summary>
             <!--#include virtual="demo-codes/Interactive_fortran.html"-->
        </details>

        <details>
            <summary> Interactive cpp code </summary>
             <!--#include virtual="demo-codes/Interactive_cpp.html"-->
        </details>

            Again, this highlights a use of <mark class="CPL">CPL library</mark> framework beyond simple
            coupled simulation, where a user interface could be attached to a large scale
            simulation running on many processors to visualise and adjust the simulation
            as required during a run. 

            In the next section, the example of a fully coupled
            CFD and MD code is presented.
        </p> 

        <img width=80% src="./images/interactive_grid_plot.png" >

        <h2 id="MD_CFD">Minimal CFD and MD code with coupling</h2>


        <p>
            Bringing the previous examples of topological setup and data exchange,
            along with a minimal CFD solver for the 2D unsteady diffusive equation,
        </p> 

        <img height=50px src="./images/unsteady_diffuse.png" 
             attribute="\frac{\partial u}{\partial t} =  \nu \left[ \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right]">

        <p>
            This equation is simulated using a simple finite difference method 
            with a forward Euler for time discretisation and central difference in space. 
            The code is written in an object oriented manner, so a CFD object can be
            created, containing all field information and allowing specification of 
            boundary conditions, time evolution and plotting:
        </p> 

        <details>
            <summary> CFD code </summary>
             <!--#include virtual="demo-codes/cfd_oo.html"-->
        </details>

        <p>
            The minimal MD code solves Newton's law for the N body problem. 
            This is extremely slow in python so only a very small number of molecules are used. 
            A simply cell list is also included to speed up the simulation and time is advanced
            using a Verlet algorithm. A number of non-equilibrium molecular dynamics and coupling 
            specific features are required, including fixed wall molecules, a specular wall at
            the domain top, constraint force and averaging to get a velocity field.
            The MD code is also written in an object oriented manner with functions for force calculation, time evolution
            and plotting along with more exotic velocity averaging, constraint forces and specular walls. 
        </p> 

        <details>
            <summary> MD code </summary>
             <!--#include virtual="demo-codes/md_oo.html"-->
        </details>

        <p>
            The code to coupled the MD and CFD code is written as
            two separate files to instantiate the respective MD or CFD solver object. 
            The <mark class="CPL">CPL library</mark> is fexible in that both CFD and MD could be contained in a single file,
            however the recommended way to couple is through the MPMD model with all
            data exchange through <mark class="CPL">CPL library</mark>. This keeps the scope of both codes
            separated and ensures changes to each piece of coupled software is minimised.
            The coupled MD code is here, notice that most of the code is boilerplate problem setup,
        </p> 

        <details>
            <summary> Coupling code MD side </summary>
             <!--#include virtual="demo-codes/md_cpl.html"-->
        </details>


        <p>
            The CFD code is here and looks very similar to previous coupling examples and the MD,
        </p> 

        <details>
            <summary> Coupling code CFD side </summary>
             <!--#include virtual="demo-codes/cfd_cpl.html"-->
        </details>

        <p>
            The resulting simulation should run and display evolving molecules on one
            figure (with overlayed averaged), and a CFD on a seperate panel.
        </p> 


        <img width=80% src="./images/MD_CFD.png" >

        <p>
            Note that these toy MD and CFD codes are provided as an example and there is no guarantee of the
            accuracy or correctness of either code. In fact, the constraint applied appears to need
            some work on the MD code.
        </p> 

    </div>

</body>

</html>
